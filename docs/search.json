[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matrix Algebra for Educational Scientists",
    "section": "",
    "text": "Foreword\nSpring 2021.\nThe contents of this book constitute information from several set of notes from a variety of QME courses, including from an old course called EPSY 8269. We are making this book available as a resource for anyone who wants to use it. We will be adding and revising the content for awhile. Feel free to offer criticism, suggestion, and feedback. You can either open an issue on the book’s github page or send us an email directly.\nAndrew Zieffler & Michael Rodriguez",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Matrix Algebra for Educational Scientists",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nMany thanks to James Terwilliger, whose initial notes gave rise to some of this material. Also, thank you to all the students in our courses who have been through previous iterations of this material. Your feedback has been invaluable, and you are the world’s greatest copy editors.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "Matrix Algebra for Educational Scientists",
    "section": "Colophon",
    "text": "Colophon\n\nThe book is typeset using Atkinson Hyperlegible. The color palette was generated using coolors.co.\nIcon and note ideas and prototypes by Desirée De Leon.\nSome of the book style and CSS code were inspired by: De Leon, D., & Hill, A. (2019). A handbook for teaching and learning with R and RStudio.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Having a basic understanding of the vocabulary, notation, and ideas of matrix algebra, is important for all educational scientists who use quantitative methods in their work. The statistical and psychometric models underlying many quantitative methodologies employed in educational research rely on matrix algebra. Subsequently, educational scientists use the language and notation of matrix algebra to communicate in the scientific literature. Moreover, matrix algebra forms the bedrock of statistical computation. Having fundamental knowledge of matrix algebra can often help an educational scientist troubleshoot problems that arise in their own work, and devise solutions for those issues.\nFor quantitative methodologists, it is important to have a much deeper understanding of matrix algebra, as it is foundational to the computational estimation and optimization used in methodological work. Statistical programming, formulating the mathematics of quantitative methods, and even back-of-the-napkin calculations are all made easier (and more efficient) through matrix algebra.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "02-data-structures.html",
    "href": "02-data-structures.html",
    "title": "1  Data Structures",
    "section": "",
    "text": "1.1 Scalars\nIn this chapter you will be introduced to four common data structures that form the building blocks of matrix algebra: scalars, vectors, matrices, and tensors. You will also be introduced to some of the vocabulary that we use to describe these structures. In future chapters, we will examine these structures in more detail and learn how to mathematically manipulate and operate on these structures.\nA scalar is a single real number. You have likely had a lot of previous experience with scalars, as they are emphasized in much of the mathematics taught in high schools in the United States. Here are three examples of scalars:\n\\[\n1 \\qquad \\sqrt{2} \\qquad -7\n\\]\nScalar arithmetic is the arithmetic operations (addition, subtraction, multiplication, and division) we perform using real numbers. For example,\n\\[\n\\begin{split}\n2 + 3 = 5 \\\\[1ex]\n21 \\div 3 = 7\n\\end{split}\n\\]\nNotice that scalar arithmetic also produces a scalar. For example the scalar addition in the example, \\(2+3\\) produces the scalar \\(5\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "02-data-structures.html#vectors",
    "href": "02-data-structures.html#vectors",
    "title": "1  Data Structures",
    "section": "1.2 Vectors",
    "text": "1.2 Vectors\nA vector is a specifically ordered one-dimensional array of values. Here are three examples of vectors:\n\\[\n\\begin{bmatrix}\n-1 \\\\ 3 \\\\ \\sqrt{7} \\\\2\n\\end{bmatrix} \\qquad \\begin{bmatrix}\n5 & 4\n\\end{bmatrix} \\qquad \\begin{bmatrix}\na_1 \\\\ a_2 \\\\ a_3\n\\end{bmatrix}\n\\]\nA vector may be written as a row or a column, and are respectively referred to as row vectors or column vectors. Each value in a vector is called an element or component. Vectors are also typically described in terms of the number of elements they have. For example, the following is a two-element row vector:\n\\[\n\\begin{bmatrix}\n5 & 4\n\\end{bmatrix}\n\\]\nHere, a is a four-element column vector:\n\\[\n\\mathbf{a} = \\begin{bmatrix}\n5 \\\\ 4 \\\\ 7 \\\\2\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "02-data-structures.html#matrices",
    "href": "02-data-structures.html#matrices",
    "title": "1  Data Structures",
    "section": "1.3 Matrices",
    "text": "1.3 Matrices\nA matrix is specifically ordered two-dimensional array of values. Here are three examples of matrices:\n\\[\n\\begin{bmatrix}\n-1 & 5\\\\ 3 & 2 \\\\ \\sqrt{7} & -4 \\\\2 & 2\n\\end{bmatrix} \\qquad \\begin{bmatrix}\n5 & 4 \\\\ 1 & 6\n\\end{bmatrix} \\qquad \\begin{bmatrix}\na_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\ a_{3,1} & a_{32}\n\\end{bmatrix}\n\\] Matrices have both rows and columns, and are typically described by the number of rows and columns they have. For example, the matrix B (below) has 3 rows and 2 columns:\n\\[\n\\underset{3\\times 2}{\\mathbf{B}} = \\begin{bmatrix}\n5 & 1 \\\\\n7 & 3 \\\\\n-2 & -1\n\\end{bmatrix}\n\\]\nWe say that B is a “3 by 2” matrix. The number of rows and columns are referred to as the dimensions or order of the matrix, and, for matrix B, is denoted as \\(3\\times 2\\). The dimension is often appended to the bottom of the matrix (e.g., \\(\\underset{3\\times 2}{\\mathbf{B}}\\)).\nThe elements within the matrix are indexed by their row number and column number, respectively. For example, \\(\\mathbf{B}_{1,2} = 1\\) since the element in the first row and second column is 1. The subscripts on each element indicate the row and column positions of the element.1\nMore generally, we define matrix A, which has n rows and k columns as:\n\\[\n\\underset{n\\times k}{\\mathbf{A}} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} & \\ldots & a_{1k} \\\\\na_{21} & a_{22} & a_{23} & \\ldots & a_{2k} \\\\\na_{31} & a_{32} & a_{33} & \\ldots & a_{3k} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\na_{n1} & a_{n2} & a_{n3} & \\ldots & a_{nk}\n\\end{bmatrix}\n\\]\nwhere element \\(a_{ij}\\) is in the \\(i^{\\mathrm{th}}\\) row and \\(j^{\\mathrm{th}}\\) column of A.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "02-data-structures.html#tensors",
    "href": "02-data-structures.html#tensors",
    "title": "1  Data Structures",
    "section": "1.4 Tensors",
    "text": "1.4 Tensors\nTensors, generally speaking, are the generalization of the matrix to three or more dimensions. Here is an example of a tensor:\n\\[\n\\begin{bmatrix}\n\\begin{bmatrix}\n5 & 4\n\\end{bmatrix}  & \\begin{bmatrix}\n1 & 6\n\\end{bmatrix}  \\\\ \\begin{bmatrix}\n2 & 3\n\\end{bmatrix} & \\begin{bmatrix}\n0 & 7\n\\end{bmatrix}\n\\end{bmatrix}\n\\]\nTechnically, this definition is not quite true, but for our purposes it will be adequate to think of a tensor as a structure for data in N dimensions.\nThis book will primarily deal with scalars, vectors, and matrices, but tensors do come up in statistical work. For example, image data are often represented as tensors; with each pixel in a two-dimensional image having multiple values associated with it to represent the color information for the pixel. In longitudinal data analysis, the variance–covariance matrix of the responses for multiple subjects is also represented as a tensor. Bi et al. (2021) and McCullagh (2018) are good resources to learn more about working with and operating on tensors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "02-data-structures.html#a-word-about-notation",
    "href": "02-data-structures.html#a-word-about-notation",
    "title": "1  Data Structures",
    "section": "1.5 A Word about Notation",
    "text": "1.5 A Word about Notation\nAuthors and textbooks use a wide variety of notation to represent scalars, vectors, and matrices. For example, vectors might be denoted using a lower-case underlined letter (\\(\\underline{a}\\)), a lower-case bolded letter (a), or a lower-case letter with an overset arrow (\\(\\vec{a}\\)). In this book, we will try to use a consistent notation to denote each of these structures.\n\nFYI\n\nScalars will be denoted with an italicized lower-case letter (e.g., \\(a\\)) or a non-bolded lower-case Greek letter (e.g., \\(\\lambda\\)).\nVectors will be denoted using a bold-faced lower-case letter (e.g., a.\nMatrices will be denoted using a bold-faced upper-case letter (e.g., A) or a bold-faced upper-case Greek letter (e.g., \\(\\boldsymbol\\Phi\\)).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "02-data-structures.html#references",
    "href": "02-data-structures.html#references",
    "title": "1  Data Structures",
    "section": "References",
    "text": "References\n\n\n\n\nBi, X., Tang, X., Yuan, Y., Zhang, Y., & Qu, A. (2021). Tensors in Statistics. Annual Review of Statistics and Its Application, 8(1), 345–368. https://doi.org/10.1146/annurev-statistics-042720-020816\n\n\nMcCullagh, P. (2018). Tensor methods in statistics. http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=5228165",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "02-data-structures.html#footnotes",
    "href": "02-data-structures.html#footnotes",
    "title": "1  Data Structures",
    "section": "",
    "text": "Many authors do not include the comma between the row and column parts of the subscript (e.g., \\(\\mathbf{B}_{1,2} = \\mathbf{B}_{12}\\)).↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "03-vectors.html",
    "href": "03-vectors.html",
    "title": "2  Vectors",
    "section": "",
    "text": "2.1 What is a Vector?\nIn this chapter you will learn more about the vector data structure. You will be introduced to some of the vocabulary that we use to describe vectors. You will also be introduced to the geometry of vectors. It is important to recognize that although the illustration of vector geometry is restricted to 2-dimensional space, all of these ideas can be extended to n-dimensions.\nA vector is a specifically ordered one-dimensional array of values written as a row or column. Vectors are typically described by the number of elements they have. Here, for example, a is a four-element column vector:\n\\[\n\\mathbf{a} = \\begin{bmatrix}\n5 \\\\ 4 \\\\ 7 \\\\ 2\n\\end{bmatrix}\n\\]\nWe might also say that a is a column vector in 4-dimensions.\nIn statistical or psychometric applications, vectors are how we typically represent and structure data collected on a particular attribute. For example, a might represent test scores for \\(n=4\\) students. Each element in this vector would correspond to a student’s test score. In this case, \\(a_1=5\\) would be the test score for the first student recorded in the vector, \\(a_2=4\\) would be the test score for the second student recorded in the vector, etc.\nComputationally, we can also work with vectors using R. We use the matrix() function to create a column or row vector in R. We include the elements of the vector in a c() function and provide this to the data= argument. We also include either the argument ncol=1 (column vector) or nrow=1 (row vector). Below we create vector a. We can also use the length() function to count the number of elements in a vector.\n# Create column vector a\na = matrix(data = c(5, 4, 7, 2), ncol = 1)\na\n\n     [,1]\n[1,]    5\n[2,]    4\n[3,]    7\n[4,]    2\n\n# Create row vector b\nb = matrix(data = c(1, 0, 0), nrow = 1)\nb\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n\n# Count number of elements in a\nlength(a)\n\n[1] 4\n\n# Count number of elements in b\nlength(b)\n\n[1] 3",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "03-vectors.html#what-is-a-vector",
    "href": "03-vectors.html#what-is-a-vector",
    "title": "2  Vectors",
    "section": "",
    "text": "2.1.1 Transposition\nOne important vector operation is transposition. Transposition is an operation in which we replace the ith element of a column vector as the ith element of a row vector, and vice-versa. In other words, we are converting a column vector into a row vector, or, conversely, converting a row vector into a column vector. Notationally, we use a superscripted prime or a superscripted intercalate symbol (looks like a “T”) to denote a vector’s transpose. For example, transposing vector a, which we defined earlier:\n\\[\n\\mathbf{a} = \\begin{bmatrix}\n5 \\\\ 4 \\\\ 7 \\\\ 2\n\\end{bmatrix} \\qquad \\mathbf{a}^{\\prime} = \\mathbf{a}^{\\intercal} = \\begin{bmatrix}\n5 & 4 & 7 & 2\n\\end{bmatrix}\n\\]\nIn R, the t() function will compute the transpose of a vector. Here we use t() to compute the transpose of the earlier defined vectors a and b.\n\n# Transpose of a\nt(a)\n\n     [,1] [,2] [,3] [,4]\n[1,]    5    4    7    2\n\n# Transpose of b\nt(b)\n\n     [,1]\n[1,]    1\n[2,]    0\n[3,]    0\n\n\n\nNOTATION\nThroughout the remainder of the book, row vectors will be denoted using a transpose. So, for example, a will indicate a column vector, and \\(\\mathbf{a}^{\\intercal}\\) will indicate a row vector.",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "03-vectors.html#general-form-of-a-vector",
    "href": "03-vectors.html#general-form-of-a-vector",
    "title": "2  Vectors",
    "section": "2.2 General Form of a Vector",
    "text": "2.2 General Form of a Vector\nNow that we have introduced transposition, we can formalize the general notation for both column and row vectors. The general form of a column vector with n elements is:\n\\[\n\\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix}\n\\]\nWhile, the general form of a row vector with n elements is:\n\\[\n\\mathbf{x}^{\\intercal} = \\begin{bmatrix}\nx_1 & x_2 & x_3 & \\ldots & x_n\n\\end{bmatrix}\n\\]\nVectors can also be thought of as a special case of a matrix in one of the dimensions is equal to 1. For example x could be considered a \\(n \\times 1\\) matrix, and \\(\\mathbf{x}^{\\intercal}\\) could be considered a \\(1 \\times n\\) matrix.",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "03-vectors.html#the-geometry-of-vectors",
    "href": "03-vectors.html#the-geometry-of-vectors",
    "title": "2  Vectors",
    "section": "2.3 The Geometry of Vectors",
    "text": "2.3 The Geometry of Vectors\nA vector can be represented geometrically by using a coordinate system where each element in the vector corresponds to a distance along one of the reference axes defining the coordinate system. Consider the column vector a:\n\\[\n\\mathbf{a} = \\begin{bmatrix}\n2 \\\\ 3\n\\end{bmatrix}\n\\]\nThis vector has a distance of 2 on the first reference axis (R1) and a distance of 3 on the second reference axis (R2).\n\n\n\n\n\nFigure 2.1: Plot showing vector a (in red) in the R1–R2 dimensional space.\n\n\n\n\n\n\n\n\nIn Figure 2.1 the reference coordinate system (R1–R2) define a two-dimensional space. Vector a had two elements and thus resides in a two-dimensional space. In general, an n-dimensional vector resides in an n-dimensional space.\nThe reference axes always intersect at the origin of the space (i.e., the reference point \\((0, 0)\\)). Here the reference axes are at right angles to one another, although this is not a requirement. When the reference axes are at right angles to each other, the reference system is referred to as an orthogonal coordinate system.",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "03-vectors.html#vector-properties",
    "href": "03-vectors.html#vector-properties",
    "title": "2  Vectors",
    "section": "2.4 Vector Properties",
    "text": "2.4 Vector Properties\nGeometrically, a vector is displayed as a line segment with an arrowhead at one end of the segment. The end of the vector with the arrowhead is called the head or terminus, and the other end of the vector is called the tail or origin. The head of the vector indicates the direction of the vector.\nAll vectors have a direction. They also have a length. These two properties completely define a vector within a given reference system. Note that location of the vector in the reference system is not a property of the vector. For example, the red vector and the blue vector below are identical—they have the same direction and length!\n\n\n\n\n\nFigure 2.2: Plot showing two identical vectors (in red and blue) in the R1–R2 dimensional space. They are identical because they have the same length and direction. Location in the reference system is a convenience, not a property of the vector.\n\n\n\n\n\n\n\n\n\nPROTIP\nLocation is a convenience in that we can re-locate any vector within the reference system to make it easier to work with.\n\n\n\n2.4.1 Vector Length\nThe length of a vector (also referred to as the vector’s magnitude or norm) is the distance from the vector’s tip to tail. In an orthogonal reference system, if we locate the tail of the vector at the origin, multiple right triangles with the vector as hypotenuse can be formed with the reference axes. The Pythagorean Theorem can then be applied to determine the length of a vector.\n\n\n\n\n\nFigure 2.3: Plot showing vector a (in red) in the R1–R2 dimensional space. This vector is the hypotenuse of a right triangle with legs of length 2 and 3.\n\n\n\n\n\n\n\n\nIn Figure 2.3, vector a is the hypotenuse of a right triangle with legs of length 2 and 3. We can use the Pythagorean Theorem to find the length of the vector:\n\\[\n\\begin{split}\nC^2 &= A^2 + B^2 \\\\[2ex]\n&= 2^2 + 3 ^2 \\\\[2ex]\n&= 13 \\\\[2ex]\nC &= \\sqrt{13} \\approx 3.61\n\\end{split}\n\\]\nNote that the sum of the squared elements is equal to the square of the length of the vector, and to determine the length of a (denoted as \\(\\lvert\\lvert \\mathbf{a} \\rvert\\rvert\\)), we take the square root of that sum. For our two-dimensional vector, this is\n\\[\n\\lvert\\lvert \\mathbf{a} \\rvert\\rvert = \\sqrt{a_1^2 + a_2^2}\n\\]\nWe can generalize this to finding the length of a vector a with n-dimensions:\n\\[\n\\lvert\\lvert \\mathbf{a} \\rvert\\rvert = \\sqrt{a_1^2 + a_2^2 + a_3^2 + \\ldots + a_n^2}\n\\] Computationally, to compute the length of a vector, we chain together several computations that (1) square the elements of in the vector, (2) sum those squares together, and (3) compute the square root of this sum. Below we create a 2-element column vector a and compute its length.\n\n# Create vector a\na = matrix(data = c(2, 3), ncol = 1)\na\n\n     [,1]\n[1,]    2\n[2,]    3\n\n# Compute length of vector a\nsqrt(sum(a ^ 2))\n\n[1] 3.605551\n\n\n\n\n\n2.4.2 Vector Direction\nA vector’s direction is often expressed as the measure of the angle between the horizontal reference axis and the vector.1 Figure 2.4 shows this angle (denoted \\(\\theta\\)) for vector a.\n\n\n\n\n\nFigure 2.4: Plot showing vector a (in red) in the R1–R2 dimensional space. The direction of this vector is the measure of the angle (\\(\\theta\\)) between the horizontal reference axis and the vector.\n\n\n\n\n\n\n\n\nTo determine this angle, we need to use a trigonometric function. For example, here we can use the fact that the cosine of an angle \\(\\theta\\) is defined as the length of the side of the right triangle which is adjacent to \\(\\theta\\) divided by the length of the hypotenuse of the triangle.2\n\\[\n\\cos (\\theta) = \\frac{\\lvert\\lvert\\mathrm{Adjacent~Side}\\rvert\\rvert}{\\lvert\\lvert\\mathrm{Hypotenuse}\\rvert\\rvert}\n\\]\nIn our example,\n\\[\n\\cos (\\theta) = \\frac{2}{\\sqrt{13}}\n\\]\nTo find \\(\\theta\\), we need to calculate the arc-cosine of \\(\\frac{2}{\\sqrt{13}}\\). We can compute the arc-cosine in R using the acos() function. This function will compute the value of \\(\\theta\\) in radians. We can also convert this to degrees by multiplying by \\(\\frac{180}{\\pi}\\) (where \\(\\pi\\) is the mathematical constant approximated as 3.14).\n\n# Find theta using arccosine (radians)\nacos(2 / sqrt(13))\n\n[1] 0.9827937\n\n# Find theta using arccosine (degrees)\nacos(2 / sqrt(13)) * 180 / pi\n\n[1] 56.30993\n\n\nWe always measure the angle counter-clockwise to the reference axis. So if we measure the direction associated with the vector \\(\\begin{bmatrix}-2 \\\\ -2\\end{bmatrix}\\), the direction is more than \\(\\pi\\) radians (or \\(180^\\circ\\)).\n\n\n\n\n\nFigure 2.5: Plot showing vector (-2, -2) (in red) in the R1–R2 dimensional space. The direction of this vector is the measure of the angle (\\(\\theta\\)) between the vector and the horizontal reference axis measured in the counter-clockwise direction from the axis.\n\n\n\n\n\n\n\n\nThere are several ways to compute this angle. Remember that when we use the trigonometric functions, we are computing the angle in the right triangle formed by the vector and the reference axes (see Figure 2.6).\n\n\n\n\n\nFigure 2.6: Plot showing vector (-2, -2) (in red) in the R1–R2 dimensional space. The direction of this vector is the measure of the angle (\\(\\theta\\)) between the vector and the horizontal reference axis measured in the counter-clockwise direction from the axis. Here we have split this angle up into two parts; \\(\\alpha\\) can be computed using a trigonometric function, and \\(\\beta = \\pi\\) radians.\n\n\n\n\n\n\n\n\nTo find \\(\\theta\\) we need to find \\(\\alpha\\) using trigonometric functions and then add it to \\(\\beta\\). For example,\n\\[\n\\begin{split}\n\\tan (\\alpha) &= \\frac{\\lvert\\lvert\\mathrm{Opposite~Side}\\rvert\\rvert}{\\lvert\\lvert\\mathrm{Adjacent~Side}\\rvert\\rvert} \\\\[2ex]\n&= \\frac{2}{2} \\\\[2ex]\n&= 1\n\\end{split}\n\\]\nComputing the arc-tangent with the atan() function we find that \\(\\alpha=0.785\\) radians. Adding this to the \\(\\beta\\) value which is \\(\\pi\\) radians, we get the vector’s direction of 3.927 radians (or \\(225^\\circ\\)).\n\n# Compute alpha (in radians)\natan(1)\n\n[1] 0.7853982\n\n# Compute theta = alpha + beta (in radians)\natan(1) + pi\n\n[1] 3.926991\n\n# Compute theta = alpha + beta (in degrees)\n(atan(1) + pi) * 180 / pi\n\n[1] 225\n\n\nBeyond two dimensions, direction in more complicated, involving more than one angle. For example, reporting the direction of a vector in three dimensions requires two angles. One common method is to give (1) the angle between the first reference axis (e.g., x-axis) and the projection of the vector onto the plane defined by the first and second reference axes (e.g., the xy-plane), and (2) the angle between the vector and the third reference axis (e.g., the z-axis). Suffice it to say that we need additional angles to fully define the direction of a vector in higher dimensions.",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "03-vectors.html#vector-equality",
    "href": "03-vectors.html#vector-equality",
    "title": "2  Vectors",
    "section": "2.5 Vector Equality",
    "text": "2.5 Vector Equality\nTwo vectors are said to be equal if they satisfy two conditions:\n\nBoth vectors have the same dimensions (i.e., they have the same number of elements and both are row or column vectors);\nCorresponding elements from each vector must be equal.\n\nConsider the following vectors:\n\\[\n\\mathbf{a} = \\begin{bmatrix}\n5 \\\\ 4 \\\\ 7 \\\\ 2\n\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}\n5 \\\\ 4 \\\\ 2 \\\\ 7\n\\end{bmatrix} \\qquad \\mathbf{c}^{\\intercal} = \\begin{bmatrix}\n5 & 4 & 7 & 2\n\\end{bmatrix}\n\\]\nWithin this set of vectors,\n\n\\(\\mathbf{a} \\neq \\mathbf{b}\\), since not all of the corresponding elements are equal.\n\\(\\mathbf{a} \\neq \\mathbf{c}^{\\intercal}\\), since a’s dimensions are \\(4 \\times 1\\) (column vector) and \\(\\mathbf{c}^{\\intercal}\\)’s dimensions are \\(1 \\times 4\\) (row vector).\n\\(\\mathbf{a} = \\mathbf{c}\\) since both vectors a and c have the same dimensions (\\(4 \\times 1\\)) and all corresponding elements are equal.\n\nGeometrically, two vectors are equal if they have the same length and direction. (Remember, location in the reference coordinate system is not a property of the vectors, so two equal vectors might be in different locations.)",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "03-vectors.html#special-vectors",
    "href": "03-vectors.html#special-vectors",
    "title": "2  Vectors",
    "section": "2.6 Special Vectors",
    "text": "2.6 Special Vectors\nThere are several vectors that are special in that they have unique properties.\n\n\n2.6.1 Zero Vector\nOne special vector is the zero vector. A zero vector (denoted 0) is a vector in which every element is 0. For example, the following vector is a zero vector:\n\\[\n\\mathbf{0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n\\]\nThe length of a zero vector is 0. (Convince yourself of this!) Furthermore, since geometrically the head and tail of a zero vector coincide (are at the same location in the reference coordinate system) they have an undefinable direction.\n\n\n\n2.6.2 Ones Vector\nAnother special vector is referred to as a ones vector. A ones vector is a vector in which every elements is 1. For example, the following vector 1 is a ones vector:\n\\[\n\\mathbf{1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\n\\]\nThe length of a ones vector is \\(\\sqrt{n}\\). (Convince yourself of this!) The direction of a ones vector is \\(45^\\circ\\) or 0.785 radians since the each element in the vector is equal. For example, locating the tail of a 2-dimensional ones vector at the origin, it would lie on the \\(R1=R2\\) line. Figure 2.7 shows a 2-dimensional ones vector with its tail located at the origin.\n\n\n\n\n\nFigure 2.7: Plot showing the ones vector (in red) in the R1–R2 dimensional space. The \\(R1=R2\\) line is also displayed.\n\n\n\n\n\n\n\n\n\n\n\n2.6.3 Unit Vector\nConsider the n-element column vector u:\n\\[\n\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ \\vdots \\\\ u_n \\end{bmatrix}\n\\]\nu is a unit vector if the length of the vector is 1. For example, each of the following vectors are unit vectors:\n\\[\n\\mathbf{u}_1 = \\begin{bmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{bmatrix} \\qquad \\mathbf{u}_2 = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{u}_3 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\n\\]\nAlthough the length of a unit vector must be 1, the direction is not specified. Thus there are an infinite number of unit vectors. Figure 2.8 shows several unit vectors with their tail located at the origin.\n\n\n\n\n\nFigure 2.8: Plot showing several unit vectors in the R1–R2 dimensional space. All vector tails have been located at the origin.\n\n\n\n\n\n\n\n\n\n\n\n2.6.4 Elementary Vectors\nAn elementary vector is a vector that has one element that is equal to one and the remainder of its elements equal to 0. For example, each of the 3-element column vectors below are elementary vectors.\n\\[\n\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n\\]\nFor an n-dimensional vector there are always n elementary vectors. Moreover, every elementary vector is also a unit vector. Geometrically, each elementary vector lies on one of the reference axes. Figure 2.9 shows the two elementary vectors (with their tail located at the origin) in our 2-dimensional reference coordinate system.\n\n\n\n\n\nFigure 2.9: Plot showing the two elementary vectors in the R1–R2 dimensional space. All vector tails have been located at the origin.",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "03-vectors.html#exercises",
    "href": "03-vectors.html#exercises",
    "title": "2  Vectors",
    "section": "Exercises",
    "text": "Exercises\nCreate a 2-dimensional orthogonal reference system. Add the following vectors to that system.\n\n\\(\\mathbf{x} = \\begin{bmatrix}2 \\\\ -1 \\end{bmatrix}\\)\n\n\nShow/Hide Solution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathbf{y} = \\begin{bmatrix}-3 \\\\ 2 \\end{bmatrix}\\)\n\n\nShow/Hide Solution\n\n\n\n\n\n\n\n\n\n\n\n\n\nA unit-vector z in which all the elements are equal.\n\n\nShow/Hide Solution\n\n\n\\[\n\\mathbf{z} = \\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nAll elementary vectors.\n\n\nShow/Hide Solution\n\n\n\\[\n\\mathbf{e}_1 = \\begin{bmatrix} 0 \\\\ 1\\end{bmatrix} \\quad \\mathbf{e}_2 = \\begin{bmatrix} 1 \\\\ 0\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\nCompute the following:\n\n\\(\\lvert\\lvert\\mathbf{x}\\rvert\\rvert\\)\n\n\nShow/Hide Solution\n\n\nC \\[\n\\begin{split}\n\\lvert\\lvert\\mathbf{x}\\rvert\\rvert &= \\sqrt{(2)^2 + (-1)^2} \\\\[2ex]\n&= \\sqrt{5}\n\\end{split}\n\\]\n\n\n\\(\\lvert\\lvert\\mathbf{y}\\rvert\\rvert\\)\n\n\nShow/Hide Solution\n\n\nC \\[\n\\begin{split}\n\\lvert\\lvert\\mathbf{y}\\rvert\\rvert &= \\sqrt{(-3)^2 + (2)^2} \\\\[2ex]\n&= \\sqrt{13}\n\\end{split}\n\\]\n\n\n\\(\\lvert\\lvert\\mathbf{z}\\rvert\\rvert\\)\n\n\nShow/Hide Solution\n\n\nC \\[\n\\begin{split}\n\\lvert\\lvert\\mathbf{z}\\rvert\\rvert &= \\sqrt{\\bigg(\\frac{1}{\\sqrt{2}}\\bigg)^2 + \\bigg(\\frac{1}{\\sqrt{2}}\\bigg)^2} \\\\[2ex]\n&= \\sqrt{\\frac{1}{2} + \\frac{1}{2}} \\\\[2ex]\n&= \\sqrt{1} \\\\[2ex]\n&= 1\n\\end{split}\n\\] Remember, a unit-vector has a length of 1.\n\n\nThe direction based on the angle (in radians) from the horizontal reference axis for x.\n\n\nShow/Hide Solution\n\n\nC \\[\n\\begin{split}\n\\theta &= 2\\pi - \\arccos(\\frac{2}{\\sqrt{5}}) \\\\[2ex]\n&= 5.8195 \\mathrm{~radians}\n\\end{split}\n\\]\n\n\nThe direction based on the angle (in degrees) from the horizontal reference axis for y.\n\n\nShow/Hide Solution\n\n\nC \\[\n\\begin{split}\n\\theta &= \\bigg(\\frac{\\pi}{2} + \\arctan(\\frac{3}{2})\\bigg) \\times \\frac{180}{\\pi} \\\\[2ex]\n&= 146.31 \\mathrm{~degrees}\n\\end{split}\n\\]\n\n\nThe direction based on the angle (in radians) from the horizontal reference axis for z.\n\n\nShow/Hide Solution\n\n\nC \\[\n\\begin{split}\n\\theta &= \\arctan\\bigg(\\frac{\\frac{1}{\\sqrt{2}}}{\\frac{1}{\\sqrt{2}}}\\bigg) \\\\[2ex]\n&= \\arctan(1) \\\\[2ex]\n&= 0.786 \\mathrm{~radians}\n\\end{split}\n\\]\n\nIndicate whether the vectors are equal or not equal.\n\n\\(\\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix} \\overset{?}{=} \\begin{bmatrix} 6 \\\\ 5 \\end{bmatrix}\\)\n\n\nShow/Hide Solution\n\n\nNot equal\n\n\n\\(\\begin{bmatrix} 55 \\\\ 66 \\\\ 48 \\end{bmatrix} \\overset{?}{=} \\begin{bmatrix} 55 & 66 & 48 \\end{bmatrix}\\)\n\n\nShow/Hide Solution\n\n\nNot equal\n\nAssume a and b are equal. Find the values for \\(a_1\\), \\(a_3\\), and \\(b_2\\).\n\n\\(\\mathbf{a} = \\begin{bmatrix} a_1 \\\\ 0 \\\\ a_3 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 9 \\\\ b_2 \\\\ 1 \\end{bmatrix}\\)\n\n\nShow/Hide Solution\n\n\n\\(a_1=9\\), \\(a_3=1\\), and \\(b_2=0\\)",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "03-vectors.html#footnotes",
    "href": "03-vectors.html#footnotes",
    "title": "2  Vectors",
    "section": "",
    "text": "While it is common to express the direction as the measure of the angle between the horizontal reference axis, it could also be expressed as the measure of the angle between the vector and any of the reference axes. In more than two-dimensions, the direction would need to include multiple angles to specify the direction; you would need \\(n-1\\) angles to specify an n-dimensional vector.↩︎\nYou could also use the definition for sine or tangent to compute \\(\\theta\\).↩︎",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "04-vector-operations.html",
    "href": "04-vector-operations.html",
    "title": "3  Vector Operations",
    "section": "",
    "text": "3.1 Vector Addition and Subtraction\nIn this chapter you will learn about some of the common arithmetic operations (addition, subtraction, and multiplication) that can be performed with vectors. As in the previous chapter, you will also be introduced to the geometry of these vector operations in 2-dimensional space.\nVectors can be added together if they have the same dimensions (i.e., they have the same number of elements and are both row or column vectors). To add two vectors, each of length n, together, we sum the corresponding elements in the vectors. For example, consider the column vectors a and b, where,\n\\[\n\\mathbf{a} = \\begin{bmatrix}5 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}1 \\\\ 3\\end{bmatrix}\n\\]\nSince both vectors have dimensions \\(2 \\times 1\\), we can compute the sum of these vectors as:\n\\[\n\\begin{split}\n\\mathbf{a} + \\mathbf{b}  &= \\begin{bmatrix}5 \\\\ 2\\end{bmatrix} + \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}5 + 1 \\\\ 2 + 3\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}6 \\\\ 5\\end{bmatrix}\n\\end{split}\n\\]\nNote that the resulting vector is a column vector with two elements, the same as the vectors we summed together. In general, if we sum two n-dimensional column vectors, x and y, the resulting vector will also be an n-dimensional column vector:\n\\[\n\\begin{split}\n\\mathbf{x} + \\mathbf{y}  &= \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\\end{bmatrix} + \\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}x_1 + y_1 \\\\ x_2 + y_2 \\\\ x_3 + y_3 \\\\ \\vdots \\\\ x_n + y_n\\end{bmatrix}\n\\end{split}\n\\]\nThis process is similar for summing two n-dimensional row vectors, except the resulting vector will be an n-dimensional row vector.",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Operations</span>"
    ]
  },
  {
    "objectID": "04-vector-operations.html#vector-addition-and-subtraction",
    "href": "04-vector-operations.html#vector-addition-and-subtraction",
    "title": "3  Vector Operations",
    "section": "",
    "text": "3.1.1 Geometry of Adding Vectors\nGeometrically, adding two vectors, say a and b, is equivalent to drawing vector b so that its tail is placed at the head of vector a. Then the sum is the new vector that originates at vector a’s tail and terminates at vector b’s head. Figure 3.1 geometrically shows the sum of the following vectors:\n\\[\n\\mathbf{a} = \\begin{bmatrix}\n5 \\\\ 2\n\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}\n1 \\\\ 3\n\\end{bmatrix} \\qquad \\mathbf{a}+\\mathbf{b} = \\begin{bmatrix}\n6 \\\\ 5\n\\end{bmatrix}\n\\]\n\n\n\n\n\nFigure 3.1: Plot showing the sum of vector a (in red) and vector b (in blue) in the R1–R2 dimensional space. For convenience we have located the tail of vector a at the origin. The vector that corresponds to the sum of a and b is shown in purple.\n\n\n\n\n\n\n\n\n\n\n\n3.1.2 Vector Subtraction\nTo subtract vector b from vector a, we subtract the corresponding elements in vector b from those in vector a. For example, working with our previously defined vectors,\n\\[\n\\begin{split}\n\\mathbf{a} - \\mathbf{b} & = \\begin{bmatrix}\n5 \\\\ 2\n\\end{bmatrix} - \\begin{bmatrix}\n1 \\\\ 3\n\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}\n5-1 \\\\ 2 - 3\n\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}\n4 \\\\ -1\n\\end{bmatrix}\n\\end{split}\n\\] Since subtraction is equivalent to adding the inverse, subtracting the elements of b from a is equivalent to adding the inverted elements of b to the elements of a (where “inverting the elements” means switching the sign on each element). For example,\n\\[\n\\begin{split}\n\\mathbf{a} - \\mathbf{b} & = \\mathbf{a} + -\\mathbf{b} \\\\[2ex]\n&= \\begin{bmatrix}\n5 \\\\ 2\n\\end{bmatrix} + \\begin{bmatrix}\n-1 \\\\ -3\n\\end{bmatrix}\n\\end{split}\n\\]\nFigure 3.2 geometrically shows the operation of vector subtraction by adding the inverse of b to a.\n\n\n\n\n\nFigure 3.2: Plot showing the sum of vector a (in red) and the inverted vector b (in blue) in the R1–R2 dimensional space. For convenience we have located the tail of vector a at the origin. The vector that corresponds to the difference of a and b is shown in purple.\n\n\n\n\n\n\n\n\n\n\n\n3.1.3 Properties of Vector Addition and Subtraction\nIn vector addition, since each corresponding element is added, vector addition satisfies both the commutative and associative properties. That is,\n\\[\n\\mathbf{a} + \\mathbf{b} = \\mathbf{b} + \\mathbf{a}\n\\]\nand\n\\[\n\\begin{split}\n\\mathbf{a} + (\\mathbf{b} + \\mathbf{c}) &= (\\mathbf{a} + \\mathbf{b}) + \\mathbf{c} \\\\[2ex]\n&= \\mathbf{a} + \\mathbf{b} + \\mathbf{c}\n\\end{split}\n\\]\nConvince yourself these two properties are satisfied using the following vectors.\n\\[\n\\mathbf{a} = \\begin{bmatrix}5 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} \\qquad \\mathbf{c} = \\begin{bmatrix}2 \\\\ 0\\end{bmatrix}\n\\]\nSince we can re-write vector subtraction as vector addition, these same conditions and properties also apply for vector subtraction.",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Operations</span>"
    ]
  },
  {
    "objectID": "04-vector-operations.html#vectorscalar-multiplication",
    "href": "04-vector-operations.html#vectorscalar-multiplication",
    "title": "3  Vector Operations",
    "section": "3.2 Vector–Scalar Multiplication",
    "text": "3.2 Vector–Scalar Multiplication\nWhen a vector is multiplied by a scalar, each element of the vector is multiplied by the value of the scalar. Consider the following vector a and scalar \\(\\lambda\\),\n\\[\n\\mathbf{a} = \\begin{bmatrix}2 \\\\ 3 \\\\ 0 \\\\ -1\\end{bmatrix} \\qquad \\lambda=3\n\\]\nMultiplying a by \\(\\lambda\\) gives:\n\\[\n\\begin{split}\n\\lambda\\mathbf{a} &= 3\\begin{bmatrix}2 \\\\ 3 \\\\ 0 \\\\ -1\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}3 \\times 2 \\\\ 3 \\times 3 \\\\ 3 \\times 0 \\\\ 3 \\times -1\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}6 \\\\ 9 \\\\ 0 \\\\ -3\\end{bmatrix}\n\\end{split}\n\\]\nThis same method applies to row vectors. Scalars may be fractions, negative numbers, or unknowns. For example, if the scalar \\(\\gamma\\) is an unknown scalar, then\n\\[\n\\gamma\\mathbf{a} = \\begin{bmatrix}2\\gamma \\\\ 3\\gamma \\\\ 0 \\\\ -1\\gamma\\end{bmatrix}\n\\]\n\n\n3.2.1 Division by a Scalar\nTechnically, division is undefined for vectors. However, if we are dividing by a scalar, we can multiply the vector by the scalar’s reciprocal. For example, using our previously defined vector a and scalar \\(\\lambda\\),\n\\[\n\\mathbf{a} \\div \\lambda \\quad \\mathrm{does~not~exist}\n\\]\nBut, we can multiply by the reciprocal of \\(\\lambda\\), namely \\(\\dfrac{1}{\\lambda}\\). For example, using the values from our previous example:\n\\[\n\\begin{split}\n\\mathbf{a} = \\begin{bmatrix}2 \\\\ 3 \\\\ 0 \\\\ -1\\end{bmatrix} \\quad &\\mathrm{and} \\quad \\lambda = 3 \\\\[2ex]\n\\frac{1}{\\lambda}\\mathbf{a} &= \\frac{1}{3}\\begin{bmatrix}2 \\\\ 3 \\\\ 0 \\\\ -1\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix} \\frac{2}{3} \\\\ 1 \\\\ 0 \\\\ -\\frac{1}{3}\\end{bmatrix}\n\\end{split}\n\\]\n\n\n\n3.2.2 Geometry of Vector–Scalar Multiplication\nConsider the product \\(\\lambda \\mathbf{a}\\) where,\n\\[\n\\lambda = 2 \\quad \\mathrm{and} \\quad \\mathbf{a} = \\begin{bmatrix}\n2 \\\\ 3\n\\end{bmatrix}\n\\]\nThe product is\n\\[\n\\begin{bmatrix}\n4 \\\\ 6\n\\end{bmatrix}\n\\]\nTo show what happens geometrically when we multiply a vector by a scalar, let’s examine a plot of the original vector and the product.\n\n\n\n\n\nFigure 3.3: LEFT: Plot showing vector a in the R1–R2 dimensional space. RIGHT: Plot showing vector 2(a) in the R1–R2 dimensional space. For convenience we have located the tail of both vectors at the origin.\n\n\n\n\n\n\n\n\nMultiplying vector a by 2 doubled its length. However the direction of the new vector is the same as the original. In general, multiplying a vector by a positive scalar changes the length of a vector but not the direction. If it is multiplied by a negative scalar, not only does the length of the resulting vector change, but its direction is \\(180^\\circ\\) from the original. For example, Figure 3.4 shows vector a and the vector \\(\\lambda \\mathbf{a}\\) where \\(\\lambda = -0.5\\).\n\n\n\n\n\nFigure 3.4: LEFT: Plot showing vector a in the R1–R2 dimensional space. RIGHT: Plot showing vector -0.5(a) in the R1–R2 dimensional space. For convenience we have located the tail of both vectors at the origin.\n\n\n\n\n\n\n\n\nThe product vector is half the length of the original and is pointed in the complete opposite direction.",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Operations</span>"
    ]
  },
  {
    "objectID": "04-vector-operations.html#vectorvector-multiplication-dot-product",
    "href": "04-vector-operations.html#vectorvector-multiplication-dot-product",
    "title": "3  Vector Operations",
    "section": "3.3 Vector–Vector Multiplication: Dot Product",
    "text": "3.3 Vector–Vector Multiplication: Dot Product\nFor two column vectors, a and b each having n elements, the dot product (i.e., scalar product) is defined as:\n\\[\n\\mathbf{a} \\bullet \\mathbf{b} = \\sum_{i=1}^n a_ib_i\n\\]\nIn other words, the dot product is calculated by multiplying together the corresponding elements of each vector, and summing those products. Consider the vectors,\n\\[\n\\mathbf{a} = \\begin{bmatrix}5 \\\\ 4 \\\\ 7 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}1 \\\\ 0 \\\\ -1 \\\\ 2\\end{bmatrix}\n\\]\nThe dot product, or \\(\\mathbf{a} \\bullet \\mathbf{b}\\), is calculated as:\n\\[\n\\begin{split}\n\\mathbf{a} \\bullet \\mathbf{b} &= 5(1) + 4(0) + 7(-1) + 2(2) \\\\[2ex]\n&= 2\n\\end{split}\n\\]\nRemember that the result of a dot product is a scalar.\n\n\n3.3.1 Re-visiting Vector Length\nRecall that to find the length of a vector a with n-dimensions:\n\\[\n\\lvert\\lvert \\mathbf{a} \\rvert\\rvert = \\sqrt{a_1^2 + a_2^2 + a_3^2 + \\ldots + a_n^2}\n\\]\nThe sum under the square root is equivalent to computing the dot product of a with itself (i.e., \\(\\mathbf{a} \\bullet \\mathbf{a}\\)). Therefore, the length of an n-dimensional vector a can be found by computing the square root of the dot product between a and itself:\n\\[\n\\lvert\\lvert \\mathbf{a} \\rvert\\rvert = \\sqrt{\\mathbf{a} \\bullet \\mathbf{a}}\n\\]\n\n\n\n3.3.2 Dot Products Using the Special Vectors\nIn the previous chapter we introduced several special vectors, including zero vectors, ones vectors, and elementary vectors. It is useful to explore what happens when one of these special vectors is used to calculate an dot product.\nConsider finding the dot product between an n-dimensional vector a and an n-dimensional zero vector 0:\n\\[\n\\begin{split}\n\\mathbf{a} \\bullet \\mathbf{0} &= \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\bullet \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\\\[2ex]\n&= a_1(0) + a_2(0) + a_3(0) + \\ldots + a_n(0) \\\\[2ex]\n&= 0\n\\end{split}\n\\]\nThe dot product is 0. Now, consider finding the dot product between an n-dimensional vector a and an n-dimensional ones vector 1:\n\\[\n\\begin{split}\n\\mathbf{a} \\bullet \\mathbf{1} &= \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\bullet \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\\\[2ex]\n&= a_1(1) + a_2(1) + a_3(1) + \\ldots + a_n(1) \\\\[2ex]\n&= \\sum_{i=1}^n a_i\n\\end{split}\n\\]\nNotice that the dot product here is simply the sum of the elements in a. Because of this property, ones vectors are also sometimes referred to as sum vectors.\nNext, consider finding the dot product between an n-dimensional vector a and an n-dimensional elementary vector, say \\(\\mathbf{e}_1\\) in which the first element is 1 and the rest are 0:\n\\[\n\\begin{split}\n\\mathbf{a} \\bullet \\mathbf{e}_1 &= \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\bullet \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\\\[2ex]\n&= a_1(1) + a_2(0) + a_3(0) + \\ldots + a_n(0) \\\\[2ex]\n&= a_1\n\\end{split}\n\\]\nNotice that the dot product here is simply the same as the first element in a. In general, the dot product between an n-element column vector, a, and an n-element elementary vector, \\(\\mathbf{e}_i\\) with element \\(i=1\\), is:\n\\[\n\\mathbf{a} \\bullet \\mathbf{e}_i = a_i\n\\]",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Operations</span>"
    ]
  },
  {
    "objectID": "04-vector-operations.html#vector-operations-using-r",
    "href": "04-vector-operations.html#vector-operations-using-r",
    "title": "3  Vector Operations",
    "section": "3.4 Vector Operations Using R",
    "text": "3.4 Vector Operations Using R\nWe can also carry out these vector operations using R. Below we show how to carry out the operations of vector addition and vector subtraction.\n\n# Create vectors\na = matrix(data = c(5, 2), ncol = 1)\na\n\n     [,1]\n[1,]    5\n[2,]    2\n\nb = matrix(data = c(1, 3), ncol = 1)\nb\n\n     [,1]\n[1,]    1\n[2,]    3\n\n# Vector addition\na + b\n\n     [,1]\n[1,]    6\n[2,]    5\n\n# Vector subtraction\na - b\n\n     [,1]\n[1,]    4\n[2,]   -1\n\n\nMultiplication of a vector by a scalar is carried out using the * operator. This operator carries out element-wise multiplication; in this case it multiplies each element in the vector by the given scalar.\n\n# Create vectors\na = matrix(data = c(2, 3, 0, -1), ncol = 1)\na\n\n     [,1]\n[1,]    2\n[2,]    3\n[3,]    0\n[4,]   -1\n\n# Multiplication by 3\n3 * a\n\n     [,1]\n[1,]    6\n[2,]    9\n[3,]    0\n[4,]   -3\n\n# Division by 3 (this multiplies each element by the reciprical)\na / 3\n\n           [,1]\n[1,]  0.6666667\n[2,]  1.0000000\n[3,]  0.0000000\n[4,] -0.3333333\n\n\nLastly, although there is no dot product operator, we can mimic this function by using the elemement-wise multiplication operator to find the product of the corresponding elements of two vectors, and then use the sum() function to add those products together.\n\n# Create vectors\na = matrix(data = c(5, 4, 7, 2), ncol = 1)\na\n\n     [,1]\n[1,]    5\n[2,]    4\n[3,]    7\n[4,]    2\n\nb = matrix(data = c(1, 0, -1, 2), ncol = 1)\nb\n\n     [,1]\n[1,]    1\n[2,]    0\n[3,]   -1\n[4,]    2\n\n# Element-wise multiplication\na * b\n\n     [,1]\n[1,]    5\n[2,]    0\n[3,]   -7\n[4,]    4\n\n# Compute dot product\nsum(a * b)\n\n[1] 2\n\n# Compute length of a using dot product\nsqrt(sum(a * a))\n\n[1] 9.69536",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Operations</span>"
    ]
  },
  {
    "objectID": "04-vector-operations.html#exercises",
    "href": "04-vector-operations.html#exercises",
    "title": "3  Vector Operations",
    "section": "Exercises",
    "text": "Exercises\nConsider the following vectors:\n\\[\n\\mathbf{a} = \\begin{bmatrix}1 \\\\ 2 \\\\ 4 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}2 \\\\ 1 \\\\ -1\\end{bmatrix} \\qquad \\mathbf{c} = \\begin{bmatrix}5 \\\\ 2 \\\\ 3 \\end{bmatrix} \\qquad \\mathbf{d} = \\begin{bmatrix}2 \\\\ 1 \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{e} = \\begin{bmatrix}1 \\\\ 3 \\\\ 2 \\end{bmatrix}\n\\]\n\nFind \\(\\mathbf{a}\\bullet\\mathbf{b}\\).\n\n\nShow/Hide Solution\n\n\n\\[\n1(2) + 2(1) + 4(-1) = 0\n\\]\n\n\nFind \\(\\mathbf{c}\\bullet\\mathbf{c}\\).\n\n\nShow/Hide Solution\n\n\n\\[\n5(5) + 2(2) + 3(3) = 38\n\\]\n\n\nFind \\(\\mathbf{d}^{\\intercal}\\mathbf{e}\\) and \\(\\mathbf{e}^{\\prime}\\mathbf{d}\\)\n\n\nShow/Hide Solution\n\n\n\\(5(5) + 2(2) + 3(3) = 38\\)\n\nCreate a 2-dimensional orthogonal reference system. Show the following vector operations on that system.\n\n\\(\\mathbf{x}+\\mathbf{y}\\) where \\(\\mathbf{x} = \\begin{bmatrix}2 \\\\ -1 \\end{bmatrix}\\) and \\(\\mathbf{y} = \\begin{bmatrix}-3 \\\\ 2 \\end{bmatrix}\\)\n\n\nShow/Hide Solution\n\n\n\\[\n\\begin{split}\n\\mathbf{x} + \\mathbf{y} &= \\begin{bmatrix} 2 \\\\ -1\\end{bmatrix} + \\begin{bmatrix} -3 \\\\ 2\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix} -1 \\\\ 1\\end{bmatrix}\n\\end{split}\n\\]\n\n\n\n\n\nFigure 3.5: Plot showing the sum of vector a (in red) and vector b (in blue) in the R1–R2 dimensional space. For convenience we have located the tail of vector a at the origin. The vector that corresponds to the sum of a and b is shown in purple.\n\n\n\n\n\n\n\n\n\n\n\\(3\\mathbf{z}\\) where z is a unit vector in which all the elements are equal.\n\n\nShow/Hide Solution\n\n\n\n\n\n\n\n\n\n\n\n\nStudents’ scores on two exams ranged from 32 to 98 out of a possible 100 points on both exams. The scores for six students are shown below for Exam 1 (x) and Exam 2 (y).\n\\[\n\\mathbf{x} = \\begin{bmatrix}56 \\\\ 64 \\\\ 32 \\\\ 88 \\\\ 90 \\\\ 79\\end{bmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix}50 \\\\ 69 \\\\ 51 \\\\ 98 \\\\ 87 \\\\ 70\\end{bmatrix}\n\\]\n\nDetermine the total score for each student by finding \\(\\mathbf{z} = \\mathbf{x} + \\mathbf{y}\\).\n\n\nShow/Hide Solution\n\n\n\\[\n\\mathbf{z} = \\begin{bmatrix}56 + 50 \\\\ 64 + 69 \\\\ 32 + 51 \\\\ 88 + 98 \\\\ 90 + 87 \\\\ 79 + 70\\end{bmatrix} = \\begin{bmatrix}106 \\\\ 133 \\\\ 83 \\\\ 186 \\\\ 177 \\\\ 149\\end{bmatrix}\n\\]\n\n\nDetermine the mean score for each student by finding \\(\\frac{1}{2} \\mathbf{z}\\).\n\n\nShow/Hide Solution\n\n\n\\[\n\\frac{1}{2}\\mathbf{z} = \\frac{1}{2}\\begin{bmatrix}106 \\\\ 133 \\\\ 83 \\\\ 186 \\\\ 177 \\\\ 149\\end{bmatrix}= \\begin{bmatrix}53.0 \\\\ 66.5 \\\\ 41.5 \\\\ 93.0 \\\\ 88.5 \\\\ 74.5\\end{bmatrix}\n\\]\n\n\nIf the students took a third exam, and the scores for the same six students were presented in vector w, how would you write the algebraic expression (using vector notation) to obtain the students’ mean scores for all three exams?\n\n\nShow/Hide Solution\n\n\n\\[\n\\frac{1}{3}(\\mathbf{x} + \\mathbf{y} + \\mathbf{w})\n\\]\n\n\nIf the mean score on Exam 3 was 60, write the algebraic expression (using vector notation) to find the six students’ mean deviation scores on Exam 3.\n\n\nShow/Hide Solution\n\n\n\\[\n\\mathbf{w} - \\begin{bmatrix}60 \\\\ 60 \\\\ 60 \\\\ 60 \\\\ 60 \\\\ 60\\end{bmatrix}\n\\]",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Operations</span>"
    ]
  },
  {
    "objectID": "05-projection.html",
    "href": "05-projection.html",
    "title": "4  Vector Geometry: Angles, Projection, and Decomposition",
    "section": "",
    "text": "4.1 Angle Between Vectors\nIn this chapter you will learn some about some additional ideas in the geometry of vectors. Again, while the illustration of these concepts is restricted to 2-dimensional space, all of these ideas can be extended to n-dimensions.\nIt can be quite useful to determine the angle between two vectors. For example, what is the angle between vector a and b where,\n\\[\n\\mathbf{a} = \\begin{bmatrix}\n2 \\\\ 3\n\\end{bmatrix} \\quad \\mathrm{and} \\quad \\mathbf{b}= \\begin{bmatrix}\n-4 \\\\ 1\n\\end{bmatrix}\n\\]\nFigure 4.1 shows both vectors displayed in the same two-dimensional reference coordinate system.\nFigure 4.1: Plot showing two vector a (in red) and b (in blue) in the R1–R2 dimensional space. The angle between them is denoted as \\(\\theta\\). For convenience we have located the tail of both vectors at the origin.\nThe angle between these vectors is denoted as \\(\\theta\\), and can be found using the following:\n\\[\n\\cos (\\theta) = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert}\n\\]\nThat is, the cosine of the angle between the two vectors is equal to the dot product of the vectors divided by the product of their lengths. To find the angle (\\(\\theta\\)), we can compute the arc-cosine of this ratio. In our example,\n\\[\n\\begin{split}\n\\cos (\\theta) &= \\frac{-5}{\\sqrt{13}\\times\\sqrt{17}} \\\\[2ex]\n&= -0.336 \\\\[4ex]\n\\arccos(1.914) &= 1.914\n\\end{split}\n\\]\nand \\(\\theta=109.65^{\\circ}\\). Below is the R syntax to compute this angle.\n# Create vectors\na = matrix(data = c(2, 3), ncol = 1)\nb = matrix(data = c(-4, 1), ncol = 1)\n\n# Compute dot product between a and b\na_dot_b = sum(a * b)\n\n# Compute vector lengths\nl_a = sqrt(sum(a * a))\nl_b = sqrt(sum(b * b))\n\n# Compute theta (in radians)\nacos(a_dot_b / (l_a * l_b))\n\n[1] 1.91382\n\n# Compute theta (in degrees)\nacos(a_dot_b / (l_a * l_b)) * 180 / pi\n\n[1] 109.6538",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Geometry: Angles, Projection, and Decomposition</span>"
    ]
  },
  {
    "objectID": "05-projection.html#angle-between-vectors",
    "href": "05-projection.html#angle-between-vectors",
    "title": "4  Vector Geometry: Angles, Projection, and Decomposition",
    "section": "",
    "text": "MATH NOTE\nManipulating the formula to compute the angle between two vectors provides a common formula to determine the dot product between two vectors.\n\\[\n\\begin{split}\n\\cos (\\theta) &= \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\\\[2ex]\n\\mathbf{a}\\bullet\\mathbf{b} &= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert \\cos(\\theta)\n\\end{split}\n\\]\nThat is the dot product between vectors a and b is equal to the product of their magnitudes and the cosine of the angle between them.\n\n\n\n4.1.1 Orthogonal Vectors\nTwo vectors a and b are orthogonal when the angle between them is \\(90^\\circ\\). Since the cosine of a \\(90^\\circ\\) angle is 0, if a and b are orthogonal, then\n\\[\n0 = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert}\n\\]\nFor example, consider the following two elementary vectors\n\\[\n\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\qquad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1  \\end{bmatrix}\n\\]\nThe dot product between these two vectors is 0, which implies that the cosine of the angle between them must also be 0, indicating that \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) are orthogonal.\n\n\n\n\n4.1.2 Collinear Vectors\nTwo vectors a and b are collinear when the angle between them is \\(0^\\circ\\). Since the cosine of a \\(0^\\circ\\) angle is 1, if a and b are collinear, then\n\\[\n1 = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert}\n\\]\nFor example, consider the following two vectors\n\\[\n\\mathbf{a} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 6 \\\\ 3  \\end{bmatrix}\n\\]\n\\[\n\\begin{split}\n\\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} &= \\frac{14}{\\sqrt{5}\\times\\sqrt{45}} \\\\[2ex]\n&= \\frac{14}{\\sqrt{225}} \\\\[2ex]\n&= \\frac{14}{14} \\\\[2ex]\n&= 1\n\\end{split}\n\\]\nThis implies that a and b are collinear. Two vectors are collinear when one can be written as a linear combination of the other. In our example,\n\\[\n\\mathbf{b} = 3\\mathbf{a}\n\\]\nGeometrically, collinear vectors are parallel to one another (remember location in the reference space is a convenience).",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Geometry: Angles, Projection, and Decomposition</span>"
    ]
  },
  {
    "objectID": "05-projection.html#orthogonal-projection",
    "href": "05-projection.html#orthogonal-projection",
    "title": "4  Vector Geometry: Angles, Projection, and Decomposition",
    "section": "4.2 Orthogonal Projection",
    "text": "4.2 Orthogonal Projection\nOrthogonal projection of vector a on vector b occurs by dropping a perpendicular line from the terminus of a to intersect with x2.\n\n\n\n\n\nFigure 4.2: Orthogonal projection of vector a (vermillion) onto vector b (black). Note that the projection creates a 90-degree angle with vector b. The result of the projection is the vector p (blue).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe result is a vector p which is collinear with b but has a different length. To compute the length of p, we make use of the fact that the projection creates a right triangle having a hypotenuse of a and an adjacent leg of p to the angle \\(\\theta\\). Then,\n\\[\n\\begin{split}\n\\cos(\\theta) &= \\frac{\\lvert\\lvert\\mathbf{p}\\rvert\\rvert}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert} \\\\[2em]\n\\lvert\\lvert\\mathbf{p}\\rvert\\rvert &= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\cos(\\theta)\n\\end{split}\n\\]\nSince \\(\\theta\\) is the angle between a and b,\n\\[\n\\begin{split}\n\\lvert\\lvert\\mathbf{p}\\rvert\\rvert &= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\cos(\\theta) \\\\[2em]\n&= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert}\\\\[2em]\n&= \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{b}\\rvert\\rvert}\n\\end{split}\n\\]\nThat is, the magnitude of the projection p, is the ratio of the dot product between vectors a and b to the magnitude of b. Consider the following two vectors:\n\\[\n\\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 2 \\\\ -1  \\end{bmatrix}\n\\]\nProjecting a onto b, geometrically,\n\n\n\n\n\nFigure 4.3: Orthogonal projection of vector a (in red) onto vector b (in blue). The result of the projection is the vector p (in black).\n\n\n\n\n\n\n\n\nWe can find the magnitude of p using our formula:\n\\[\n\\begin{split}\n\\lvert\\lvert\\mathbf{p}\\rvert\\rvert &= \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{b}\\rvert\\rvert} \\\\[2ex]\n&= \\frac{1}{\\sqrt{5}} \\\\[2ex]\n&= 0.447\n\\end{split}\n\\]\n\n# Create vectors\na = c(2, 3)\nb = c(2, -1)\n\n# Compute dot product of a and b\na_dot_b = sum(a * b)\n\n# Compute length of b\nl_b = sqrt(sum(b * b))\n\n# Compute length of p\nl_p = a_dot_b / l_b\nl_p\n\n[1] 0.4472136\n\n\n\n\nNOTATION\nHenceforth, we will denote the projection of vector a onto vector b as:\n\\[\n\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}}\n\\]",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Geometry: Angles, Projection, and Decomposition</span>"
    ]
  },
  {
    "objectID": "05-projection.html#orthogonal-decomposition",
    "href": "05-projection.html#orthogonal-decomposition",
    "title": "4  Vector Geometry: Angles, Projection, and Decomposition",
    "section": "4.3 Orthogonal Decomposition",
    "text": "4.3 Orthogonal Decomposition\nOrthogonal projection of a vector results in a geometric decomposition of the vector into two additive components. Figure 4.4 illustrates the decomposition of vector a into two additive components, \\(\\mathbf{p}_1\\) and \\(\\mathbf{p}_2\\). That is,\n\\[\n\\mathbf{a} = \\mathbf{p}_1 + \\mathbf{p}_2\n\\]\n\n\n\n\n\nFigure 4.4: Two orthogonal projections of vector a (vermillion). The first orthogonal projection is from vector a onto vector b (horizontal black) and the secondorthogonal projection is from vector a is onto vector o (vertical black). The result of the projections are the vectors \\(\\mathbf{p}_1\\) (blue horizontal) and \\(\\mathbf{p}_2\\) (blue vertical).\n\n\n\n\n\n\n\n\nThe vector \\(\\mathbf{p}_1\\) is the same orthogonal projection from the earlier example, namely \\(\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}}\\). The vector \\(\\mathbf{p}_1\\) is a second projection, of a onto a vector o (\\(\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\)). The vector o is, by definition, orthogonal to b.\nThe lengths of the two projections correspond to the lengths of the sides of the right triangle where the hypotenuse is a. Namely1,\n\\[\n\\begin{split}\n\\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}}\\rvert\\rvert &= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\cos(\\theta) \\\\[2em]\n\\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\rvert\\rvert &= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\sin(\\theta)\n\\end{split}\n\\]\nAs an example, we can decompose a Using our two previous example vectors:\n\\[\n\\mathbf{a} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix} 2 \\\\ -1  \\end{bmatrix}\n\\]\nWe previously determined that \\(\\lvert\\lvert\\mathbf{a}\\rvert\\rvert=\\sqrt{13}\\), \\(\\lvert\\lvert\\mathbf{b}\\rvert\\rvert=\\sqrt{5}\\), and \\(\\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}}\\rvert\\rvert = 0.447\\). Recall,\n\\[\n\\cos(\\theta) = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\lvert\\lvert\\mathbf{b}\\rvert\\rvert}\n\\]\nThis implies that \\(\\cos(\\theta)=\\frac{1}{\\sqrt{65}}\\) and, taking the arc-cosine, that \\(\\theta = 82.87^\\circ\\) (or 1.45 radians). Using this value, we can compute the magnitude of the second projection as:\n\\[\n\\begin{split}\n\\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\rvert\\rvert &= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\sin(\\theta) \\\\[2em]\n&= \\sqrt{13} \\times \\sin(82.87^\\circ) \\\\[2em]\n&= 3.58\n\\end{split}\n\\]\n\n# Compute length of a\nl_a = sqrt(sum(a * a))\n\n# Compute theta (in radians)\ntheta = acos(a_dot_b / (l_a * l_b))\ntheta\n\n[1] 1.446441\n\n# Compute length of projection of a onto o\nl_p2 = l_a * sin(theta)\nl_p2\n\n[1] 3.577709\n\n\nWe can use the Pythagorean theorem to verify the computation of the two projections’ lengths. Since the square of the hypotenuse of a right triangle is the sum of the squares of the sides,\n\\[\n\\begin{split}\n\\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{b}}\\rvert\\rvert^2 +\\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\rvert\\rvert^2 &= \\lvert\\lvert\\mathbf{a}\\rvert\\rvert^2 \\\\[2em]\n3.58^2 + 0.447^2 &= (\\sqrt{13})^2 \\\\[2em]\n13 &= 13\n\\end{split}\n\\]\n\n# Compute sum of the squared projection lengths  \nl_p^2 + l_p2^2\n\n[1] 13\n\n# Compute length of a squared\nl_a^2\n\n[1] 13\n\n# Check values using Pythagorean Theorem\nl_a^2 == l_p^2 + l_p2^2\n\n[1] TRUE\n\n\nThe final system is shown in Figure 4.5.\n\n\n\n\n\nFigure 4.5: Orthogonal projection of vector a (in red) onto vector b (in blue). The result of the projection is the vector \\(\\mathbf{p}_1\\) (in black). A second projection is the vector \\(\\mathbf{p}_2\\) (in black) on to the vector o, which is orthogonal to b.",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Geometry: Angles, Projection, and Decomposition</span>"
    ]
  },
  {
    "objectID": "05-projection.html#footnotes",
    "href": "05-projection.html#footnotes",
    "title": "4  Vector Geometry: Angles, Projection, and Decomposition",
    "section": "",
    "text": "There are other ways to compute the length of \\(\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\). For example, since o is orthogonal to b, the angle between o and a is \\(\\phi=90-\\theta\\). Then \\(\\lvert\\lvert\\mathbf{p}_{\\mathbf{a}\\perp\\mathbf{o}}\\rvert\\rvert=\\lvert\\lvert\\mathbf{a}\\rvert\\rvert \\times \\cos(\\phi)\\).↩︎",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Geometry: Angles, Projection, and Decomposition</span>"
    ]
  },
  {
    "objectID": "06-statistical-application-vectors.html",
    "href": "06-statistical-application-vectors.html",
    "title": "5  Statistical Application: Vectors",
    "section": "",
    "text": "5.1 Deviation Scores and the Standard Deviation\nIn this chapter, we will provide examples of how vectors define the underlying geometry of various statistical summaries (standard deviation and correlation coefficient), including linear models. We will provide an example using a single predictor, but again, the ideas can be extended to models that include multiple predictors.\nConsider the following vector scores (x) and the vector of the mean deviation scores (\\(\\mathbf{d_x}\\)):\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\\end{bmatrix} \\qquad \\mathbf{d_x} = \\begin{bmatrix} x_1-\\bar{x} \\\\ x_2-\\bar{x} \\\\ x_3-\\bar{x} \\\\ \\vdots \\\\ x_n-\\bar{x}\\end{bmatrix}\n\\]\nRemember that the length of a vector is the square root of the dot product of the vector with itself. If we were to compute the length of the deviation score vector, the length would be the square root of the sum of squared deviations:\n\\[\n\\sqrt{\\mathbf{d_x}\\bullet \\mathbf{d_x}} = \\sqrt{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + (x_3 - \\bar{x})^2 + \\ldots + (x_n - \\bar{x})^2}\n\\]\nIf we divided this result by \\(\\sqrt{n}\\), this would be equivalent to the standard deviation of the scores in x (\\(s_\\mathbf{x}\\)).\n\\[\ns_\\mathbf{x} = \\frac{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert}{\\sqrt{n}}\n\\]\nSo there is a direct relation between the length of a deviation vector and the standard deviation, namely,\n\\[\n\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert = \\sqrt{n} (s_\\mathbf{x})\n\\]\nConsider the following vectors of data representing SAT scores (x) and GPA values (y) for \\(n=10\\) student:\n\\[\n\\mathbf{x} = \\begin{bmatrix}760 \\\\ 710 \\\\ 680 \\\\ 730 \\\\ 420 \\\\ 410 \\\\ 620 \\\\ 630 \\\\ 720 \\\\ 670\\end{bmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix} 3.8 \\\\ 2.4 \\\\ 2.6 \\\\ 3.1 \\\\ 1.9 \\\\ 1.7 \\\\ 2.5 \\\\ 2.4 \\\\ 3.5 \\\\ 3.1\\end{bmatrix}\n\\]\nWe compute the mean scores for each vector as \\(\\bar{x}=635\\) and \\(\\bar{y}=2.7\\), respectively. By subtracting the mean (which is a scalar) from each vector of data, we can create deviation vectors:\n\\[\n\\mathbf{d_x} = \\mathbf{x} - 635 = \\begin{bmatrix}125 \\\\ 75 \\\\ 45 \\\\ 95 \\\\ -215 \\\\ -225 \\\\ -15 \\\\ -5 \\\\ 85 \\\\ 35\\end{bmatrix} \\qquad \\mathbf{d_y} =\\mathbf{y}-2.7 = \\begin{bmatrix}1.1 \\\\ -0.3 \\\\ -0.1 \\\\ 0.4 \\\\ -0.8 \\\\ -1.0 \\\\ -0.2 \\\\ -0.3 \\\\ 0.8 \\\\ 0.4\\end{bmatrix}\n\\] The length of the deviation vectors are computed as \\(\\sqrt{\\mathbf{d}\\bullet\\mathbf{d}}\\), namely,\n\\[\n\\begin{split}\n\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert &= \\sqrt{\\mathbf{d_x}\\bullet\\mathbf{d_x}} \\\\[2em]\n\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert &= \\sqrt{\\mathbf{d_y}\\bullet\\mathbf{d_y}}\n\\end{split}\n\\]\nAnd using the values in the deviation vectors,\n\\[\n\\begin{split}\n\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert &= \\sqrt{137850} = 371.28 \\\\[2em]\n\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert &= \\sqrt{4.04} = 2.01\n\\end{split}\n\\]\nFinally, we can compute the standard deviations for x and y by dividing these lengths by \\(\\sqrt{n}\\).\n\\[\n\\begin{split}\ns_\\mathbf{x} &= \\frac{371.28}{\\sqrt{10}} = 117.41 \\\\[2em]\ns_\\mathbf{y} &= \\frac{4.04}{\\sqrt{10}} = 0.64\n\\end{split}\n\\]\n# Original vectors\nx = matrix(data = c(760, 710, 680, 730, 420, 410, 620, 630, 720, 670), ncol = 1)\ny = matrix(data = c(3.8, 2.4, 2.6, 3.1, 1.9, 1.7, 2.5, 2.4, 3.5, 3.1), ncol = 1)\n\n# Compute deviation vectors\nd_x = x - mean(x)\nd_x\n\n      [,1]\n [1,]  125\n [2,]   75\n [3,]   45\n [4,]   95\n [5,] -215\n [6,] -225\n [7,]  -15\n [8,]   -5\n [9,]   85\n[10,]   35\n\nd_y = y - mean(y)\nd_y\n\n      [,1]\n [1,]  1.1\n [2,] -0.3\n [3,] -0.1\n [4,]  0.4\n [5,] -0.8\n [6,] -1.0\n [7,] -0.2\n [8,] -0.3\n [9,]  0.8\n[10,]  0.4\n\n# Compute lengths of x deviation vector\nl_x = sqrt(sum(d_x * d_x))\nl_x\n\n[1] 371.2816\n\n# Compute lengths of y deviation vector\nl_y = sqrt(sum(d_y * d_y))\nl_y\n\n[1] 2.009975\n\n# Compute sd of x\ns_x = l_x / sqrt(10)\ns_x\n\n[1] 117.4095\n\n# Compute sd of y\ns_y = l_y / sqrt(10)\ns_y\n\n[1] 0.6356099\nNote that if we are using \\(s_\\mathbf{x}\\) as an estimate for the population parameter \\(\\sigma_\\mathbf{x}\\), that is x is a sample of student scores, then we need to divide the length of vector x by \\(n-1\\) rather than \\(n\\). In that case,\n\\[\n\\begin{split}\n\\hat\\sigma_\\mathbf{x} &= \\frac{371.28}{\\sqrt{9}} = 123.76 \\\\[2em]\n\\hat\\sigma_\\mathbf{y} &= \\frac{4.04}{\\sqrt{9}} = 0.67\n\\end{split}\n\\]\nThis is the denominator that is used in the sd() function in R.\n# Compute sd of x\nsigma_x = l_x / sqrt(9)\nsigma_x\n\n[1] 123.7605\n\n# Compute sd of y\nsigma_y = l_y / sqrt(9)\nsigma_y\n\n[1] 0.6699917\n\n# Check results\nsd(x)\n\n[1] 123.7605\n\nsd(y)\n\n[1] 0.6699917",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Application: Vectors</span>"
    ]
  },
  {
    "objectID": "06-statistical-application-vectors.html#vector-correlation-and-separation",
    "href": "06-statistical-application-vectors.html#vector-correlation-and-separation",
    "title": "5  Statistical Application: Vectors",
    "section": "5.2 Vector Correlation and Separation",
    "text": "5.2 Vector Correlation and Separation\nThe correlation between two vectors can also be expressed in terms of deviation scores:\n\\[\n\\begin{split}\nr_{xy} &= \\frac{\\mathrm{Cov}_{\\mathbf{xy}}}{s_\\mathbf{x} s_\\mathbf{y}} \\\\[2em]\n&= \\frac{\\frac{\\sum (\\mathbf{x}-\\bar{x})(\\mathbf{y}-\\bar{y})}{n}}{s_\\mathbf{x} s_\\mathbf{y}}\n\\end{split}\n\\]\nThe expression \\(\\sum (x-\\bar{x})(y-\\bar{y})\\) is equivalent to \\(\\mathbf{x}\\bullet\\mathbf{y}\\). Furthermore, we can re-write the standard deviations using our previous relationship with length of the deviation vectors. This implies,\n\\[\n\\begin{split}\nr_{xy} &= \\frac{\\frac{\\mathbf{d_x}\\bullet\\mathbf{d_y}}{n}}{\\frac{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert}{\\sqrt{n}}\\frac{\\lvert\\lvert\\mathbf{d_y}\\rvert\\rvert}{\\sqrt{n}}} \\\\[2em]\n&= \\frac{\\frac{\\mathbf{d_x}\\bullet\\mathbf{d_y}}{n}}{\\frac{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert~\\lvert\\lvert\\mathbf{d_y}\\rvert\\rvert}{n}} \\\\[2em]\n&= \\frac{\\mathbf{d_x}\\bullet\\mathbf{d_y}}{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert~\\lvert\\lvert\\mathbf{d_y}\\rvert\\rvert}\n\\end{split}\n\\] That is, the correlation between x and y is equal to the ratio between the dot product of the deviation vectors and the product of their lengths.1 In our example,\n\n# Compute correlation\nsum(d_x * d_y) / (l_x * l_y)\n\n[1] 0.8468822\n\n# Check with correlation function\ncor(x, y)\n\n          [,1]\n[1,] 0.8468822\n\n\nRecall that the angle between two vectors a and b, denoted \\(\\theta_{\\mathbf{ab}}\\), is given by,\n\\[\n\\cos (\\theta_{\\mathbf{ab}}) = \\frac{\\mathbf{a}\\bullet\\mathbf{b}}{\\lvert\\lvert\\mathbf{a}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{b}\\rvert\\rvert}\n\\]\nThus to compute the angle between the two deviation vectors (\\(\\theta\\)), this is:\n\\[\n\\cos (\\theta) = \\frac{\\mathbf{d_x}\\bullet\\mathbf{d_y}}{\\lvert\\lvert\\mathbf{d_x}\\rvert\\rvert\\times\\lvert\\lvert\\mathbf{d_y}\\rvert\\rvert}\n\\]\nThis is the same as the formula for the correlation! In other words,\n\\[\nr_{\\mathbf{xy}} = \\cos (\\theta)\n\\]\nThe correlation between two vectors x and y is equal to the cosine of the angle between their deviation vectors. Using this equality, we can find the angle between the deviation vectors for our example SAT scores and GPA values. Since \\(r_{\\mathbf{xy}}=0.847\\), that implies \\(\\theta\\approx32^\\circ\\).\n\n# Compute theta (in radians)\nacos(0.847)\n\n[1] 0.5604801\n\n# Compute theta (in degrees)\nacos(0.847) * 180 / pi\n\n[1] 32.11314\n\n\nThis correlation corresponds to an angle of approximately \\(32^\\circ\\) of separation between the deviation score vectors for SAT scores and GPA values in the 10-dimensional space defined by our 10 students.\n\nFYI\nWe can use the fact that \\(r_{\\mathbf{xy}} = \\cos (\\theta)\\) to make some connections between the value of the correlation coefficient and the angle between the deviation vectors.\n\nPerfect positive correlation (\\(r_{\\mathbf{xy}}=1\\)) indicates that deviation vectors are collinear. In this case \\(\\cos(\\theta)=1\\) which implies that \\(\\theta=0^\\circ\\).\nPerfect negative correlation (\\(r_{\\mathbf{xy}}=-1\\)) indicates that deviation vectors are in opposite directions. In this case \\(\\cos(\\theta)=-1\\) which implies that \\(\\theta=180^\\circ\\).\nPerfect lack of correlation (\\(r_{\\mathbf{xy}}=0\\)) indicates that deviation vectors are orthogonal. In this case \\(\\cos(\\theta)=0\\) which implies that \\(\\theta=90^\\circ\\).",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Application: Vectors</span>"
    ]
  },
  {
    "objectID": "06-statistical-application-vectors.html#orthogonal-decomposition-and-bivariate-regression",
    "href": "06-statistical-application-vectors.html#orthogonal-decomposition-and-bivariate-regression",
    "title": "5  Statistical Application: Vectors",
    "section": "5.3 Orthogonal Decomposition and Bivariate Regression",
    "text": "5.3 Orthogonal Decomposition and Bivariate Regression\nRecall that the simple regression model seeks to explain variation in an outcome variable Y using a predictor variable X. Mathematically, the model fitted is expressed as:\n\\[\n\\mathbf{y} = b_0 + b_1(\\mathbf{x}) + \\mathbf{e}\n\\]\nwhere y is a vector of fitted values, \\(b_0\\) and \\(b_1\\) are scalars produced from the OLS estimation, x is a vector of the predictor values, and e is a vector of residuals. Recall that when we have mean centered the outcome and predictor, the intercept (\\(b_0\\)) drops out of this equation. we can express this as:\n\\[\n\\mathbf{y} - \\bar{y} = b_1(\\mathbf{x} - \\bar{x}) + \\mathbf{e}\n\\]\nThe outcome and predictors are now expressed as deviation vectors, say\n\\[\n\\mathbf{d}_\\mathbf{y} = b_1(\\mathbf{d}_\\mathbf{x} ) + \\mathbf{e}\n\\] where \\(\\hat{\\mathbf{d}}_\\mathbf{y}=b_1(\\mathbf{d}_\\mathbf{x} )\\). That is, we can partition the deviation vector of y into two components:\n\\[\n\\mathbf{d}_\\mathbf{y} = \\hat{\\mathbf{d}}_\\mathbf{y} + \\mathbf{e}\n\\]\nOrdinary least squares (OLS) estimation determines the value of \\(b_1\\) by minimizing the sum of squared residuals, that is, it minimizes the quantity \\(\\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2\\). Geometrically, minimizing \\(\\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2\\) is determining the orthogonal projection of \\(\\mathbf{d}_\\mathbf{y}\\) onto \\(\\mathbf{d}_\\mathbf{x}\\).2 This projection is the first component of the partitioning of the deviation vector described previously, \\(\\hat{\\mathbf{d}}_\\mathbf{y}=b_1(\\mathbf{d}_\\mathbf{x})\\), which is collinear with \\(\\mathbf{d}_\\mathbf{x}\\) since \\(b_1(\\mathbf{d}_\\mathbf{x})\\) is a scalar multiple of \\(\\mathbf{d}_\\mathbf{x}\\) This is shown in Figure 5.1.\n\n\n\n\n\nFigure 5.1: The two orthogonal projections from the deviation vector of y form the basis for the model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the e vector.\n\n\n\n\n\n\n\n\nThe ‘triangle’ formed by the vectors \\(\\hat{\\mathbf{d}}_\\mathbf{y}\\), e, and \\(\\mathbf{d}_\\mathbf{y}\\) is referred to as the model triangle. Figure 5.2 shows the model triangle. (Remember e can be moved to the right-side of the triangle since location is not a vector property.)\n\n\n\n\n\nFigure 5.2: The model triangle (yellow). Note that the vector making up the right side of the model triangle is the same as the e vector.\n\n\n\n\n\n\n\n\nThe geometry of this triangle is the same as the geometry visualizing the sum of these vectors, namely\n\\[\n\\mathbf{d}_\\mathbf{y} = \\hat{\\mathbf{d}}_\\mathbf{y} + \\mathbf{e}\n\\]\nNamely that the vectors that create the legs of the model triangle can be added together to create the \\(\\mathbf{d}_\\mathbf{y}\\) hypotenuse vector. The e vector is also the second orthogonal projection vector, \\(\\mathbf{e} = \\mathbf{p}_{\\mathbf{d}_\\mathbf{y}\\perp\\mathbf{o}}\\). By definition, this means that the vector of residuals (e) is orthogonal to the vector of fitted values (\\(\\hat{\\mathbf{d}}_\\mathbf{y}\\)), which means, the correlation between those two vectors is zero.\n\\[\nr_{\\mathbf{e},\\hat{\\mathbf{d}}_\\mathbf{y}} = 0\n\\]\nIt also means that the model triangle is a right triangle, whose side lengths are governed by the Pythagorean Theorem.\n\\[\n\\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert^2 = \\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert^2 + \\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2\n\\]\nExpressing these lengths using the deviations we get\n\\[\n\\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 + \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\nThis is the partitioning that describes the ANOVA decomposition:\n\\[\n\\mathrm{Total~SS} = \\mathrm{Model~SS} + \\mathrm{Residual~SS}\n\\]\nEach of these sum of squares is the squared length of one of the vectors in the model triangle.\n\\[\n\\begin{split}\n\\mathrm{Total~SS} &= \\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert^2 \\\\[2em]\n\\mathrm{Model~SS} &= \\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert^2 \\\\[2em]\n\\mathrm{Residual~SS} &= \\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2\n\\end{split}\n\\]\nRelatedly, the lengths of the vectors making up the model triangle are the square roots of the these sum of square terms:\n\\[\n\\begin{split}\n\\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert &= \\sqrt{\\mathrm{Total~SS}} \\\\[2em]\n\\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert &= \\sqrt{\\mathrm{Model~SS}} \\\\[2em]\n\\lvert\\lvert\\mathbf{e}\\rvert\\rvert &= \\sqrt{\\mathrm{Residual~SS}}\n\\end{split}\n\\]\n\n\n\n\n\nFigure 5.3: The side lengths of the model triangle (yellow) correspond to the square roots of the sum of square terms used in the ANOVA decomposition.\n\n\n\n\n\n\n\n\nLastly, since the model-level \\(R^2\\) value is defined as \\(R^2 = \\frac{\\mathrm{Model~SS}}{\\mathrm{Total~SS}}\\) and there is only a single predictor in the model then,\n\\[\n\\begin{split}\nr_{\\mathbf{xy}} &= \\sqrt{R^2} \\\\[2em]\n&= \\sqrt{\\frac{\\mathrm{Model~SS}}{\\mathrm{Total~SS}}} \\\\[2em]\n&= \\frac{\\sqrt{\\mathrm{Model~SS}}}{\\sqrt{\\mathrm{Total~SS}}} \\\\[2em]\n&= \\frac{\\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert}{\\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert}\n\\end{split}\n\\]\nThat is, the correlation coefficient between x and y is equivalent to the ratio of the lengths between the orthogonal projection vector collinear with the deviation vector of x and the deviation vector of y. Note this is also exactly how we compute cosine of \\(\\theta\\).\n\n\n5.3.1 Back to the SAT and GPA Example\nHere we return to our GPA and SAT data to provide an example of these computations. Suppose we want to predict SAT (y) from GPA (x), employing deviation scores. We begin by computing the length of both deviation vectors:\n\\[\n\\begin{split}\n\\lvert\\lvert\\mathbf{d}_\\mathbf{x}\\rvert\\rvert &= \\sqrt{137850} = 371.28\\\\[2em]\n\\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert &= \\sqrt{4.04} = 2.01\n\\end{split}\n\\]\n\n# Compute length of deviation vector d_x\nsqrt(sum(l_x * l_x))\n\n[1] 371.2816\n\n# Compute length of deviation vector d_y\nsqrt(sum(l_y * l_y))\n\n[1] 2.009975\n\n\nWe can also compute the correlation coefficient between SAT scores and GPAs by finding the cosine of the angle between the two vectors, \\(r = 0.847\\). Taking the arc-cosine, we find that \\(\\theta = 32.12^\\circ\\).\n\n# Compute correlation\nr = sum(d_x * d_y) / (l_x * l_y)\nr\n\n[1] 0.8468822\n\n\n\n\n\n\n\nFigure 5.4: The model triangle (yellow) for our regression of GPA (y) onto SAT scores (x).\n\n\n\n\n\n\n\n\nWe can now use the definition of cosine and sine to compute the lengths of the two orthogonal projections from \\(\\mathbf{d}_\\mathbf{y}\\). The length of the projection onto \\(\\mathbf{d}_\\mathbf{x}\\) is calculated as:\n\\[\n\\begin{split}\n\\cos(\\theta) &= \\frac{\\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert}{\\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert} \\\\[2em]\n0.847 &= \\frac{\\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert}{2.01} \\\\[2em]\n\\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert &= 1.70\n\\end{split}\n\\]\nThe length of the second projection onto o is calculated as:\n\\[\n\\begin{split}\n\\sin(\\theta) &= \\frac{\\lvert\\lvert\\mathbf{e}\\rvert\\rvert}{\\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert} \\\\[2em]\n0.531 &= \\frac{\\lvert\\lvert\\mathbf{e}\\rvert\\rvert}{2.01} \\\\[2em]\n\\lvert\\lvert\\mathbf{e}\\rvert\\rvert &= 1.07\n\\end{split}\n\\]\n\n# Compute length of projection on d_x\nr * l_y\n\n[1] 1.702212\n\n# Compute length of projection on o\nsin(acos(r)) * l_y\n\n[1] 1.068866\n\n\nFigure 5.5 shows the model triangle for the regression of GPA (y) onto SAT scores (x) with all of the computed side lengths.\n\n\n\n\n\nFigure 5.5: The model triangle (yellow) for our regression of GPA (y) onto SAT scores (x).\n\n\n\n\n\n\n\n\nFinally, we can use these lengths, along with the length of \\(\\mathbf{d}_\\mathbf{y}\\) to write out the ANOVA decomposition; the partitioning of the sum of squares.\n\\[\n\\begin{split}\n\\mathrm{Total~SS} &= \\lvert\\lvert\\mathbf{d}_\\mathbf{y}\\rvert\\rvert^2 = 2.01^2 = 4.04\\\\[2em]\n\\mathrm{Model~SS} &= \\lvert\\lvert\\hat{\\mathbf{d}}_\\mathbf{y}\\rvert\\rvert^2 = 1.70^2 = 2.90\\\\[2em]\n\\mathrm{Residual~SS} &= \\lvert\\lvert\\mathbf{e}\\rvert\\rvert^2 = 1.07^2 = 1.14\n\\end{split}\n\\]\nThese sums of squares are additive within rounding:\n\\[\n\\begin{split}\n\\mathrm{Total~SS} &= \\mathrm{Model~SS} + \\mathrm{Residual~SS} \\\\[2em]\n4.04 &= 2.90 + 1.14\n\\end{split}\n\\]\nUsing these values we can also compute the model-level \\(R^2\\).\n\\[\nR^2 = \\frac{\\mathrm{Model~SS}}{\\mathrm{Total~SS}} =  \\frac{2.90}{4.04} = 0.718\n\\]\nThat is, differences in SAT scores explain 71.8% of the variation in GPAs.",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Application: Vectors</span>"
    ]
  },
  {
    "objectID": "06-statistical-application-vectors.html#footnotes",
    "href": "06-statistical-application-vectors.html#footnotes",
    "title": "5  Statistical Application: Vectors",
    "section": "",
    "text": "Most formulas for the correlation will use \\(n-1\\) rather than \\(n\\), but here it doesn’t matter as the \\(n\\)s drop out when we reduce this.↩︎\nRemember from geometry that the shortest distance from a point (at the end of the \\(\\mathbf{d}_\\mathbf{y}\\)) to a line (spanned by \\(\\mathbf{d}_\\mathbf{x}\\)) is the perpendicular line segment between them.↩︎",
    "crumbs": [
      "Vectors and Vector Operations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Application: Vectors</span>"
    ]
  },
  {
    "objectID": "07-matrices.html",
    "href": "07-matrices.html",
    "title": "6  Matrices",
    "section": "",
    "text": "6.0.1 Dimensions of a Matrix\nA matrix is a rectangular array of elements arranged in rows and columns. We typically denote matrices using a bold-faced, upper-case letter. For example, consider the matrix B which has 3 rows and 2 columns:\n\\[\n\\mathbf{B} = \\begin{bmatrix}\n5 & 1 \\\\\n7 & 3 \\\\\n-2 & -1\n\\end{bmatrix}\n\\]\nIn psychometric and statistical applications, the data we work with typically have this type of rectangular arrangement. For example, the data in Table 6.1, which includes measures of 5 variables for 100 students, is arranged into the familiar case-by-variable rectangular ‘data matrix’.1\nWe define matrices in terms of their dimensions or order; the number of rows and columns within the matrix. The dimensions of the matrix B is \\(3\\times 2\\); it has three rows and two columns. Whereas the dimensions of the data matrix is \\(100\\times 5\\). We often denote the matrix’s dimension by appending it to the bottom of the matrix’s name,\n\\[\n\\underset{3\\times 2}{\\mathbf{B}} = \\begin{bmatrix}\n5 & 1 \\\\\n7 & 3 \\\\\n-2 & -1\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "07-matrices.html#matrix-equality",
    "href": "07-matrices.html#matrix-equality",
    "title": "6  Matrices",
    "section": "6.1 Matrix Equality",
    "text": "6.1 Matrix Equality\nTwo matrices are said to be equal if they satisfy two conditions:\n\nThey have the same dimensions, and\nAll corresponding elements are equal.\n\nConsider the following matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n112 & 86 & 0 \\\\ 134 & 94 & 0\n\\end{bmatrix} \\qquad\n\\mathbf{B} = \\begin{bmatrix}\n112 & 134 \\\\86 & 94 \\\\ 0 & 0\n\\end{bmatrix} \\qquad\n\\mathbf{C} = \\begin{bmatrix}\n112 & 86 & 0 \\\\ 134 & 94 & 0\n\\end{bmatrix}\n\\]\nWithin this set of matrices,\n\n\\(\\mathbf{A} \\neq \\mathbf{B}\\), since they do not have the same dimensions. A is a \\(2\\times3\\) matrix and B is a \\(3\\times2\\) matrix.\nSimilarly, \\(\\mathbf{A} \\neq \\mathbf{B}\\), they also do not have the same dimensions.\n\\(\\mathbf{A} = \\mathbf{C}\\) since both matrices have the same dimensions and all corresponding elements are equal.",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "07-matrices.html#exercises",
    "href": "07-matrices.html#exercises",
    "title": "6  Matrices",
    "section": "Exercises",
    "text": "Exercises\nConsider the following matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}5 & 6 & 4& 5 & 9 \\\\21 & 23 & 24 & 22 & 20 \\end{bmatrix} \\qquad \\mathbf{b}^\\intercal = \\begin{bmatrix}0 & 1 & 0 & 0 \\end{bmatrix} \\qquad \\mathbf{Y} = \\begin{bmatrix}2 & 3 & 1 \\\\5 & 6 & 8\\\\9 & 4 & 7 \\end{bmatrix}\n\\]\n\nUse notation to describe the dimensions of A.\n\n\nShow/Hide Solution\n\n\n\\[\n\\underset{2\\times5}{\\mathbf{A}}\n\\]\n\n\nUse notation to describe the dimensions of b.\n\n\nShow/Hide Solution\n\n\n\\[\n\\underset{4\\times1}{\\mathbf{b}}\n\\]\n\n\nUse notation to describe the dimensions of Y.\n\n\nShow/Hide Solution\n\n\n\\[\n\\underset{3\\times3}{\\mathbf{Y}}\n\\]\n\nThe matrix X contains data from four students’ of four different course exams.\n\\[\n\\mathbf{X} = \\begin{bmatrix}32 & 54 & 56 & 21 \\\\42 & 23 & 52 & 35 \\\\ 16 & 41 & 54 & 56 \\\\ 58 & 52 & 31 & 24 \\end{bmatrix}\n\\]\n\nWhich student obtained a 35 and on which test? Report the row and column using subscript notation.\n\n\nShow/Hide Solution\n\n\n\\[\nX_{2,4}=35\n\\]\nThe 2nd student received a 35 on the 4th exam.\n\n\nUsing subscript notation, indicate the 4th student’s score on the 1st exam.\n\n\nShow/Hide Solution\n\n\n\\[\nX_{4,1}=58\n\\]",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "07-matrices.html#footnotes",
    "href": "07-matrices.html#footnotes",
    "title": "6  Matrices",
    "section": "",
    "text": "In computation, a matrix is a very particular type of data structure in which every column has the same type of data (e.g., every column is numeric, or every column is a character variable). A data frame is a more generalized computational structure that accommodates multiple types of columns. In some computational languages this structure is referred to as a data matrix.↩︎",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "10-matrix-addition-subtraction.html",
    "href": "10-matrix-addition-subtraction.html",
    "title": "7  Matrix Addition and Subtraction",
    "section": "",
    "text": "7.1 Properties of Matrix Addition\nTwo or more matrices can be added or subtracted if they have the same dimensions; if not, matrix addition and subtraction is undefined. Just as in vector addition, each corresponding element is added or subtracted and placed in the corresponding location in the new matrix. Consider the following two matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n112 & 86 & 0 \\\\ 134 & 94 & 0\n\\end{bmatrix} \\qquad\n\\mathbf{B} = \\begin{bmatrix}\n101 & 89 & 1 \\\\110 & 90 & 0\n\\end{bmatrix}\n\\]\nThen\n\\[\n\\begin{split}\n\\mathbf{A} + \\mathbf{B} &= \\begin{bmatrix}\n112 & 86 & 0 \\\\ 134 & 94 & 0\n\\end{bmatrix} + \\begin{bmatrix}\n101 & 89 & 1 \\\\110 & 90 & 0\n\\end{bmatrix} \\\\[2em]\n&= \\begin{bmatrix}\n112 + 101 & 86+89 & 0+1 \\\\ 134+110 & 94+90 & 0+0\n\\end{bmatrix} \\\\[2em]\n&= \\begin{bmatrix}\n213 & 175 & 1 \\\\ 244 & 184 & 0\n\\end{bmatrix}\n\\end{split}\n\\]\nIn general, the elements in the summed matrix, C, are defined as \\(\\mathbf{C}_{ij}=\\mathbf{A}_{ij} + \\mathbf{B}_{ij}\\) for all i and j. Since subtraction is equivalent to adding the inverse, subtracting the elements of B from A is equivalent to adding the inverted elements of B to the elements of A (where “inverting the elements” means switching the sign on each element).\n\\[\n\\begin{split}\n\\mathbf{A} - \\mathbf{B} &= \\begin{bmatrix}\n112 & 86 & 0 \\\\ 134 & 94 & 0\n\\end{bmatrix} + \\begin{bmatrix}\n-101 & -89 & -1 \\\\-110 & -90 & 0\n\\end{bmatrix} \\\\[2em]\n&= \\begin{bmatrix}\n11 & -3 & -1 \\\\ 24 & 4 & 0\n\\end{bmatrix}\n\\end{split}\n\\]\nComputationally, we can use the + and - operators in R to add and subtract matrices.\nMatrix addition satisfies both the commutative and associative properties. That is,\n\\[\n\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}\n\\]\nand\n\\[\n\\begin{split}\n\\mathbf{A} + (\\mathbf{B} + \\mathbf{C}) &= (\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} \\\\[2ex]\n&= \\mathbf{A} + \\mathbf{B} + \\mathbf{C}\n\\end{split}\n\\]",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Matrix Addition and Subtraction</span>"
    ]
  },
  {
    "objectID": "10-matrix-addition-subtraction.html#statistical-application-deviation-matrix",
    "href": "10-matrix-addition-subtraction.html#statistical-application-deviation-matrix",
    "title": "7  Matrix Addition and Subtraction",
    "section": "7.2 Statistical Application: Deviation Matrix",
    "text": "7.2 Statistical Application: Deviation Matrix\nWe have seen that deviation scores are particularly useful in statistics. We can create a deviation matrix by taking a score matrix and subtracting from it a matrix of means, where each column contains the mean for each corresponding column in the original matrix, that is:\n\\[\n\\mathbf{D} = \\mathbf{X} - \\mathbf{M}\n\\]\nFor example, consider the following score matrix:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n3.0 & 11 & 112 \\\\ 3.9 & 10 & 143 \\\\ 2.9 & 19 & 124 \\\\ 2.7 & 7 & 129\n\\end{bmatrix}\n\\] To compute the mean deviation matrix, D we use:\n\\[\n\\begin{split}\n\\mathbf{D} &= \\mathbf{X} - \\mathbf{M} \\\\[2em]\n&= \\begin{bmatrix}\n3.0 & 11 & 112 \\\\ 3.9 & 10 & 143 \\\\ 2.9 & 19 & 124 \\\\ 2.7 & 7 & 129\n\\end{bmatrix} - \\begin{bmatrix}\n3.125 & 11.75 & 127 \\\\ 3.125 & 11.75 & 127 \\\\ 3.125 & 11.75 & 127 \\\\ 3.125 & 11.75 & 127\n\\end{bmatrix} \\\\[2em]\n&= \\begin{bmatrix}\n-0.125 & -0.75 & -15 \\\\ 0.775 & -1.75 & 16 \\\\ -0.225 & 7.25 & -3 \\\\ -0.425 & -4.75 & 2\n\\end{bmatrix}\n\\end{split}\n\\]\nComputationally, we can use the colMeans() function to compute the mean in each column of X. Then we can create matrix M by using the rep() function to repeat this set of means four times. (There are four rows in M that have the exact same elements.)\n\n# Create X\nX = matrix(data = c(3.0, 11, 112, 3.9, 10, 143, 2.9, 19, 124, 2.7, 7, 129), byrow = TRUE, nrow = 4)\nX\n\n     [,1] [,2] [,3]\n[1,]  3.0   11  112\n[2,]  3.9   10  143\n[3,]  2.9   19  124\n[4,]  2.7    7  129\n\n# Compute column means\ncolMeans(X)\n\n[1]   3.125  11.750 127.000\n\n# Create M\nM = matrix(rep(colMeans(X), 4), byrow = TRUE, nrow =4)\nM\n\n      [,1]  [,2] [,3]\n[1,] 3.125 11.75  127\n[2,] 3.125 11.75  127\n[3,] 3.125 11.75  127\n[4,] 3.125 11.75  127\n\n# Compute deviation matrix\nX - M\n\n       [,1]  [,2] [,3]\n[1,] -0.125 -0.75  -15\n[2,]  0.775 -1.75   16\n[3,] -0.225  7.25   -3\n[4,] -0.425 -4.75    2",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Matrix Addition and Subtraction</span>"
    ]
  },
  {
    "objectID": "11-matrix-multiplication.html",
    "href": "11-matrix-multiplication.html",
    "title": "8  Matrix Multiplication",
    "section": "",
    "text": "8.1 Scalar–Matrix Multiplication\nIn this chapter you will learn about scalar-matrix multiplication and matrix-matrix multiplication.\nAny matrix can be multiplied by a scalar. The result is a matrix in which each element in the original matrix is multiplied by the value of the scalar. For example to multiply matrix A (from the previous section) by a scalar \\(\\lambda = -2\\),\n\\[\n\\begin{split}\n-2\\mathbf{A} &= -2\\begin{bmatrix}\n112 & 86 & 0 \\\\ 134 & 94 & 0\n\\end{bmatrix} \\\\[2em]\n&= \\begin{bmatrix}\n-2(112) & -2(86) & -2(0) \\\\ -2(134) & -2(94) & -2(0)\n\\end{bmatrix} \\\\[2em]\n&= \\begin{bmatrix}\n-224 & -172 & 0 \\\\ -268 & -188 & 0\n\\end{bmatrix}\n\\end{split}\n\\] In general the elements of the product matrix, \\(\\lambda\\mathbf{A}\\), are \\(\\lambda (a_{ij})\\) for every i and j. Scalar–matrix multiplication is commutative, that is:\n\\[\n\\lambda\\mathbf{A} = \\mathbf{A}\\lambda\n\\]\nIn R we use the * operator to perform scalar–matrix multiplication.\n# Create A\nA = matrix(data = c(112, 86, 0, 134, 94, 0), byrow = TRUE, nrow = 2)\n\n#View A\nA\n\n     [,1] [,2] [,3]\n[1,]  112   86    0\n[2,]  134   94    0\n\n# Scalar multiplication\n-2 * A\n\n     [,1] [,2] [,3]\n[1,] -224 -172    0\n[2,] -268 -188    0",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrix Multiplication</span>"
    ]
  },
  {
    "objectID": "11-matrix-multiplication.html#matrix-multiplication",
    "href": "11-matrix-multiplication.html#matrix-multiplication",
    "title": "8  Matrix Multiplication",
    "section": "8.2 Matrix Multiplication",
    "text": "8.2 Matrix Multiplication\nThe process of multiplying matrices follows the same basic principle as vector multiplication, where we consider matrices to be a collection of column vectors. When we multiply vectors, they must have the same number of elements because we multiply corresponding elements and add the resulting products, obtaining a scalar (dot) product (i.e., \\(\\mathbf{a}\\bullet\\mathbf{b}\\)). To multiply matrix X by matrix Y, we are going to compute the dot product between each row in X and each column in Y, that is, \\(\\mathbf{x}^\\intercal_{j}\\bullet\\mathbf{b}_j\\). As an example, consider the following two matrices:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n2 & 3 \\\\ 1 & 2\n\\end{bmatrix} \\qquad \\mathbf{Y} = \\begin{bmatrix}\n5 & -2 \\\\ 4 & -1\n\\end{bmatrix}\n\\]\nTo complete the multiplication of XY, we compute the dot product between the first row of X and the first column of Y (i.e., \\(\\mathbf{X}_1^\\intercal\\bullet\\mathbf{Y}_1\\)).\n\\[\n\\begin{split}\n\\begin{bmatrix}2 & 3\\end{bmatrix}\\begin{bmatrix}5 \\\\ 4\\end{bmatrix} &= 2(5) + 3(4)\\\\[2em]\n&= 22\n\\end{split}\n\\]\nThis gives us the element in the first row and first column of the product matrix, Z. The element in the first row and second column of Z is based on the dot product between the the first row of X and the second column of Y. This continues, finding the dot product between rows of X and columns of Y for each corresponding element in Z.\n\\[\n\\begin{split}\n\\mathbf{Z} &= \\begin{bmatrix}\\begin{bmatrix}2 & 3\\end{bmatrix}\\begin{bmatrix}5 \\\\ 4\\end{bmatrix} & \\begin{bmatrix}2 & 3\\end{bmatrix}\\begin{bmatrix}-2 \\\\ -1\\end{bmatrix} \\\\ \\begin{bmatrix}1 & 2\\end{bmatrix}\\begin{bmatrix}5 \\\\ 4\\end{bmatrix} & \\begin{bmatrix}1 & 2\\end{bmatrix}\\begin{bmatrix}-2 \\\\ -1\\end{bmatrix}\\end{bmatrix}\\\\[2em]\n&= \\begin{bmatrix}\n22 & -7 \\\\ 13 & -4\n\\end{bmatrix}\n\\end{split}\n\\]\n\nIMPORTANT\nThis process must always be carefully followed: Finding the dot product between a row of the first matrix and a column of the second matrix. The general rule on matrix multiplication for \\(\\mathbf{AB} = \\mathbf{C}\\), for each element in C, \\(C_{ij}\\) is the dot product of the ith row of A and the jth column of B.\n\nTo compute the product between two matrices we use the matrix multiplication operator, %*%.\n\n# Create A\nA = matrix(data = c(2, 3, 1, 2), byrow = TRUE, nrow = 2)\n\n# Create B\nB = matrix(data = c(5, -2, 4, -1), byrow = TRUE, nrow = 2)\n\n# Matrix multiplication\nA %*% B\n\n     [,1] [,2]\n[1,]   22   -7\n[2,]   13   -4\n\n\nIn contrast to scalar multiplication, matrix multiplication is rarely commutative, that is, it is rare that \\(\\mathbf{AB}=\\mathbf{BA}\\). In our previous example, \\(\\mathbf{AB}\\neq\\mathbf{BA}\\).\n\n# Matrix multiplication\nB %*% A\n\n     [,1] [,2]\n[1,]    8   11\n[2,]    7   10\n\n\nThe resulting product is not the same as computing AB. The order of the multiplication is quite important, so rather than saying that “A is multiplied by B”, we instead specify the order by saying:\n\nA is postmultiplied by B; or\nB is premultiplied by A.\n\n\n\n8.2.1 Conformability\nWe can also multiply matrices that are not of equal dimensions, as long as they are conformable. To be conformable, the number of columns in the premultiplied (first) matrix must equal the number of rows in the postmultiplied (second) matrix. For example, matrices A and B (below) are conformable since the number of columns in A is equal to the number of rows in B, namely 4.\n\\[\n\\underset{2\\times4}{\\mathbf{A}} = \\begin{bmatrix}\n5 & 2 & 3 & 4 \\\\ 5 & 4 & 6 & 1\n\\end{bmatrix} \\qquad \\underset{4\\times3}{\\mathbf{B}} = \\begin{bmatrix}\n8 & 9 & 4 \\\\ 6 & 5 & 1 \\\\ 2 & 3 & 4 \\\\ 6 & 1 & 2\n\\end{bmatrix}\n\\]\nOne way to consider conformability is to ensure that the inner dimensions of the two matrices being multiplied are equal. Here the inner dimensions (shown in red) are both 4.\n\\[\n\\underset{2\\times\\color{red}{4}}{\\mathbf{A}}~\\underset{{\\color{red}{4}} \\times3}{\\mathbf{B}}\n\\]\n\n# Create A\nA = matrix(data = c(5, 2, 3, 4, 5, 4, 6, 1), byrow = TRUE, nrow = 2)\n\n# Create B\nB = matrix(data = c(8, 9, 4, 6, 5, 1, 2, 3, 4, 6, 1, 2), byrow = TRUE, nrow = 4)\n\n# Postmultiply A by B\nA %*% B\n\n     [,1] [,2] [,3]\n[1,]   82   68   42\n[2,]   82   84   50\n\n\nNotice the product AB has dimensions of \\(2\\times4\\), which are the outer dimensions shown in the product (below in red).\n\\[\n\\underset{{\\color{red}{2}} \\times4}{\\mathbf{A}}~\\underset{4\\times\\color{red}{3}}{\\mathbf{B}}\n\\]\nIf we tried to postmultiply B by A, that product is not conformable since the inner dimensions would no longer match (i.e., \\(3\\neq2\\)).\n\\[\n\\underset{4\\times\\color{red}{3}}{\\mathbf{B}} ~ \\underset{{\\color{red}{2}}\\times4}{\\mathbf{A}}\n\\]\n\n# Postmultiply B by A\nB %*% A\n\nError in B %*% A: non-conformable arguments",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrix Multiplication</span>"
    ]
  },
  {
    "objectID": "11-matrix-multiplication.html#properties-of-matrix-multiplication",
    "href": "11-matrix-multiplication.html#properties-of-matrix-multiplication",
    "title": "8  Matrix Multiplication",
    "section": "8.3 Properties of Matrix Multiplication",
    "text": "8.3 Properties of Matrix Multiplication\nAlthough matrix multiplication is rarely commutative, it does have the following properties:\n\n\\(\\mathbf{A}(\\mathbf{BC})=(\\mathbf{AB})\\mathbf{C})\\) (Associative Property)\n\\(\\mathbf{A}(\\mathbf{B}+\\mathbf{C})=\\mathbf{AB} +\\mathbf{AC})\\) (Left Distributive Property)\n\\((\\mathbf{B}+\\mathbf{C})\\mathbf{A}=\\mathbf{BA} +\\mathbf{CA})\\) (Right Distributive Property)\n\nso long as the matrices being considered are conformable.\nAside from commutativity, there are also some other properties that matrix multiplication lack. For example, in scalar arithmetic, if \\(ab = 0\\), then either a or b must be zero. In matrix multiplication, however, this is not always the case. To illustrate, consider the following three matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}-2 & 4 \\\\-2 & 4 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}0 & 0 \\\\0 & 0 \\end{bmatrix} \\qquad \\mathbf{C} = \\begin{bmatrix}4 & 4 \\\\2 & 2 \\end{bmatrix}\n\\]\nMatrix B, in which every element is zero is called the null matrix. (A null matrix is a matrix in which every element is zero.) If we postmultiply matrix A by the null matrix B, we get:\n\\[\n\\begin{split}\n\\mathbf{A}\\mathbf{B} &= \\begin{bmatrix}-2 & 4 \\\\-2 & 4 \\end{bmatrix} \\begin{bmatrix}0 & 0 \\\\0 & 0 \\end{bmatrix} \\\\[2em]\n&= \\begin{bmatrix}0 & 0 \\\\0 & 0 \\end{bmatrix}\n\\end{split}\n\\]\nThe result of postmultiplying a matrix (so long as it is conformable) by the null matrix is a null matrix. This is also true when we premultiply by a null matrix. Now consider postmultiplying matrix A by matrix C:\n\\[\n\\begin{split}\n\\mathbf{A}\\mathbf{C} &= \\begin{bmatrix}-2 & 4 \\\\-2 & 4 \\end{bmatrix} \\begin{bmatrix}4 & 4 \\\\2 & 2 \\end{bmatrix} \\\\[2em]\n&= \\begin{bmatrix}0 & 0 \\\\0 & 0 \\end{bmatrix}\n\\end{split}\n\\]\nWe again obtain a null matrix. However, neither A nor C was a null matrix. (In fact, none of the elements of A or C were zero).\nRelatedly, in scalar arithmetic, if \\(ab = 0\\), then \\(ba = 0\\). This property does not hold in matrix multiplication. For example, we just saw that \\(\\mathbf{A}\\mathbf{C}=\\mathbf{0}\\). But, if we postmultiply C by A:\n\\[\n\\begin{split}\n\\mathbf{C}\\mathbf{A} &=  \\begin{bmatrix}4 & 4 \\\\2 & 2 \\end{bmatrix}\\begin{bmatrix}-2 & 4 \\\\-2 & 4 \\end{bmatrix} \\\\[2em]\n&= \\begin{bmatrix}-16 & 16 \\\\-8 & 8 \\end{bmatrix}\n\\end{split}\n\\]\nThat is, although \\(\\mathbf{A}\\mathbf{C}=\\mathbf{0}\\), \\(\\mathbf{C}\\mathbf{A}\\neq\\mathbf{0}\\). Changing the order of the matrix multiplication changes the resulting product.\nDrawing from this example we see that since \\(\\mathbf{AB}=\\mathbf{0}\\) and \\(\\mathbf{AC}=\\mathbf{0}\\), this means that \\(\\mathbf{AB}=\\mathbf{AC}\\). However, \\(\\mathbf{B}\\neq\\mathbf{C}\\). This implies that “cancellation” is not a valid property of matrix multiplication.",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrix Multiplication</span>"
    ]
  },
  {
    "objectID": "11-matrix-multiplication.html#revisiting-dot-products",
    "href": "11-matrix-multiplication.html#revisiting-dot-products",
    "title": "8  Matrix Multiplication",
    "section": "8.4 Revisiting Dot Products",
    "text": "8.4 Revisiting Dot Products\nIt turns out that we can express the dot product between two vectors as matrix multiplication. Consider the column vectors a and b:\n\\[\n\\mathbf{a} = \\begin{bmatrix}5 \\\\ 4 \\\\ 7 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{b} = \\begin{bmatrix}1 \\\\ 0 \\\\ -1 \\\\ 2\\end{bmatrix}\n\\]\nRemember column vectors are matrices with one column. The dot product between a and b is the same as premultiplying b by the transpose of a, that is:\n\\[\n\\mathbf{a}\\bullet\\mathbf{b} = \\underset{1\\times4}{\\mathbf{a}^\\intercal}~\\underset{4\\times1}{\\mathbf{b}}\n\\]\nBy transposing a, the matrices are conformable, and the resulting product will be a \\(1\\times1\\) matrix (i.e., a scalar).\n\n# Create a\na = matrix(data = c(5, 4, 7, 2), ncol = 1)\n\n# Create b\nb = matrix(data = c(1, 0, -1, 2), ncol = 1)\n\n# Postmultiply the transpose of a by b\nt(a) %*% b\n\n     [,1]\n[1,]    2\n\n# Compute dot product as sum of products\nsum(a * b)\n\n[1] 2",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrix Multiplication</span>"
    ]
  },
  {
    "objectID": "12-matrix-transposition.html",
    "href": "12-matrix-transposition.html",
    "title": "9  Matrix Transposition",
    "section": "",
    "text": "Exercises\nTransposition is another operation that can also be carried out on matrices. Just as when we transpose a vector, we transpose a matrix by taking each column in turn and making it a row. In other words, we interchange each column and row, so the first column becomes the first row, the second column becomes the second row, etc. For example,\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n112 & 86 & 0 \\\\ 134 & 94 & 0\n\\end{bmatrix} \\qquad\n\\mathbf{A}^\\intercal = \\begin{bmatrix}\n112 & 134 \\\\86 & 94 \\\\ 0 & 0\n\\end{bmatrix}\n\\]\nFormally, if A is an \\(n \\times k\\) matrix with elements \\(a_{ij}\\), then the transpose of A, denoted \\(\\mathbf{A}^\\intercal\\) is a \\(k \\times n\\) matrix where element \\(a^\\intercal_{ij}=a_{ji}\\). Some properties of the matrix transpose are:\nComputationally, the t() function will produce the transpose of a matrix in R.\nConsider the following matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}1 & 5 & -1 \\\\ 3 & 2 & -1 \\\\ 0 & 1 & 5 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}3 & 1 & 6 \\\\ 2 & 0 & 1 \\\\ -7 & -1 & 2 \\end{bmatrix}\n\\]",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrix Transposition</span>"
    ]
  },
  {
    "objectID": "12-matrix-transposition.html#exercises",
    "href": "12-matrix-transposition.html#exercises",
    "title": "9  Matrix Transposition",
    "section": "",
    "text": "Verify that \\((\\mathbf{A}+\\mathbf{B})^\\intercal = \\mathbf{A}^\\intercal + \\mathbf{B}^\\intercal\\)?\n\n\nShow/Hide Solution\n\n\n\\[\n\\begin{split}\n(\\mathbf{A}+\\mathbf{B})^\\intercal &= \\begin{bmatrix}4 & 6 & 5\\\\5 & 2 & 0\\\\ -7 & 0 & 7 \\end{bmatrix}^\\intercal = \\begin{bmatrix}4 & 5 & -7\\\\6 & 2 & 0\\\\5 & 0& 7 \\end{bmatrix} \\\\[3em]\n\\mathbf{A}^\\intercal + \\mathbf{B}^\\intercal &= \\begin{bmatrix}1 & 3 & 0\\\\5 & 2 & 1\\\\-1 & -1 & 5 \\end{bmatrix} + \\begin{bmatrix}3 & 2 & -7\\\\1 & 0 & -1\\\\6 & 1 & 2 \\end{bmatrix} = \\begin{bmatrix}4 & 5 & -7\\\\6 & 2 & 0\\\\5 & 0& 7 \\end{bmatrix}\n\\end{split}\n\\]\n\n\nVerify that \\((\\mathbf{A}\\mathbf{B})^\\intercal = \\mathbf{B}^\\intercal \\mathbf{A}^\\intercal\\)?.\n\n\nShow/Hide Solution\n\n\n\\[\n\\begin{split}\n(\\mathbf{A}\\mathbf{B})^\\intercal &= \\begin{bmatrix}20 & 2 & 9\\\\20 & 4 & 18\\\\-33 & -5 & 11 \\end{bmatrix}^\\intercal = \\begin{bmatrix}20 & 20 & -33\\\\ 2 & 4& -5\\\\9 & 18 & 11 \\end{bmatrix} \\\\[3em]\n\\mathbf{B}^\\intercal \\mathbf{A}^\\intercal &= \\begin{bmatrix}3 & 2 & -7\\\\1 & 0 & -1\\\\6 & 1 & 2 \\end{bmatrix}\\begin{bmatrix}1 & 3 & 0\\\\5 & 2 & 1\\\\-1 & -1 & 5 \\end{bmatrix} = \\begin{bmatrix}20 & 20 & -33\\\\ 2 & 4& -5\\\\9 & 18 & 11 \\end{bmatrix}\n\\end{split}\n\\]",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrix Transposition</span>"
    ]
  },
  {
    "objectID": "15-matrix-operations-exercises.html",
    "href": "15-matrix-operations-exercises.html",
    "title": "10  Exercises: Matrix Operations",
    "section": "",
    "text": "Statistics Example: Weights\nConsider the following matrices:\n\\[\n\\mathbf{X} = \\begin{bmatrix}2 & 3 \\\\1 & 2 \\end{bmatrix} \\qquad \\mathbf{Y} = \\begin{bmatrix}3 & 4 \\\\2 & 1 \\end{bmatrix} \\qquad \\mathbf{Z} = \\begin{bmatrix}2 & 3 & 1 \\\\5 & 6 & 8\\\\9 & 4 & 7 \\end{bmatrix}\n\\]\nConsider the following matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}0 & 6 \\\\5 & 1 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}0 & 5 \\\\2 & \\frac{1}{2} \\end{bmatrix} \\qquad \\mathbf{C} = \\begin{bmatrix}6 & 2 & 1 \\\\5 & 3 & 1\\\\8 & 4 & 1 \\end{bmatrix} \\qquad \\mathbf{D} = \\begin{bmatrix}0& 1 & 4 & 6 \\\\1 & 2 & 5 & -2\\\\1 & 3 & 2 & 8 \\end{bmatrix}\n\\]\n\\[\n\\mathbf{X} = \\begin{bmatrix}32 & 54 & 56 & 21 \\\\42 & 23 & 52 & 35 \\\\ 16 & 41 & 54 & 56 \\\\ 58 & 52 & 31 & 24 \\\\ 41 & 50 & 42 & 40 \\end{bmatrix} \\qquad \\mathbf{w} = \\begin{bmatrix} 0.10 \\\\ 0.10 \\\\ 0.30 \\\\ 0.50 \\end{bmatrix}\n\\]",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exercises: Matrix Operations</span>"
    ]
  },
  {
    "objectID": "15-matrix-operations-exercises.html#statistics-example-weights",
    "href": "15-matrix-operations-exercises.html#statistics-example-weights",
    "title": "10  Exercises: Matrix Operations",
    "section": "",
    "text": "Consider the scores for five students on four course exams (each out of 100 points) shown in matrix X. The final percentage in the course is based on the following weighting: the first and second exams are worth 10% of the course, the third exam is worth 30% of the course, and the fourth exam is worth 50% of the course. These weights are presented in the column vector w. Use R to find the final percentage for each student by postmultiplying the score matrix by the weight vector.\n\n\n\nShow/Hide Solution\n\n\n\n# Create X\nX = matrix(\n  data = c(32, 54, 56, 21,\n           42, 23, 52, 35,\n           16, 41, 54, 56,\n           58, 52, 31, 24,\n           41, 50, 42, 40),\n  byrow = TRUE, \n  ncol = 4\n)\n\n# Create w\nw = matrix(data = c(0.10, 0.10, 0.30, 0.50), ncol = 1)\n\n# Compute Xw\nX %*% w\n\n     [,1]\n[1,] 35.9\n[2,] 39.6\n[3,] 49.9\n[4,] 32.3\n[5,] 41.7",
    "crumbs": [
      "Matrices and Matrix Operations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exercises: Matrix Operations</span>"
    ]
  },
  {
    "objectID": "21-square-matrices.html",
    "href": "21-square-matrices.html",
    "title": "11  Square Matrices and Friends",
    "section": "",
    "text": "11.1 Square Matrices\nIn this chapter, you will learn about square matrices and several specific types of square matrices encountered in statistical and psychometric applications. Many of these matrices have properties that make them incredibly useful for computation.\nWhen the number of rows and columns in a matrix are equal, the matrix is referred to as a square matrix. For example, X and Y are both square matrices.\n\\[\n\\underset{2\\times 2}{\\mathbf{X}} = \\begin{bmatrix}\n4 & 1 \\\\\n0 & 3\n\\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{Y}} = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n1 & 3 & -8\\\\\n10 & 4 & -2\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Square Matrices and Friends</span>"
    ]
  },
  {
    "objectID": "21-square-matrices.html#square-matrices",
    "href": "21-square-matrices.html#square-matrices",
    "title": "11  Square Matrices and Friends",
    "section": "",
    "text": "11.1.1 Main Diagonal\nIn a square matrix, the main diagonal (a.k.a., major diagonal or principal diagonal) includes the elements that lie along the diagonal from the upper-left element to the lower-right element. For example, the main diagonal in Y (highlighted in red) includes the elements \\(0\\), \\(3\\), and \\(-2\\). Elements not in the main diagonal are referred to as off-diagonal elements.\n\\[\n\\underset{3\\times 3}{\\mathbf{Y}} = \\begin{bmatrix}\n\\color{red}{0} & 1 & 0 \\\\\n1 & \\color{red}{3} & -8\\\\\n10 & 4 & \\color{red}{-2}\n\\end{bmatrix}\n\\]\nOne interesting and useful property of a square matrix is that if we compute its transpose, the main diagonal is the same as in the original matrix.\n\\[\n\\underset{3\\times 3}{\\mathbf{Y}} = \\begin{bmatrix}\n\\color{red}0 & 1 & 0 \\\\\n1 & \\color{red}3 & -8\\\\\n10 & 4 & \\color{red}{-2}\n\\end{bmatrix} \\qquad\n\\underset{3\\times 3}{\\mathbf{Y}^\\intercal} = \\begin{bmatrix}\n\\color{red}0 & 1 & 10 \\\\\n1 & \\color{red}3 & 4\\\\\n0 & -8 & \\color{red}{-2}\n\\end{bmatrix}\n\\]\nWe can use the diag() function to return the elements on the main diagonal in a square matrix.\n\n# Create Y\nY = matrix(\n  data = c(0, 1, 10, 1, 3, 4, 0, -8, -2),\n  nrow = 3\n  )\n\n# Display Y\nY\n\n     [,1] [,2] [,3]\n[1,]    0    1    0\n[2,]    1    3   -8\n[3,]   10    4   -2\n\n# Find diagonal elements\ndiag(Y)\n\n[1]  0  3 -2\n\n\n\nCAUTION\nThe diag() function also works on non-square matrices. However, it returns the elements on the diagonal starting with the element in the first row and column.\n\n# Create X\nX = matrix(\n  data = c(0, 1, 1, 3, 4, 0),\n  nrow = 3\n  )\n\n# Display X\nX\n\n     [,1] [,2]\n[1,]    0    3\n[2,]    1    4\n[3,]    1    0\n\n# Find 'diagonal' elements\ndiag(X)\n\n[1] 0 4\n\n\nThis is because non-square matrices also have a main diagonal. The main diagonal in a non-square matrix includes the elements \\(a_{11}, a_{22}, \\ldots, a_{mm}\\) where m is the minimum of the row and column values.",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Square Matrices and Friends</span>"
    ]
  },
  {
    "objectID": "21-square-matrices.html#diagonal-matrices",
    "href": "21-square-matrices.html#diagonal-matrices",
    "title": "11  Square Matrices and Friends",
    "section": "11.2 Diagonal Matrices",
    "text": "11.2 Diagonal Matrices\nA diagonal matrix is a square matrix in which all the off-diagonal elements are zero. Two examples of diagonal matrices are as follows:\n\\[\n\\underset{2\\times 2}{\\mathbf{D}_1} = \\begin{bmatrix}\n5 & 0 \\\\\n0 & -1\n\\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{D}_2} = \\begin{bmatrix}\n3 & 0 & 0 \\\\\n0 & 3 & 0 \\\\\n0 & 0 & 3\n\\end{bmatrix}\n\\]\nIn general, D is a diagonal matrix if element \\(d_{ij}=0\\) for all \\(i\\neq j\\).",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Square Matrices and Friends</span>"
    ]
  },
  {
    "objectID": "21-square-matrices.html#scalar-matrices",
    "href": "21-square-matrices.html#scalar-matrices",
    "title": "11  Square Matrices and Friends",
    "section": "11.3 Scalar Matrices",
    "text": "11.3 Scalar Matrices\nA diagonal matrix in which all the diagonal elements have the same value is called a scalar matrix. The following two matrices are scalar matrices:\n\\[\n\\underset{2\\times 2}{\\mathbf{S}_1} = \\begin{bmatrix}\n-3 & 0 \\\\\n0 & -3\n\\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{S}_2} = \\begin{bmatrix}\n10 & 0 & 0 \\\\\n0 & 10 & 0 \\\\\n0 & 0 & 10\n\\end{bmatrix}\n\\]\nThe null matrix is also a scalar matrix. In general S is a scalar matrix if element\n\\[\ns_{ij}=\\begin{cases}k \\quad \\mathrm{when} \\quad i=j \\\\0 \\quad \\mathrm{when} \\quad i\\neq j\\end{cases}\n\\]\n\n\n11.3.1 Identity Matrices\nA special scalar matrix, the identity matrix, is one in which all the diagonal elements are the value of one (1).\n\\[\n\\underset{2\\times 2}{\\mathbf{I}} = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} \\qquad \\underset{3\\times 3}{\\mathbf{I}} = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nThe identity matrix has the following property:\n\nFor any matrix A, \\(\\mathbf{AI} = \\mathbf{IA} = \\mathbf{A}\\)\n\nThe diag() function can be used to create an identity matrix. We give this function an argument which provides the number of rows and columns.\n\n# Create a 3x3 identity matrix\nI = diag(3)\n\n# Display I\nI\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Square Matrices and Friends</span>"
    ]
  },
  {
    "objectID": "21-square-matrices.html#symmetric-matrices",
    "href": "21-square-matrices.html#symmetric-matrices",
    "title": "11  Square Matrices and Friends",
    "section": "11.4 Symmetric Matrices",
    "text": "11.4 Symmetric Matrices\nIf \\(\\mathbf{A} = \\mathbf{A}^{\\intercal}\\), the matrix A is considered symmetric. For example, the following matrix R is symmetric:\n\\[\n\\underset{3\\times 3}{\\mathbf{R}} = \\begin{bmatrix}\n1.00 & 0.32 & 0.69\\\\\n0.32 & 1.00 & 0.85 \\\\\n0.69 & 0.85 & 1.00\n\\end{bmatrix} =\n\\underset{3\\times 3}{\\mathbf{R}^\\intercal} = \\begin{bmatrix}\n1.00 & 0.32 & 0.69\\\\\n0.32 & 1.00 & 0.85 \\\\\n0.69 & 0.85 & 1.00\n\\end{bmatrix}\n\\]\nIn general, a matrix A is symmetric if,\n\\[\n\\mathbf{A}_{ij} = \\mathbf{A}_{ji}\n\\]\nIf matrix A is symmetric, it necessitates that:\n\n\\(\\mathbf{A}\\) and \\(\\mathbf{A}^{\\intercal}\\) have the same dimensions. This also implies that the matrix must be a square matrix.\nAll of the corresponding elements in \\(\\mathbf{A}\\) and \\(\\mathbf{A}^{\\intercal}\\) are equal.\n\nAll scalar matrices, including the identity matrix, are symmetric.\nIn statistical practice, variance–covariance matrices and correlation matrices are symmetric. Computationally, we can examine whether a matrix is symmetric by checking whether the logical statement equating a matrix and its transpose evaluates as TRUE for all elements. For example, consider the correlation matrix R:\n\\[\n\\mathbf{R}=\\begin{bmatrix}1.00 & 0.32 & 0.69\\\\ 0.32 & 1.00 & 0.85\\\\ 0.69 & 0.85 & 1.00\\end{bmatrix}\n\\]\nTo check its symmetry, we can use the following syntax.\n\n# Create matrix R\nR = matrix(\n  data = c(1.00, 0.32, 0.69, 0.32, 1.00, 0.85, 0.69, 0.85, 1.00),\n  nrow = 3\n)\n\n# Display R\nR\n\n     [,1] [,2] [,3]\n[1,] 1.00 0.32 0.69\n[2,] 0.32 1.00 0.85\n[3,] 0.69 0.85 1.00\n\n# Test for symmetry\nR == t(R)\n\n     [,1] [,2] [,3]\n[1,] TRUE TRUE TRUE\n[2,] TRUE TRUE TRUE\n[3,] TRUE TRUE TRUE\n\n\nIf the logical statement of equality evaluates as TRUE for all the elements, the matrix is symmetric. If the logical expression returns an non-conformable array error, or evaluates as FALSE for any of the elements, then it is not symmetric.\n\n\n11.4.1 Skew Symmetric Matrices\nA matrix A is said to be skew symmetric if \\(\\mathbf{A} = -\\mathbf{A}^{\\intercal}\\).For example, the following matrix S is skew symmetric:\n\\[\n\\underset{3\\times 3}{\\mathbf{S}} = \\begin{bmatrix}\n0 & 2 & -3\\\\\n-2 & 0 & 1 \\\\\n3 & -1 & 0\n\\end{bmatrix}\n\\]\nIn general, a matrix A is skew symmetric if, \\(\\mathbf{a}_{ij} = -\\mathbf{a}_{ji}\\). Note that if a matrix is skew symmetric, all of its diagonal elements must be zero.",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Square Matrices and Friends</span>"
    ]
  },
  {
    "objectID": "21-square-matrices.html#triangular-matrices",
    "href": "21-square-matrices.html#triangular-matrices",
    "title": "11  Square Matrices and Friends",
    "section": "11.5 Triangular Matrices",
    "text": "11.5 Triangular Matrices\nA matrix is a triangular matrix if all of the elements above or below the main diagonal are zero. These are referred to as upper triangular matrices and lower triangular matrices, respectively. A matrix A is a lower triangular matrix if elements \\(a_{ij}=0\\) for \\(j&gt;i\\) (all the elements above the main diagonal are zero). For example the matrix L is lower triangular:\n\\[\n\\underset{4\\times 4}{\\mathbf{L}} = \\begin{bmatrix}\n6 & \\color{red}{0} & \\color{red}{0} & \\color{red}{0}\\\\\n-1 & 5 & \\color{red}{0} & \\color{red}{0} \\\\\n0 & 1 & 2 & \\color{red}{0} \\\\\n3 & -2 & 5 & 7\n\\end{bmatrix}\n\\]\nA matrix A is an upper triangular matrix if elements \\(a_{ij}=0\\) for \\(i&gt;j\\) (all the elements below the main diagonal are zero). For example the matrix U is lower triangular:\n\\[\n\\underset{4\\times 4}{\\mathbf{U}} = \\begin{bmatrix}\n3 & -2 & 5 & 7\\\\\n\\color{red}{0} & 1 & 2 & 0 \\\\\n\\color{red}{0} & \\color{red}{0} & -1 & 5 & \\\\\n\\color{red}{0} & \\color{red}{0} & \\color{red}{0} & 6\\\\\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Square Matrices and Friends</span>"
    ]
  },
  {
    "objectID": "21-square-matrices.html#exercises",
    "href": "21-square-matrices.html#exercises",
    "title": "11  Square Matrices and Friends",
    "section": "Exercises",
    "text": "Exercises\nConsider the following matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}1 & 5 & -1 \\\\ 3 & 2 & -1 \\\\ 0 & 1 & 5 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}3 & 1 & 6 \\\\ 2 & 0 & 1 \\\\ -7 & -1 & 2 \\end{bmatrix}\n\\]\n\nList the diagonal elements of \\(\\mathbf{A}^\\intercal\\).\n\n\nShow/Hide Solution\n\n\n\\[\n\\begin{split}\nB^\\intercal_{1,1}=1 \\\\[1em]\nB^\\intercal_{2,2}=2 \\\\[1em]\nB^\\intercal_{3,3}=5 \\\\[1em]\n\\end{split}\n\\]\n\n\nCreate an identity matrix of order 4.\n\n\nShow/Hide Solution\n\n\n\\[\n\\underset{4\\times4}{\\mathbf{I}} = \\begin{bmatrix}1 & 0 & 0& 0\\\\0 & 1 & 0 & 0\\\\0 & 0 & 1 & 0\\\\0 & 0 & 0 & 1\\end{bmatrix}\n\\]\n\n\nExplain why the transpose of an upper triangular matrix will be a lower triangular matrix.\n\n\nShow/Hide Solution\n\n\nAn upper triangular matrix, U, has elements \\(u_{ij}=0\\) for \\(i&gt;j\\) (all the elements below the main diagonal are zero). Taking the transpose of U, element \\(u_{ij}\\) becomes \\(u^\\intercal_{ji}\\). This means that in \\(\\mathbf{U}^\\intercal\\) the zero elements all be where \\(j&gt;i\\).\n\nConsider the following:\n\\[\n\\mathbf{X} = \\begin{bmatrix} 10 & 4 & 6\\\\4 & 10 & 8\\\\6 & 8 & 9 \\end{bmatrix} \\qquad \\mathbf{Y} = \\begin{bmatrix}0 & 1 & 2\\\\-1 & 0 & 4\\\\-2 & -4 & 0 \\end{bmatrix}\n\\]\n\nUse R to show that X is symmetric.\n\n\nShow/Hide Solution\n\n\n\n# Create X\nX = matrix(\n  data = c(10, 4, 6, 4, 10, 8, 6, 8, 9),\n  byrow = TRUE,\n  ncol = 3\n)\n\n# Check symmetry\nX == t(X)\n\n     [,1] [,2] [,3]\n[1,] TRUE TRUE TRUE\n[2,] TRUE TRUE TRUE\n[3,] TRUE TRUE TRUE\n\n\n\n\nUse R to show that Y is skew symmetric.\n\n\nShow/Hide Solution\n\n\n\n# Create X\nY = matrix(\n  data = c(0, 1, 2, -1, 0, 4, -2, -4, 0),\n  byrow = TRUE,\n  ncol = 3\n)\n\n# Check symmetry\nY == -t(Y)\n\n     [,1] [,2] [,3]\n[1,] TRUE TRUE TRUE\n[2,] TRUE TRUE TRUE\n[3,] TRUE TRUE TRUE",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Square Matrices and Friends</span>"
    ]
  },
  {
    "objectID": "22-properties-of-square-matrices.html",
    "href": "22-properties-of-square-matrices.html",
    "title": "12  Properties of Square Matrices",
    "section": "",
    "text": "12.1 Matrix Trace\nIn this chapter, you will learn about several properties of square matrices encountered in statistical and psychometric applications. These properties are also useful for computation.\nThe trace of a square matrix is the sum of the elements along the main diagonal.1 For example, consider the matrix Y:\n\\[\n\\underset{3\\times 3}{\\mathbf{Y}} = \\begin{bmatrix}\n\\color{red}0 & 1 & 0 \\\\\n1 & \\color{red}3 & -8\\\\\n10 & 4 & \\color{red}{-2}\n\\end{bmatrix}\n\\]\nThe trace of Y is \\(0 + 3 + -2 = 1\\). To compute the trace using R, we use the sum() and diag() functions.\n# Create matrix\nY = matrix(\n  data = c(0, 1, 0, 1, 3, -8, 10, 4, -2),\n  byrow = TRUE,\n  ncol = 3\n  )\n\n# Compute trace of Y\nsum(diag(Y))\n\n[1] 1\nYou can also use the tr() function from the psych library to compute the trace.\nlibrary(psych)\ntr(Y)\n\n[1] 1",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Properties of Square Matrices</span>"
    ]
  },
  {
    "objectID": "22-properties-of-square-matrices.html#determinant",
    "href": "22-properties-of-square-matrices.html#determinant",
    "title": "12  Properties of Square Matrices",
    "section": "12.2 Determinant",
    "text": "12.2 Determinant\nEvery square matrix has a unique scalar value associated with it, called the determinant. The determinant, denoted by \\(\\left| \\mathbf{A} \\right|\\) or \\(\\mathrm{det}(\\mathbf{A})\\), has many uses in statistical and psychometric applications. For example, the determinant of a variance-covariance matrix provides information about the generalized variance of several variables. It is also useful for computing several multivariate statistics.\n\nFYI\nWe will illustrate the process for calculating the determinant for a \\(1 \\times 1\\), \\(2 \\times 2\\), and \\(3 \\times 3\\) matrix. Beyond a \\(3 \\times 3\\) matrix the determinant is more difficult to calculate, and those methods will not be discussed here. In practice, computation is used to find the determinant.\n\n\n\n12.2.1 Determinant for a \\(1 \\times 1\\) matrix.\nThe determinant of a \\(1 \\times 1\\) scalar matrix is equal to the scalar, namely \\(\\mathrm{det}\\big([a_1]\\big) = a_1\\). For example, consider the scalar matrix having an element of \\(-1\\):\n\\[\n\\begin{vmatrix}-1\\end{vmatrix} = -1\n\\]\nThe determinant of this matrix is \\(-1\\).\n\n\n\n12.2.2 Determinant for a \\(2 \\times 2\\) matrix.\nThe determinant of the \\(2\\times 2\\) matrix A, given as\n\\[\n\\underset{2\\times 2}{\\mathbf{A}} = \\begin{bmatrix}\na_{11} & a_{12}  \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\\]\nis the scalar \\(a_{11}(a_{22}) - a_{12}(a_{21})\\). That is, we are taking the product of the elements along the main diagonal (shown in pink below) and subtracting the product of the elements on the off diagonal (shown in blue below).\n\n\n\n\n\nFigure 12.1: Procedure for finding the product terms in the determinant for a 2x2 matrix. The first term is computed by multiplying the elements along the pink arrow, and the second term is computed by multiplying the elements along the blue arrow.\n\n\n\n\n\n\n\n\nColor-coding the formula:\n\\[\n\\begin{split}\n\\mathrm{det}(\\mathbf{A}) = ~&\\color{#CC79A7}{\\overbrace{a_{11}(a_{22})}^\\mathrm{Term~1}} - \\color{#56B4E9}{ \\overbrace{a_{12}(a_{21})}^\\mathrm{Term~2} }\n\\end{split}\n\\]\nAs an example, consider the matrix Y:\n\\[\n\\mathbf{Y} = \\begin{bmatrix}\n1 & 2  \\\\\n3 & 4\n\\end{bmatrix}\n\\]\nThe determinant of Y is:\n\\[\n\\begin{split}\n\\mathrm{det}(\\mathbf{Y}) &=\n\\begin{vmatrix}\n1 & 2  \\\\\n3 & 4\n\\end{vmatrix} \\\\[2ex]\n&= 1(4) - 2(3) \\\\[2ex]\n&= -2\n\\end{split}\n\\]\n\n\n\n12.2.3 Determinant for a \\(3 \\times 3\\) matrix.\nThe determinant of a \\(3\\times 3\\) matrix A, where\n\\[\n\\underset{3\\times 3}{\\mathbf{A}} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\]\nis defined as:\n\\[\n\\begin{split}\n\\mathrm{det}(\\mathbf{A}) = &a_{11}(a_{22})(a_{33}) + a_{12}(a_{23})(a_{31}) + a_{13}(a_{21})(a_{32}) -\\\\\n&a_{13}(a_{22})(a_{31}) - a_{11}(a_{23})(a_{32}) - a_{12}(a_{21})(a_{33})\n\\end{split}\n\\]\nAlthough this looks complicated the procedure for finding this is easily described. Write out the matrix A and additionally append the first two columns to the right of the matrix. These additional columns are shown in red below.\n\\[\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix} \\color{red}{\\begin{matrix}a_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\ a_{31} & a_{32} \\end{matrix}}\n\\]\nThe first three product terms in the determinant are computed by: multiplying the elements found in the main diagonal (first product term), and the elements in the remaining two parallel diagonals (shown in pink below). Similarly, the second set of three products are found by multiplying the elements in the opposite diagonal and the elements in the two parallel diagonals (shown in blue below).\n\n\n\n\n\nFigure 12.2: Procedure for finding the product terms in the determinant for a 3x3 matrix. The first three terms computed by multiplying the elements along each of the pink arrows, and the second set of three terms are computed by multiplying the elements along each of the blue arrows.\n\n\n\n\n\n\n\n\nWe add together the first three product terms and subtract the last three product terms. This gives us the determinant.\n\\[\n\\begin{split}\n\\mathrm{det}(\\mathbf{A}) = ~&\\color{#CC79A7}{\\overbrace{a_{11}(a_{22})(a_{33})}^\\mathrm{Term~1} + \\overbrace{a_{12}(a_{23})(a_{31})}^\\mathrm{Term~2} + \\overbrace{a_{13}(a_{21})(a_{32})}^\\mathrm{Term~3}} \\color{#56B4E9}{-}\\\\\n&\\color{#56B4E9}{\\underbrace{a_{13}(a_{22})(a_{31})}_\\mathrm{Term~4} - \\underbrace{a_{11}(a_{23})(a_{32})}_\\mathrm{Term~5} - \\underbrace{a_{12}(a_{21})(a_{33})}_\\mathrm{Term~6}}\n\\end{split}\n\\]\nAs an example consider the following \\(3 \\times 3\\) matrix:\n\\[\n\\underset{3\\times 3}{\\mathbf{X}} = \\begin{bmatrix}\n3 & 5 & 0 \\\\\n1 & 2 & 1 \\\\\n3 & 6 & 4\n\\end{bmatrix}\n\\]\nThe determinant of X is:\n\\[\n\\begin{split}\n\\mathrm{det}(\\mathbf{X}) &=\n\\begin{vmatrix}\n3 & 5 & 0 \\\\\n1 & 2 & 1 \\\\\n3 & 6 & 4\n\\end{vmatrix} \\\\[2ex]\n&= 3(2)(4) + 5(1)(3) + 0(1)(6) - 0(2)(3) - 3(1)(6) - 5(1)(4) \\\\[2ex]\n&= 24 + 15 + 0 - 0 - 18 - 20 \\\\[2ex]\n&= 1\n\\end{split}\n\\]\n\n\n\n12.2.4 Using R to Find the Determinant\nWe can use the det() function to compute the determinant, using R. Below we use this function to compute the determinant for each of the matrices given in the examples above.\n\n# Create matrix\nY = matrix(\n  data = c(1, 2, 3, 4),\n  byrow = TRUE,\n  ncol = 2\n)\n\n# Find determinant\ndet(Y)\n\n[1] -2\n\n# Create matrix\nX = matrix(\n  data = c(3, 5, 0, 1, 2, 1, 3, 6, 4),\n  byrow = TRUE,\n  ncol = 3\n)\n\n# Find determinant\ndet(X)\n\n[1] 1\n\n\n\n\n\n12.2.5 Properties of the Determinant\nThere are several useful properties of the determinant. For each of these properties A and B are matrices and \\(\\lambda\\) is a scalar.\n\nIf every element in a row (or column) of a matrix is zero, then the determinant of that matrix is zero.\nIf two rows (or columns) of a matrix are swapped, the determinant changes sign.\nIf two rows (or columns) of a matrix are identical, the determinant of that matrix is zero.\n\\(\\mathrm{det}(\\mathbf{A}) = \\mathrm{det}(\\mathbf{A}^\\intercal)\\)\nIf \\(\\mathbf{B}=\\lambda\\mathbf{A}\\), then \\(\\mathrm{det}(\\mathbf{B}) = \\lambda\\big(\\mathrm{det}(\\mathbf{A})\\big)\\).\nFor an \\(n \\times n\\) matrix A, \\(\\mathrm{det}(\\lambda\\mathbf{A}) = \\lambda^n\\big(\\mathrm{det}(\\mathbf{A})\\big)\\).\nIf A and B are of the same order, \\(\\mathrm{det}(\\mathbf{A})\\mathrm{det}(\\mathbf{B}) = \\mathrm{det}(\\mathbf{AB})\\)",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Properties of Square Matrices</span>"
    ]
  },
  {
    "objectID": "22-properties-of-square-matrices.html#matrix-inverse",
    "href": "22-properties-of-square-matrices.html#matrix-inverse",
    "title": "12  Properties of Square Matrices",
    "section": "12.3 Matrix Inverse",
    "text": "12.3 Matrix Inverse\nThe inverse of a \\(n \\times n\\) matrix A, denoted \\(\\mathbf{A}^{-1}\\), is also a \\(n \\times n\\) matrix having the property that:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\n\\]\nThat is, when we postmultiply (or premultiply) a square matrix by its inverse, we obtain the \\(n \\times n\\) identity matrix. Not all square matrices have an inverse. If a square matrix has an inverse we say that it is invertible or nonsingular.\nTypically, we will use the computer to find a matrix’s inverse. However, for the \\(2\\times2\\) matrix, we can describe a procedure that can be used to find the inverse. To find \\(\\mathbf{A}^{-1}\\) we:\n\nCompute the determinant of A.\nCreate a new matrix, call it B where we swap the elements on the main diagonal of A and change the signs on the off diagonals elements.\nMultiply B by the reciprocal of the determinant.\n\nAs an example, consider the following \\(2 \\times 2\\) matrix:\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n2 & 1  \\\\\n1 & 3\n\\end{bmatrix}\n\\]\nThe determinant of A is 5. Swapping the elements on the main diagonal of A and changing the signs on the off diagonals elements, we get:\n\\[\n\\begin{bmatrix}\n3 & -1  \\\\\n-1 & 2\n\\end{bmatrix}\n\\]\nTo obtain the inverse, we multiply this converted matrix by the reciprocal of the determinant:\n\\[\n\\begin{split}\n\\mathbf{A}^{-1} &= \\frac{1}{5}\n\\begin{bmatrix}\n3 & -1  \\\\\n-1 & 2\n\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}\n\\frac{3}{5} & -\\frac{1}{5}  \\\\\n-\\frac{1}{5} & \\frac{2}{5}\n\\end{bmatrix}\n\\end{split}\n\\]\nIf we postmultiply (or premultiply) this matric by A, we should get the \\(2 \\times 2\\) identity matrix.\n\\[\n\\begin{split}\n\\mathbf{A}\\mathbf{A}^{-1} &= \\begin{bmatrix}\n2 & 1  \\\\\n1 & 3\n\\end{bmatrix}\\begin{bmatrix}\n\\frac{3}{5} & -\\frac{1}{5}  \\\\\n-\\frac{1}{5} & \\frac{2}{5}\n\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}\n1 & 0  \\\\\n0 & 1\n\\end{bmatrix} \\\\[5ex]\n\\mathbf{A}^{-1}\\mathbf{A} &= \\begin{bmatrix}\n\\frac{3}{5} & -\\frac{1}{5}  \\\\\n-\\frac{1}{5} & \\frac{2}{5}\n\\end{bmatrix}\\begin{bmatrix}\n2 & 1  \\\\\n1 & 3\n\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}\n1 & 0  \\\\\n0 & 1 \\end{bmatrix}\n\\end{split}\n\\]\n\n\n12.3.1 Find the Inverse using R\nTo find the inverse using R, we can use the solve() function. If you want fractional elements (rather than decimal elements) you can use the fractions() function from the {MASS} package to convert the decimal output to fractions.\n\n# Create matrix\nA = matrix(\n  data = c(2, 1, 1, 3),\n  byrow = TRUE,\n  ncol = 2\n)\n\n# Find inverse\nsolve(A)\n\n     [,1] [,2]\n[1,]  0.6 -0.2\n[2,] -0.2  0.4\n\n# Find inverse (output as fractions)\nMASS::fractions(solve(A))\n\n     [,1] [,2]\n[1,]  3/5 -1/5\n[2,] -1/5  2/5\n\n\nIf you get a message that the system is “exactly singular” or “computationally singular”, the matrix does not have an inverse (or it cannot be computed). In this case, we would say that A is not invertible or is singular.\n\n\n\n12.3.2 Properties of the Inverse\nThere are several useful properties of the matrix inverse.\n\nIf \\(\\mathrm{det}(\\mathbf{A})=0\\), then A is singular and \\(\\mathbf{A}^{-1}\\) does not exist.\nIf the inverse exists for A, there is only one inverse.\nIf the inverse exists for A, the inverse also exists for \\(\\mathbf{A}^\\intercal\\)\n\\((\\mathbf{A}^{-1})^{-1} = \\mathbf{A}\\)\n\\((\\mathbf{A}^\\intercal)^{-1} = (\\mathbf{A}^{-1})^\\intercal\\)\n\\((\\mathbf{AB})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\\)\n\\((\\mathbf{A})^{-n} = (\\mathbf{A}^{-1})^n\\)\nIf \\(\\lambda\\) is a non-zero scalar, \\((\\lambda\\mathbf{A})^{-1} = \\frac{1}{\\lambda}(\\mathbf{A}^{-1})\\)\nThe inverse of a nonsingular diagonal matrix D is also a diagonal matrix. Moreover, the diagonal elements of \\(\\mathbf{D}^{-1}\\) are the reciprocals of the diagonal elements of D.\nThe inverse of a nonsingular symmetric matrix is also a symmetric matrix.",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Properties of Square Matrices</span>"
    ]
  },
  {
    "objectID": "22-properties-of-square-matrices.html#exercises",
    "href": "22-properties-of-square-matrices.html#exercises",
    "title": "12  Properties of Square Matrices",
    "section": "Exercises",
    "text": "Exercises\nFor each of the following matrices, calculate (by-hand) the trace, determinant, and inverse. Then use R to check your work.\n\n\\(\\mathbf{A} = \\begin{bmatrix}2 & 4 \\\\3 & 6 \\end{bmatrix}\\)\n\n\nShow/Hide Solution\n\n\nTrace: \\(2+6=8\\)\nDeterminant: \\(2(6) - 3(4) = 0\\)\nInverse: Since the determinant is 0, A is singular and does not have an inverse.\n\n# Create A\nA = matrix(data = c(2, 4, 3, 6), byrow = TRUE, ncol = 2)\n\n# Compute trace\nsum(diag(A))\n\n[1] 8\n\n# Compute determinant\ndet(A)\n\n[1] -6.661338e-16\n\n# Compute inverse\nsolve(A)\n\nError in solve.default(A): system is computationally singular: reciprocal condition number = 7.40149e-18\n\n\n\n\n\\(\\mathbf{B} = \\begin{bmatrix}2 & -1 \\\\3 & 3 \\end{bmatrix}\\)\n\n\nShow/Hide Solution\n\n\nTrace: \\(2+3=5\\)\nDeterminant: \\(2(3) - 3(-1) = 9\\)\nInverse: \\(\\frac{1}{9}\\begin{bmatrix}3 & 1 \\\\-3 & 2 \\end{bmatrix} = \\begin{bmatrix}\\frac{1}{3} & \\frac{1}{9} \\\\-\\frac{1}{3} & \\frac{2}{9} \\end{bmatrix}\\)\n\n# Create B\nB = matrix(data = c(2, -1, 3, 3), byrow = TRUE, ncol = 2)\n\n# Compute trace\nsum(diag(B))\n\n[1] 5\n\n# Compute determinant\ndet(B)\n\n[1] 9\n\n# Compute inverse\nMASS::fractions(solve(B))\n\n     [,1] [,2]\n[1,]  1/3  1/9\n[2,] -1/3  2/9\n\n\n\nFor each of the following matrices, calculate (by-hand) the trace, and determinant. Then use R to check your work. Also use R to find the inverse.\n\n\\(\\mathbf{C} = \\begin{bmatrix}2 & -1 & 3 \\\\3 & 3 & 4\\\\1 & 2 & 2 \\end{bmatrix}\\)\n\n\nShow/Hide Solution\n\n\nTrace: \\(2+3+2=7\\)\nDeterminant: \\(2(3)(2) + (-1)(4)(1) + 3(3)(2) - 1(3)(3) - 2(4)(2) - 2(3)(-1) = 7\\)\n\n# Create C\nC = matrix(data = c(2, -1, 3, 3, 3, 4, 1, 2, 2), byrow = TRUE, ncol = 3)\n\n# Compute trace\nsum(diag(C))\n\n[1] 7\n\n# Compute determinant\ndet(C)\n\n[1] 7\n\n# Compute inverse\nMASS::fractions(solve(C))\n\n     [,1]  [,2]  [,3] \n[1,]  -2/7   8/7 -13/7\n[2,]  -2/7   1/7   1/7\n[3,]   3/7  -5/7   9/7\n\n\n\n\n\\(\\mathbf{D} = \\begin{bmatrix}2 & -1 & 3 \\\\3 & 3 & 4\\\\5 & 2 & 7 \\end{bmatrix}\\)\n\n\nShow/Hide Solution\n\n\nTrace: \\(2+3+7=12\\)\nDeterminant: \\(2(3)(7) + (-1)(4)(5) + 3(3)(2) - 5(3)(3) - 2(4)(2) - 7(3)(-1) = 0\\)\n\n# Create D\nD = matrix(data = c(2, -1, 3, 3, 3, 4, 5, 2, 7), byrow = TRUE, ncol = 3)\n\n# Compute trace\nsum(diag(D))\n\n[1] 12\n\n# Compute determinant\ndet(D)\n\n[1] 6.994405e-15\n\n# Compute inverse\nMASS::fractions(solve(D))\n\nError in solve.default(D): system is computationally singular: reciprocal condition number = 2.17218e-17\n\n\nNote: When you used R to compute the determinant of D you did not get 0; the determinant was computed as \\(1.20 \\times 10^{-14}=0.000000000000012\\). This is because R uses a computational method for computing the determinant called LU decomposition. We will examine this in a later chapter.",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Properties of Square Matrices</span>"
    ]
  },
  {
    "objectID": "22-properties-of-square-matrices.html#footnotes",
    "href": "22-properties-of-square-matrices.html#footnotes",
    "title": "12  Properties of Square Matrices",
    "section": "",
    "text": "Since rectangular matrices also have a main diagonal, we can also compute the trace of a rectangular matrix. However, in statistical and psychometric applications (e.g., principle components analysis), the trace is primarily computed on square matrices.↩︎",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Properties of Square Matrices</span>"
    ]
  },
  {
    "objectID": "23-eigenvalues-and-eigenvectors.html",
    "href": "23-eigenvalues-and-eigenvectors.html",
    "title": "13  Eigenvalues and Eigenvectors",
    "section": "",
    "text": "13.1 Eigenvalues\nIn this chapter, you will learn about eigenvalues and eigenvectors. Eigenvalues (a.k.a., characteristic roots) are scalars that are associated with linear systems of equations. Each eigenvalue has a corresponding vector, an eigenvector, associated with it. Eigenvalues and eigenvectors play a role in many statistical applications, featuring prominently in some methods of decomposition and principal component analysis (PCA).\nLet A be a \\(p \\times p\\) square matrix and I be a \\(p \\times p\\) identity matrix. Eigenvalues are the set of non-zero scalars \\(\\lambda_1,\\lambda_2, \\lambda_3,\\ldots,\\lambda_p\\) that satisfy,\n\\[\n\\mathrm{det}\\bigg(\\mathbf{A} - \\lambda \\mathbf{I}\\bigg) = 0\n\\]\nConsider an example,\n\\[\n\\underset{2\\times 2}{\\mathbf{A}} = \\begin{bmatrix}\n-3 & 5  \\\\\n4 & -2  \\\\\n\\end{bmatrix}\n\\]\nOur goal is to find the values of \\(\\lambda\\) that satisfy\n\\[\n\\mathrm{det}\\bigg(\\begin{bmatrix}\n-3 & 5  \\\\\n4 & -2  \\\\\n\\end{bmatrix} - \\lambda\\begin{bmatrix}\n1 & 0  \\\\\n0 & 1  \\\\\n\\end{bmatrix}\\bigg) = 0\n\\]\nUsing matrix algebra,\n\\[\n\\begin{split}\n\\mathrm{det}\\bigg(\\begin{bmatrix}\n-3 & 5  \\\\\n4 & -2  \\\\\n\\end{bmatrix} - \\lambda\\begin{bmatrix}\n1 & 0  \\\\\n0 & 1  \\\\\n\\end{bmatrix}\\bigg) &= 0 \\\\[2ex]\n\\mathrm{det}\\bigg(\\begin{bmatrix}\n-3 & 5  \\\\\n4 & -2  \\\\\n\\end{bmatrix} - \\begin{bmatrix}\n\\lambda & 0  \\\\\n0 & \\lambda  \\\\\n\\end{bmatrix}\\bigg) &= 0 \\\\[2ex]\n\\mathrm{det}\\begin{bmatrix}\n-3 - \\lambda & 5  \\\\\n4 & -2 - \\lambda \\\\\n\\end{bmatrix} &= 0 \\\\[2ex]\n(-3 - \\lambda)(-2 - \\lambda) - 20 &= 0\n\\end{split}\n\\]\nComputing the determinant, distributing the first set of terms, subtracting 20 we obtain:\n\\[\n\\begin{split}\n(-3 - \\lambda)(-2 - \\lambda) - 20 &= 0 \\\\[1em]\n6 + 3\\lambda + 2\\lambda + \\lambda^2 -20 &= 0 \\\\[1em]\n\\lambda^2 + 5\\lambda - 14 &= 0 \\\\[1em]\n\\end{split}\n\\]\nFactoring the left-hand side,\n\\[\n(\\lambda + 7)(\\lambda - 2) = 0\n\\]\nSolving for \\(\\lambda\\), we find that \\(\\lambda = -7\\) and \\(\\lambda = 2\\). We could also have applied the quadratic formula to solve this for \\(\\lambda\\).\nNow that we have the eigenvalues we can double-check that \\(\\mathbf{A} - \\lambda \\mathbf{I}\\) is singular. (I will skip this here, but plug in the values for \\(\\lambda\\), one at a time, and ensure that the determinant is zero.) The equation you solved,\n\\[\n\\mathrm{det}\\big(\\mathbf{A} - \\lambda \\mathbf{I}\\big)=0\n\\]\nis referred to as the characteristic equation. Solving the characteristic equation gives the eigenvalues. Sometimes the eigenvalues are referred to as the characteristic roots of matrix A.",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigenvalues and Eigenvectors</span>"
    ]
  },
  {
    "objectID": "23-eigenvalues-and-eigenvectors.html#eigenvalues",
    "href": "23-eigenvalues-and-eigenvectors.html#eigenvalues",
    "title": "13  Eigenvalues and Eigenvectors",
    "section": "",
    "text": "MATH NOTE\nRemember the quadratic formula? The roots of the polynomial \\(Ax^2+Bx+C\\) are computed using:\n\\[\nx = \\frac{-B\\pm \\sqrt{B^2-4AC}}{2A}\n\\]\nQuick recap at Khan Academy if you need it.",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigenvalues and Eigenvectors</span>"
    ]
  },
  {
    "objectID": "23-eigenvalues-and-eigenvectors.html#eigenvectors",
    "href": "23-eigenvalues-and-eigenvectors.html#eigenvectors",
    "title": "13  Eigenvalues and Eigenvectors",
    "section": "13.2 Eigenvectors",
    "text": "13.2 Eigenvectors\nIf \\(\\lambda\\) is an eigenvalue of matrix A, then it is possible to find a vector v (an eigenvector) that satisfies\n\\[\n\\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}\n\\]\nIn our previous example, A was a \\(2\\times2\\) matrix, so v will be a \\(2\\times1\\) vector to make the matrix multiplication work.\n\\[\n\\underset{2\\times2}{\\mathbf{A}}~\\underset{2\\times1}{\\mathbf{v}} = \\lambda \\underset{2\\times1}{\\mathbf{v}}\n\\]\nNotice that we can also re-arrange the terms of this expression:\n\\[\n\\begin{split}\n\\mathbf{A}\\mathbf{v} &= \\lambda \\mathbf{v} \\\\[2ex]\n\\mathbf{A}\\mathbf{v} - \\lambda \\mathbf{v} &= 0\\\\[2ex]\n\\big(\\mathbf{A} - \\lambda \\mathbf{I}\\big) \\mathbf{v} &= 0\\\\[2ex]\n\\end{split}\n\\]\nIn order to obtain a nonzero solution for v, the determinant of \\(\\mathbf{A} - \\lambda \\mathbf{I}\\) needs to be zero. This is why the eigenvalues are the \\(\\lambda\\)-values that make this nonsingular.\n\n\n13.2.1 Finding the Elements of v\nWe can use matrix algebra to solve for the elements of vector v using each eigenvalue seperately.\n\\[\n\\begin{split}\n\\mathbf{A}\\mathbf{v} &= \\lambda \\mathbf{v} \\\\[1em]\n\\begin{bmatrix}\n-3 & 5  \\\\\n4 & -2  \\\\\n\\end{bmatrix}\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\n\\end{bmatrix} &= -7\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\n\\end{bmatrix}\\\\[1em]\n\\end{split}\n\\]\nThis produces a system of two equations with two unknowns:\n\\[\n\\begin{split}\n-3v_1 + 5v_2 &= -7v_1 \\\\\n4v_1 - 2v_2 &= -7v_2\n\\end{split}\n\\]\nSimplifying this, we get\n\\[\n\\begin{split}\n4v_1 + 5v_2 &= 0 \\\\\n4v_1 + 5v_2 &= 0\n\\end{split}\n\\]\nThe homogeneous set of equations means that there are an infinite number of solutions. The general solution here is to express one variable (say \\(v_2\\)) as a function of the other.\n\\[\n\\begin{split}\nv_1 &= \\theta \\\\[0.5em]\nv_2 &= -\\frac{4}{5}\\theta\n\\end{split}\n\\]\nAny set of \\(v_1\\) and \\(v_2\\) in which \\(v_2 = -\\frac{4}{5}v_1\\) will satisfy this set of equations.\n\n\n\n13.2.2 Normalized Eigenvectors\nAlthough there are an infinite number of solutions, one that is particularly nice is that whose a sum of squared values is equal to 1, that is \\(\\mathbf{v}^\\intercal\\mathbf{v}=1\\). This is referred to as normalizing the eigenvector. Normalized eigenvectors are denoted as e.\n\\[\n\\begin{split}\n\\mathbf{e}^\\intercal \\mathbf{e} &= 1 \\\\[2ex]\ne_1^2 + e_2^2 &= 1 \\\\[2ex]\n\\theta^2 + (-\\frac{4}{5}\\theta)^2 &= 1 \\\\[2ex]\n\\frac{41}{25}\\theta^2 &= 1 \\\\[2ex]\n\\theta^2 &= \\frac{25}{41} \\\\[2ex]\n\\theta &= \\sqrt{\\frac{25}{41}} \\\\[2ex]\n&=\\frac{5}{\\sqrt{41}}\n\\end{split}\n\\]\nWhich implies that,\n\\[\n\\begin{split}\ne_1 &= \\frac{5}{\\sqrt{41}} \\\\[1em]\ne_2 &= -\\frac{4}{\\sqrt{41}}\n\\end{split}\n\\]\nAnd the normalized eigenvector corresponding to the eigenvalue of \\(-7\\) is:\n\\[\n\\mathbf{e}_1x = \\begin{bmatrix}\\frac{5}{\\sqrt{41}} \\\\ -\\frac{4}{\\sqrt{41}}\\end{bmatrix}\n\\]\nWe could verify this by ensuring that the the equation \\(\\mathbf{A}\\mathbf{e}=\\lambda\\mathbf{e}\\) holds:\n\\[\n\\begin{bmatrix}\n-3 & 5  \\\\\n4 & -2  \\\\\n\\end{bmatrix}\\begin{bmatrix}\\frac{5}{\\sqrt{41}} \\\\ -\\frac{4}{\\sqrt{41}}\\end{bmatrix} = -7\\begin{bmatrix}\\frac{5}{\\sqrt{41}} \\\\ -\\frac{4}{\\sqrt{41}}\\end{bmatrix}\n\\]\nWe can follow the same process for the second eigenvector which corresponds to the eigenvalue of 2. This produces an eigenvector of:\n\\[\n\\mathbf{e}_2 = \\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{bmatrix}\n\\]\nWhich can again be verified.",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigenvalues and Eigenvectors</span>"
    ]
  },
  {
    "objectID": "23-eigenvalues-and-eigenvectors.html#using-r-to-find-eigenvalues-and-eigenvectors",
    "href": "23-eigenvalues-and-eigenvectors.html#using-r-to-find-eigenvalues-and-eigenvectors",
    "title": "13  Eigenvalues and Eigenvectors",
    "section": "13.3 Using R to Find Eigenvalues and Eigenvectors",
    "text": "13.3 Using R to Find Eigenvalues and Eigenvectors\nWe can use the eigen() function to find the eigenvalues and eigenvectors for a matrix. The syntax to find these for our example matrix A is:\n\n# Create A\nA = matrix(\n  data = c(-3, 4, 5, -2), \n  nrow = 2\n  )\n\n\n# Compute eigenvalues and eigenvectors\neigen(A)\n\neigen() decomposition\n$values\n[1] -7  2\n\n$vectors\n           [,1]       [,2]\n[1,] -0.7808688 -0.7071068\n[2,]  0.6246950 -0.7071068\n\n\nThe eigenvectors for each of the eigenvalues are presented in the columns of the matrix given in the `\\(vector\\) component of the output. For example, the eigenvector associated with the eigenvalue of \\(-7\\) is presented in the first column of the matrix, and that associated with the eigenvalue of 2 is presented in the second column of the matrix.\n\nFYI\nNote that the signs for all elements of any particular eigenvector can be switched since the vector would represent the same vector space. Thus the second eigenvector outputted here has negative values.\n\n\nTogether, the eigenvalues and eigenvectors make up the eigenstructure of matrix A. In our example, the eigensructure of A is:\n\\[\n\\begin{split}\n\\lambda_1 &= -7 \\qquad &\\mathbf{e}_1 = \\begin{bmatrix}\\frac{5}{\\sqrt{41}} \\\\ -\\frac{4}{\\sqrt{41}}\\end{bmatrix}\\\\[2ex]\n\\lambda_2 &= 2 \\qquad &\\mathbf{e}_2 = \\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{bmatrix} \\\\[2ex]\n\\end{split}\n\\]",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigenvalues and Eigenvectors</span>"
    ]
  },
  {
    "objectID": "24-statistical-application-matrices.html",
    "href": "24-statistical-application-matrices.html",
    "title": "14  Statistical Application: SSCP, Variance–Covariance, and Correlation Matrices",
    "section": "",
    "text": "14.1 Deviation Scores\nIn this chapter, we will provide an example of how matrix operations are used in statistics to compute a sum of squares and cross-product (SSCP) matrix. To do this we will consider the following data set:\nThe data matrix (omitting ID values) could be conceived of as a \\(6 \\times 4\\) matrix:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n  560 & 3.0 & 11 & 112 \\\\\n  780 & 3.9 & 10 & 143 \\\\\n  620 & 2.9 & 19 & 124 \\\\\n  600 & 2.7 & 7 & 129 \\\\\n  720 & 3.7 & 18 & 130 \\\\\n  380 & 2.4 & 13 & 82\n  \\end{bmatrix}\n\\]\nSince so much of statistics is dependent on the use of deviation scores, we will compute a matrix of means, where each column corresponds to the mean of the corresponding column mean of X, and subtract that from the original matrix X.\n\\[\n\\begin{split}\n\\mathbf{D} &= \\mathbf{X} - \\mathbf{M} \\\\[2ex]\n&= \\begin{bmatrix}\n  560 & 3.0 & 11 & 112 \\\\\n  780 & 3.9 & 10 & 143 \\\\\n  620 & 2.9 & 19 & 124 \\\\\n  600 & 2.7 & 7 & 129 \\\\\n  720 & 3.7 & 18 & 130 \\\\\n  380 & 2.4 & 13 & 82\n  \\end{bmatrix} - \\begin{bmatrix}\n  610 & 3.1 & 13 & 120 \\\\\n  610 & 3.1 & 13 & 120 \\\\\n  610 & 3.1 & 13 & 120 \\\\\n  610 & 3.1 & 13 & 120 \\\\\n  610 & 3.1 & 13 & 120 \\\\\n  610 & 3.1 & 13 & 120\n  \\end{bmatrix} \\\\[2ex]\n  &= \\begin{bmatrix}\n  -50  & -0.1 & -2 & -8 \\\\\n  170  & 0.8  & -3 & 23 \\\\\n   10  & -0.2 &  6 & 4 \\\\\n  -10  & -0.4 & -6 & 9 \\\\\n  110  & 0.6  &  5 & 10 \\\\\n  -230 & -0.7 &  0 & -38\n  \\end{bmatrix}\n\\end{split}  \n\\]\nUsing R, the following commands produce the deviation matrix.\n# Create X\nX = matrix(\n  data = c(560, 3.0, 11, 112,\n           780, 3.9, 10, 143,\n           620, 2.9, 19, 124,\n           600, 2.7,  7, 129, \n           720, 3.7, 18, 130,\n           380, 2.4, 13, 82),\n  byrow = TRUE,\n  ncol = 4\n)\n\n# Create a ones column vector with six elements\nones = rep(1, 6)\n\n# Compute M (mean matrix)\nM = ones %*% t(ones) %*% X * (1/6)\nM\n\n     [,1] [,2] [,3] [,4]\n[1,]  610  3.1   13  120\n[2,]  610  3.1   13  120\n[3,]  610  3.1   13  120\n[4,]  610  3.1   13  120\n[5,]  610  3.1   13  120\n[6,]  610  3.1   13  120\n\n# Compute deviation matrix (D)\nD = X - M\nD\n\n     [,1] [,2] [,3] [,4]\n[1,]  -50 -0.1   -2   -8\n[2,]  170  0.8   -3   23\n[3,]   10 -0.2    6    4\n[4,]  -10 -0.4   -6    9\n[5,]  110  0.6    5   10\n[6,] -230 -0.7    0  -38",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical Application: SSCP, Variance--Covariance, and Correlation Matrices</span>"
    ]
  },
  {
    "objectID": "24-statistical-application-matrices.html#deviation-scores",
    "href": "24-statistical-application-matrices.html#deviation-scores",
    "title": "14  Statistical Application: SSCP, Variance–Covariance, and Correlation Matrices",
    "section": "",
    "text": "14.1.1 Creating the Mean Matrix\nHere we used matrix operations to compute the mean matrix rather than built-in R functions. Taking a ones column vector of n elements and post-multiplying by its transpose produces a \\(n \\times n\\) matrix where all the elements are one; in our case we get a \\(6 \\times 6\\) ones matrix.\n\nones %*% t(ones)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    1    1    1    1    1\n[2,]    1    1    1    1    1    1\n[3,]    1    1    1    1    1    1\n[4,]    1    1    1    1    1    1\n[5,]    1    1    1    1    1    1\n[6,]    1    1    1    1    1    1\n\n\nThis ones matrix is then post-multiplied by X. Since we are post-multiplyinga ones matrix, the resultin matrix will have the same elements in every row. Moreover these elements will be the column sums of X.\n\nones %*% t(ones) %*% X\n\n     [,1] [,2] [,3] [,4]\n[1,] 3660 18.6   78  720\n[2,] 3660 18.6   78  720\n[3,] 3660 18.6   78  720\n[4,] 3660 18.6   78  720\n[5,] 3660 18.6   78  720\n[6,] 3660 18.6   78  720\n\n# Compute column sums\ncolSums(X)\n\n[1] 3660.0   18.6   78.0  720.0\n\n\nFinally, since we want the means, we multiply by the scalar \\(\\frac{1}{n}\\), where n is the number of rows. This gets us the mean matrix, all via matrix operations.\n\nones %*% t(ones) %*% X * (1/6)\n\n     [,1] [,2] [,3] [,4]\n[1,]  610  3.1   13  120\n[2,]  610  3.1   13  120\n[3,]  610  3.1   13  120\n[4,]  610  3.1   13  120\n[5,]  610  3.1   13  120\n[6,]  610  3.1   13  120\n\n\nUsing mathematical notation, the deviation matrix can be expressed as:\n\\[\n\\begin{split}\n\\underset{6 \\times 4}{\\mathbf{D}} &= \\underset{6 \\times 4}{\\mathbf{X}} - \\underset{6 \\times 4}{\\mathbf{M}} \\\\[2ex]\n&= \\begin{bmatrix}\n  X_{1_{1}} & X_{2_{1}} & X_{3_{1}} & X_{4_{1}} \\\\\n  X_{1_{2}} & X_{2_{2}} & X_{3_{2}} & X_{4_{2}} \\\\\n  X_{1_{3}} & X_{2_{3}} & X_{3_{3}} & X_{4_{3}} \\\\\n  X_{1_{4}} & X_{2_{4}} & X_{3_{4}} & X_{4_{4}} \\\\\n  X_{1_{5}} & X_{2_{5}} & X_{3_{5}} & X_{4_{5}} \\\\\n  X_{1_{6}} & X_{2_{6}} & X_{3_{6}} & X_{4_{6}}\n  \\end{bmatrix} - \\begin{bmatrix}\n  \\bar{X}_1 & \\bar{X}_2 & \\bar{X}_3 & \\bar{X}_4 \\\\\n  \\bar{X}_1 & \\bar{X}_2 & \\bar{X}_3 & \\bar{X}_4 \\\\\n  \\bar{X}_1 & \\bar{X}_2 & \\bar{X}_3 & \\bar{X}_4 \\\\\n  \\bar{X}_1 & \\bar{X}_2 & \\bar{X}_3 & \\bar{X}_4 \\\\\n  \\bar{X}_1 & \\bar{X}_2 & \\bar{X}_3 & \\bar{X}_4 \\\\\n  \\bar{X}_1 & \\bar{X}_2 & \\bar{X}_3 & \\bar{X}_4\n  \\end{bmatrix} \\\\[2ex]\n  &= \\begin{bmatrix}\n  (X_{1_{1}} - \\bar{X}_1) & (X_{2_{1}} - \\bar{X}_2) & (X_{3_{1}} - \\bar{X}_3) & (X_{4_{1}} - \\bar{X}_4) \\\\\n  (X_{1_{2}} - \\bar{X}_1) & (X_{2_{2}} - \\bar{X}_2) & (X_{3_{2}} - \\bar{X}_3) & (X_{4_{2}} - \\bar{X}_4) \\\\\n  (X_{1_{3}} - \\bar{X}_1) & (X_{2_{3}} - \\bar{X}_2) & (X_{3_{3}} - \\bar{X}_3) & (X_{4_{3}} - \\bar{X}_4) \\\\\n  (X_{1_{4}} - \\bar{X}_1) & (X_{2_{4}} - \\bar{X}_2) & (X_{3_{4}} - \\bar{X}_3) & (X_{4_{4}} - \\bar{X}_4) \\\\\n  (X_{1_{5}} - \\bar{X}_1) & (X_{2_{5}} - \\bar{X}_2) & (X_{3_{5}} - \\bar{X}_3) & (X_{4_{5}} - \\bar{X}_4) \\\\\n  (X_{1_{6}} - \\bar{X}_1) & (X_{2_{6}} - \\bar{X}_2) & (X_{3_{6}} - \\bar{X}_3) & (X_{4_{6}} - \\bar{X}_4)\n  \\end{bmatrix}\n\\end{split}  \n\\]",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical Application: SSCP, Variance--Covariance, and Correlation Matrices</span>"
    ]
  },
  {
    "objectID": "24-statistical-application-matrices.html#sscp-matrix",
    "href": "24-statistical-application-matrices.html#sscp-matrix",
    "title": "14  Statistical Application: SSCP, Variance–Covariance, and Correlation Matrices",
    "section": "14.2 SSCP Matrix",
    "text": "14.2 SSCP Matrix\nTo obtain the sums of squares and cross products (SSCP) matrix, we can pre-multiply D by its transpose.\n\\[\n\\scriptscriptstyle\n\\begin{split}\n\\underset{4 \\times 4}{\\mathbf{SSCP}} &= \\underset{4 \\times 6}{\\mathbf{D}}^\\intercal\\underset{6 \\times 4}{\\mathbf{D}} \\\\[2ex]\n&= \\begin{bmatrix}\n  (X_{1_{1}} - \\bar{X}_1) & (X_{1_{2}} - \\bar{X}_1) & (X_{1_{3}} - \\bar{X}_1) & (X_{1_{4}} - \\bar{X}_1) & (X_{1_{5}} - \\bar{X}_1) & (X_{1_{6}} - \\bar{X}_1) \\\\\n  (X_{2_{1}} - \\bar{X}_2) & (X_{2_{2}} - \\bar{X}_2) & (X_{2_{3}} - \\bar{X}_2) & (X_{2_{4}} - \\bar{X}_2) & (X_{2_{5}} - \\bar{X}_2) & (X_{2_{6}} - \\bar{X}_2) \\\\\n  (X_{3_{1}} - \\bar{X}_3) & (X_{3_{2}} - \\bar{X}_3) & (X_{3_{3}} - \\bar{X}_3) & (X_{3_{4}} - \\bar{X}_3) & (X_{3_{5}} - \\bar{X}_3) & (X_{3_{6}} - \\bar{X}_3) \\\\\n  (X_{4_{1}} - \\bar{X}_4) & (X_{4_{2}} - \\bar{X}_4) & (X_{4_{3}} - \\bar{X}_4) & (X_{4_{4}} - \\bar{X}_4) & (X_{4_{5}} - \\bar{X}_4) & (X_{4_{6}} - \\bar{X}_4)\n\\end{bmatrix}\n\\begin{bmatrix}\n  (X_{1_{1}} - \\bar{X}_1) & (X_{2_{1}} - \\bar{X}_2) & (X_{3_{1}} - \\bar{X}_3) & (X_{4_{1}} - \\bar{X}_4) \\\\\n  (X_{1_{2}} - \\bar{X}_1) & (X_{2_{2}} - \\bar{X}_2) & (X_{3_{2}} - \\bar{X}_3) & (X_{4_{2}} - \\bar{X}_4) \\\\\n  (X_{1_{3}} - \\bar{X}_1) & (X_{2_{3}} - \\bar{X}_2) & (X_{3_{3}} - \\bar{X}_3) & (X_{4_{3}} - \\bar{X}_4) \\\\\n  (X_{1_{4}} - \\bar{X}_1) & (X_{2_{4}} - \\bar{X}_2) & (X_{3_{4}} - \\bar{X}_3) & (X_{4_{4}} - \\bar{X}_4) \\\\\n  (X_{1_{5}} - \\bar{X}_1) & (X_{2_{5}} - \\bar{X}_2) & (X_{3_{5}} - \\bar{X}_3) & (X_{4_{5}} - \\bar{X}_4) \\\\\n  (X_{1_{6}} - \\bar{X}_1) & (X_{2_{6}} - \\bar{X}_2) & (X_{3_{6}} - \\bar{X}_3) & (X_{4_{6}} - \\bar{X}_4)\n  \\end{bmatrix} \\\\[2ex]\n  &= \\begin{bmatrix}\n  \\sum (X_{1_{i}} - \\bar{X}_1)^2 & \\sum (X_{1_{i}} - \\bar{X}_1)(X_{2_{i}} - \\bar{X}_2) & \\sum (X_{1_{i}} - \\bar{X}_1)(X_{3_{i}} - \\bar{X}_3) & \\sum (X_{1_{i}} - \\bar{X}_1)(X_{4_{i}} - \\bar{X}_4) \\\\\n  \\sum (X_{1_{i}} - \\bar{X}_1)(X_{2_{i}} - \\bar{X}_2) & \\sum (X_{2_{i}} - \\bar{X}_2)^2 & \\sum (X_{2_{i}} - \\bar{X}_2)(X_{3_{i}} - \\bar{X}_3) & \\sum (X_{2_{i}} - \\bar{X}_2)(X_{4_{i}} - \\bar{X}_4) \\\\\n  \\sum (X_{1_{i}} - \\bar{X}_1)(X_{3_{i}} - \\bar{X}_3) & \\sum (X_{2_{i}} - \\bar{X}_2)(X_{3_{i}} - \\bar{X}_3) & \\sum (X_{3_{i}} - \\bar{X}_3)^2 & \\sum (X_{3_{i}} - \\bar{X}_3)(X_{4_{i}} - \\bar{X}_4) \\\\\n  \\sum (X_{1_{i}} - \\bar{X}_1)(X_{4_{i}} - \\bar{X}_4) & \\sum (X_{2_{i}} - \\bar{X}_2)(X_{4_{i}} - \\bar{X}_4) & \\sum (X_{3_{i}} - \\bar{X}_3)(X_{4_{i}} - \\bar{X}_4) & \\sum (X_{4_{i}} - \\bar{X}_4)^2\n  \\end{bmatrix}\n\\end{split}  \n\\]\nThe diagonal of the SSCP matrix contains the sums of squared deviations for each of the variables (columns) represented in X. For example, the element in the first row and first column of the SSCP matrix (\\(\\mathbf{SSCP}_{11}\\)) is the sum of squared deviations for the SAT variable. The off-diagonal elements in the SSCP matrix are the cross-products of the deviation scores between the different variables. That is:\n\\[\n\\mathbf{SSCP} = \\begin{bmatrix}\n  \\mathrm{SS}_1 & \\mathrm{CP}_{12} & \\mathrm{CP}_{13} & \\mathrm{CP}_{14} \\\\\n  \\mathrm{CP}_{21} & \\mathrm{SS}_2 & \\mathrm{CP}_{23} & \\mathrm{CP}_{24} \\\\\n  \\mathrm{CP}_{21} & \\mathrm{CP}_{32} & \\mathrm{SS}_3 & \\mathrm{CP}_{34} \\\\\n  \\mathrm{CP}_{41} & \\mathrm{CP}_{42} & \\mathrm{CP}_{43} & \\mathrm{SS}_4\n  \\end{bmatrix}\n\\] Also note that the SSCP matrix is square and symmetric.\nUsing R:\n\n# Compute SSCP matrix\nSSCP = t(D) %*% D\n\n# View SSCP matrix\nSSCP\n\n      [,1]  [,2] [,3]    [,4]\n[1,] 96600 370.0  260 14100.0\n[2,]   370   1.7    2    47.4\n[3,]   260   2.0  110   -33.0\n[4,] 14100  47.4  -33  2234.0\n\n\n\nThe SSCP matrix is a scalar multiple of the variance–covariance matrix (\\(\\boldsymbol{\\Sigma}\\)). Namely,\n\\[\n\\mathbf{SSCP} = n (\\boldsymbol{\\Sigma})\n\\]\nThat is if me multiply the SSCP matrix by \\(\\frac{1}{n}\\) we obtain the variance–covariance matrix. Mathematically,\n\\[\n\\scriptstyle\n\\begin{split}\n\\underset{4 \\times 4}{\\boldsymbol{\\Sigma}} &= \\frac{1}{n}(\\underset{4 \\times 4}{\\mathbf{SSCP}}) \\\\[2ex]\n&= \\frac{1}{n} \\begin{bmatrix}\n  \\sum (X_{1_{i}} - \\bar{X}_1)^2 & \\sum (X_{1_{i}} - \\bar{X}_1)(X_{2_{i}} - \\bar{X}_2) & \\sum (X_{1_{i}} - \\bar{X}_1)(X_{3_{i}} - \\bar{X}_3) & \\sum (X_{1_{i}} - \\bar{X}_1)(X_{4_{i}} - \\bar{X}_4) \\\\\n  \\sum (X_{1_{i}} - \\bar{X}_1)(X_{2_{i}} - \\bar{X}_2) & \\sum (X_{2_{i}} - \\bar{X}_2)^2 & \\sum (X_{2_{i}} - \\bar{X}_2)(X_{3_{i}} - \\bar{X}_3) & \\sum (X_{2_{i}} - \\bar{X}_2)(X_{4_{i}} - \\bar{X}_4) \\\\\n  \\sum (X_{1_{i}} - \\bar{X}_1)(X_{3_{i}} - \\bar{X}_3) & \\sum (X_{2_{i}} - \\bar{X}_2)(X_{3_{i}} - \\bar{X}_3) & \\sum (X_{3_{i}} - \\bar{X}_3)^2 & \\sum (X_{3_{i}} - \\bar{X}_3)(X_{4_{i}} - \\bar{X}_4) \\\\\n  \\sum (X_{1_{i}} - \\bar{X}_1)(X_{4_{i}} - \\bar{X}_4) & \\sum (X_{2_{i}} - \\bar{X}_2)(X_{4_{i}} - \\bar{X}_4) & \\sum (X_{3_{i}} - \\bar{X}_3)(X_{4_{i}} - \\bar{X}_4) & \\sum (X_{4_{i}} - \\bar{X}_4)^2\n  \\end{bmatrix} \\\\[2ex]\n  &= \\begin{bmatrix}\n  \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)^2}{n} & \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)(X_{2_{i}} - \\bar{X}_2)}{n} & \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)(X_{3_{i}} - \\bar{X}_3)}{n} & \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)(X_{4_{i}} - \\bar{X}_4)}{n} \\\\\n  \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)(X_{2_{i}} - \\bar{X}_2)}{n} & \\frac{\\sum (X_{2_{i}} - \\bar{X}_2)^2}{n} & \\frac{\\sum (X_{2_{i}} - \\bar{X}_2)(X_{3_{i}} - \\bar{X}_3)}{n} & \\frac{\\sum (X_{2_{i}} - \\bar{X}_2)(X_{4_{i}} - \\bar{X}_4)}{n} \\\\\n  \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)(X_{3_{i}} - \\bar{X}_3)}{n} & \\frac{\\sum (X_{2_{i}} - \\bar{X}_2)(X_{3_{i}} - \\bar{X}_3)}{n} & \\frac{\\sum (X_{3_{i}} - \\bar{X}_3)^2}{n} & \\frac{\\sum (X_{3_{i}} - \\bar{X}_3)(X_{4_{i}} - \\bar{X}_4)}{n} \\\\\n  \\frac{\\sum (X_{1_{i}} - \\bar{X}_1)(X_{4_{i}} - \\bar{X}_4)}{n} & \\frac{\\sum (X_{2_{i}} - \\bar{X}_2)(X_{4_{i}} - \\bar{X}_4)}{n} & \\frac{\\sum (X_{3_{i}} - \\bar{X}_3)(X_{4_{i}} - \\bar{X}_4)}{n} & \\frac{\\sum (X_{4_{i}} - \\bar{X}_4)^2}{n}\n  \\end{bmatrix}\n\\end{split}  \n\\]\nThe diagonal of the \\(\\boldsymbol{\\Sigma}\\) matrix contains the variances for each of the variables (columns) represented in X. For example, the element in the first row and first column of \\(\\boldsymbol{\\Sigma}\\) (\\(\\boldsymbol{\\Sigma}_{11}\\)) is the variance for the SAT variable. The off-diagonal elements in \\(\\boldsymbol{\\Sigma}\\) are the covariances between the different variables. That is:\n\\[\n\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n  \\mathrm{Var}(X_1) & \\mathrm{Cov}(X_1,X_2) & \\mathrm{Cov}(X_1,X_3) & \\mathrm{Cov}(X_1,X_4) \\\\\n  \\mathrm{Cov}(X_2,X_1) & \\mathrm{Var}(X_2) & \\mathrm{Cov}(X_2,X_3) & \\mathrm{Cov}(X_2,X_4) \\\\\n  \\mathrm{Cov}(X_3,X_1) & \\mathrm{Cov}(X_3,X_2) & \\mathrm{Var}(X_3) & \\mathrm{Cov}(X_3,X_4) \\\\\n  \\mathrm{Cov}(X_4,X_1) & \\mathrm{Cov}(X_4,X_2) & \\mathrm{Cov}(X_4,X_3) & \\mathrm{Var}(X_4)\n  \\end{bmatrix}\n\\]\nSimilar to the SSCP matrix, \\(\\boldsymbol{\\Sigma}\\) is also a square, symmetric matrix.\nUsing R:\n\n# Compute SIGMA matrix\nSIGMA = SSCP * (1/6)\n\n# View SIGMA matrix\nSIGMA\n\n            [,1]       [,2]       [,3]      [,4]\n[1,] 16100.00000 61.6666667 43.3333333 2350.0000\n[2,]    61.66667  0.2833333  0.3333333    7.9000\n[3,]    43.33333  0.3333333 18.3333333   -5.5000\n[4,]  2350.00000  7.9000000 -5.5000000  372.3333\n\n\nThe var() function can also be applied to X directly to obtain the \\(\\boldsymbol{\\Sigma}\\) matrix. Note that the var() function uses \\(\\frac{1}{n-1}\\) rather than \\(\\frac{1}{n}\\) as the scalar multiple to obtain the variance–covariance matrix.\n\n# Obtain SIGMA directly\nvar(X)\n\n      [,1]  [,2] [,3]    [,4]\n[1,] 19320 74.00 52.0 2820.00\n[2,]    74  0.34  0.4    9.48\n[3,]    52  0.40 22.0   -6.60\n[4,]  2820  9.48 -6.6  446.80\n\n# Check\nSSCP * (1/5)\n\n      [,1]  [,2] [,3]    [,4]\n[1,] 19320 74.00 52.0 2820.00\n[2,]    74  0.34  0.4    9.48\n[3,]    52  0.40 22.0   -6.60\n[4,]  2820  9.48 -6.6  446.80",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical Application: SSCP, Variance--Covariance, and Correlation Matrices</span>"
    ]
  },
  {
    "objectID": "24-statistical-application-matrices.html#correlation-matrix",
    "href": "24-statistical-application-matrices.html#correlation-matrix",
    "title": "14  Statistical Application: SSCP, Variance–Covariance, and Correlation Matrices",
    "section": "14.3 Correlation Matrix",
    "text": "14.3 Correlation Matrix\nWe can convert \\(\\boldsymbol{\\Sigma}\\) to a correlation matrix by standardizing it; that is dividing each element by its corresponding standard deviation. This is equivalent to pre- and post-multiplying \\(\\boldsymbol{\\Sigma}\\) by a scaling matrix, S, a diagonal matrix with elements equal to the reciprocal of the standard deviations of each of the variables:\n\\[\n\\mathbf{S} = \\begin{bmatrix}\n  \\frac{1}{\\mathrm{SD}(X_1)} & 0 & 0 & 0 \\\\\n  0 & \\frac{1}{\\mathrm{SD}(X_2)} & 0 & 0 \\\\\n  0 & 0 & \\frac{1}{\\mathrm{SD}(X_3)} & 0 \\\\\n  0 & 0 & 0 & \\frac{1}{\\mathrm{SD}(X_4)}\n\\end{bmatrix}\n\\]\nEmploying R, we can obtain the scaling matrix by first pulling out the diagonal elements of \\(\\boldsymbol{\\Sigma}\\) (the variances) and then using those to create S.\n\n# Get variances\nV = diag(SIGMA)\nV\n\n[1] 16100.0000000     0.2833333    18.3333333   372.3333333\n\n# Create S\nS = diag(1 / sqrt(V))\nS\n\n            [,1]     [,2]      [,3]       [,4]\n[1,] 0.007881104 0.000000 0.0000000 0.00000000\n[2,] 0.000000000 1.878673 0.0000000 0.00000000\n[3,] 0.000000000 0.000000 0.2335497 0.00000000\n[4,] 0.000000000 0.000000 0.0000000 0.05182437\n\n\nWe can now pre- and post-multiply \\(\\boldsymbol{\\Sigma}\\) by S to obtain the correlation matrix, R. That is,\n\\[\n\\mathbf{R} = \\mathbf{S}\\boldsymbol{\\Sigma}\\mathbf{S}\n\\] Since both S and \\(\\boldsymbol{\\Sigma}\\) are \\(4 \\times 4\\) matrices, R will also be a \\(4 \\times 4\\) matrix. Moreover, R will also be both square and symmetric.\n\nR = S %*% SIGMA %*% S\nR\n\n           [,1]      [,2]        [,3]        [,4]\n[1,] 1.00000000 0.9130377  0.07976061  0.95981817\n[2,] 0.91303768 1.0000000  0.14625448  0.76915222\n[3,] 0.07976061 0.1462545  1.00000000 -0.06656961\n[4,] 0.95981817 0.7691522 -0.06656961  1.00000000\n\n\nAgain, the cor() function could be applied directly to X, noting that the SSCP matrix would be scaled by \\(\\frac{1}{n-1}\\).",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical Application: SSCP, Variance--Covariance, and Correlation Matrices</span>"
    ]
  },
  {
    "objectID": "24-statistical-application-matrices.html#standardized-scores",
    "href": "24-statistical-application-matrices.html#standardized-scores",
    "title": "14  Statistical Application: SSCP, Variance–Covariance, and Correlation Matrices",
    "section": "14.4 Standardized Scores",
    "text": "14.4 Standardized Scores\nStandardized scores (z-scores) can be computed for the original values in X by post-multiplying the deviation matrix (D) by the same scaling matrix, S.\n\\[\n\\mathbf{Z} = \\mathbf{DS}\n\\]\n\n# Compute standardized scores\nZ = D %*% S\nZ\n\n            [,1]       [,2]       [,3]       [,4]\n[1,] -0.39405520 -0.1878673 -0.4670994 -0.4145950\n[2,]  1.33978769  1.5029383 -0.7006490  1.1919605\n[3,]  0.07881104 -0.3757346  1.4012981  0.2072975\n[4,] -0.07881104 -0.7514691 -1.4012981  0.4664193\n[5,]  0.86692145  1.1272037  1.1677484  0.5182437\n[6,] -1.81265393 -1.3150710  0.0000000 -1.9693261\n\n\nWe can also use Z to compute the correlation matrix,\n\\[\n\\mathbf{R} = \\frac{\\mathbf{Z}^\\intercal\\mathbf{Z}}{n}\n\\]\n\n# Compute R\nt(Z) %*% Z * (1/6)\n\n           [,1]      [,2]        [,3]        [,4]\n[1,] 1.00000000 0.9130377  0.07976061  0.95981817\n[2,] 0.91303768 1.0000000  0.14625448  0.76915222\n[3,] 0.07976061 0.1462545  1.00000000 -0.06656961\n[4,] 0.95981817 0.7691522 -0.06656961  1.00000000",
    "crumbs": [
      "Square Matrices and Their Properties",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical Application: SSCP, Variance--Covariance, and Correlation Matrices</span>"
    ]
  },
  {
    "objectID": "30-basis-vectors-and-matrices.html",
    "href": "30-basis-vectors-and-matrices.html",
    "title": "15  Basis Vectors and Matrices",
    "section": "",
    "text": "15.1 An Example\nIn this chapter, you will learn about basis vectors and basis matrices and how they are used to define the dimensionality of a coordinate space. You will also learn how to transition to different coordinate systems by changing the basis vectors.\nIn the Vectors chapter, we learned that a vector could be represented in a coordinate system where each element in the vector corresponds to a distance along one of the reference axes defining the coordinate system. For example, the vector:\n\\[\n\\mathbf{a} = \\begin{bmatrix}\n-3 \\\\ 4\n\\end{bmatrix}\n\\]\nhas a distance of \\(-3\\) on the first reference axis (R1) and a distance of 4 on the second reference axis (R2). This vector and the reference axes are shown in Figure 15.1. The reference axes (R1 and R2) define a two-dimensional coordinate space.\nFigure 15.1: Plot showing vector a (in teal) in the R1–R2 dimensional space.\nThe two-dimensional coordinate system in our example, the Cartesian coordinate system, is defined by two orthogonal vectors called the basis vectors because they form the basis for the two-dimensional space. (These are shown in black in Figure 15.1.) More specifically, the dimensions of a system are determined by the number of independent vectors within that system. For example, in a three-dimensional coordinate space there are three independent vectors (e.g., that form the x, y, and z axes).\nThe span of the basis vectors create what we think of as the axes for that system. In our example, we have two basis vectors whose spans create R1 and R2. There are many possible sets of basis vectors whose spans would create R1 and R2, but the standard basis is the set of elementary vectors, in two-dimensional space:\n\\[\n\\mathbf{e}_1 = \\begin{bmatrix}\n1 \\\\ 0\n\\end{bmatrix} \\qquad\\mathbf{e}_2 = \\begin{bmatrix}\n0 \\\\ 1\n\\end{bmatrix}\n\\]\nHere the two basis vectors are orthogonal and have unit length. We refer to vectors that have these properties as orthonormal.\nWe can represent any vector in the space as a linear combination of a set of scalars and the basis vectors. For example consider the vector:\n\\[\n\\mathbf{a} = \\begin{bmatrix}\n-3 \\\\ 4\n\\end{bmatrix}\n\\]\nWe can also express a as:\n\\[\n\\begin{split}\n\\mathbf{a} &= -3 \\begin{bmatrix}\n1 \\\\ 0\n\\end{bmatrix} + 4 \\begin{bmatrix}\n0 \\\\ 1\n\\end{bmatrix} \\\\[2em]\n&= -3\\mathbf{e}_1 + 4\\mathbf{e}_2\n\\end{split}\n\\]\nIn general a vector a is can be expressed relative to the basis vectors as:\n\\[\n\\mathbf{a} = a_1(\\mathbf{e}_1) + a_2(\\mathbf{e}_2) + a_3(\\mathbf{e}_3) + \\dots + a_n(\\mathbf{e}_n)\n\\]\nwhere \\(\\mathbf{e}_i\\) is the ith orthonormal basis vector.",
    "crumbs": [
      "Matrix Transformations",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Basis Vectors and Matrices</span>"
    ]
  },
  {
    "objectID": "30-basis-vectors-and-matrices.html#changing-basis-vectors",
    "href": "30-basis-vectors-and-matrices.html#changing-basis-vectors",
    "title": "15  Basis Vectors and Matrices",
    "section": "15.2 Changing Basis Vectors",
    "text": "15.2 Changing Basis Vectors\nThere is nothing sacred about the Cartesian coordinate system. We could also define the two-dimensional space using a different set of orthonormal basis vectors. For example consider the basis vectors:\n\\[\n\\mathbf{b}_1 = \\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2}\n\\end{bmatrix} \\qquad\\mathbf{b}_2 = \\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2}\n\\end{bmatrix}\n\\]\nThese basis vectors still are orthonormal—they are orthogonal and their length is one. However, they are not elementary vectors. Not all orthonormal vectors are elementary vectors. R1 and R2 are still defined as the spans of these vectors, but now the vector a (which had the coordinates \\(-3, 4\\) in the system defined by the standard basis) has a different set of coordinates in the new basis. We can see this in Figure 15.2.\n\n\n\n\n\nFigure 15.2: Plot showing vector a (in teal) in the R1–R2 dimensional space with basis vectors b1 and b2.\n\n\n\n\n\n\n\n\nThe coordinates in the new basis for a is:\n\\[\n\\mathbf{a}=\\begin{bmatrix}\n-4.95 \\\\ 0.707\n\\end{bmatrix}\n\\]\nTo find the coordinates of a vector a under a non-standard basis, we can express a using the new basis vectors as:\n\\[\n\\begin{split}\n\\mathbf{a} &= a_1(\\mathbf{b}_1) + a_2(\\mathbf{b}_2) \\\\[2ex]\n&= a_1\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2}\n\\end{bmatrix} + a_2\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2}\n\\end{bmatrix}\n\\end{split}\n\\]\nThen, since a has coordinates (\\(-3,4\\)) in the standard system, we can re-write this as:\n\\[\n\\begin{split}\n\\begin{bmatrix}-3 \\\\ 4\n\\end{bmatrix} &= a_1\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2}\n\\end{bmatrix} + a_2\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2}\n\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}\n\\end{bmatrix}\\begin{bmatrix}a_1 \\\\ a_2\\end{bmatrix}\n\\end{split}\n\\] Now we need to solve for the values of \\(a_1\\) and \\(a_2\\). Note the basis vectors form the columns of the basis matrix, and we can solve for the values of \\(a_1\\) and \\(a_2\\) by pre-multiplying both sides by the inverse of the basis matrix.\n\n# Create basis matrix\nB = matrix(\n  data = c(sqrt(2)/2, -sqrt(2)/2, sqrt(2)/2, sqrt(2)/2), \n  nrow = 2\n  )\n\n# Create a\na = matrix(\n  data = c(-3, 4), \n  nrow = 2\n  )\n\n# Solve for the values of the new coordinates\nsolve(B) %*% a\n\n           [,1]\n[1,] -4.9497475\n[2,]  0.7071068\n\n\n\n\n15.2.1 Non-Orthonormal Basis\nThere is no specification that the basis vectors need to be orthogonal, or have length of one. For example consider the basis vectors:\n\\[\n\\mathbf{b}_1 = \\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2}\n\\end{bmatrix} \\qquad\\mathbf{b}_2 = \\begin{bmatrix}\n1.8 \\\\ 0.2\n\\end{bmatrix}\n\\]\nHere the basis vectors are no longer orthogonal. We refer to non-orthogonal vectors as oblique. The second basis vector also has a length which is no longer one. In this basis, the vector a (which had the coordinates \\(-3, 4\\) in the system defined by the standard basis) has yet a different set of coordinates; seen in Figure 15.3.\n\n\n\n\n\nFigure 15.3: Plot showing vector a (in teal) in the R1–R2 dimensional space with the oblique basis vectors b1 and b2.\n\n\n\n\n\n\n\n\nTo find the coordinates in the oblique basis, we solve\n\\[\n\\begin{bmatrix}-3 \\\\ 4\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} & 1.8 \\\\ \\frac{\\sqrt{2}}{2} & 0.2\n\\end{bmatrix} \\begin{bmatrix}a_1 \\\\ a_2\\end{bmatrix}\n\\]\nAs long as the basis matrix is conformable, we can solve this for \\(a_1\\) and \\(a_2\\).\n\n# Create basis matrix\nB = matrix(\n  data = c(sqrt(2)/2, sqrt(2)/2, 1.8, 0.2), \n  nrow = 2\n  )\n\n# Create a\na = matrix(\n  data = c(-3, 4), \n  nrow = 2\n  )\n\n# Solve for the values of the new coordinates\nsolve(B) %*% a\n\n          [,1]\n[1,]  6.894291\n[2,] -4.375000\n\n\nThe coordinates for a in the new basis is:\n\\[\n\\mathbf{a} = \\begin{bmatrix}6.894 \\\\ -4.375\\end{bmatrix}\n\\]",
    "crumbs": [
      "Matrix Transformations",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Basis Vectors and Matrices</span>"
    ]
  },
  {
    "objectID": "30-basis-vectors-and-matrices.html#converting-non-standard-coordinates-to-the-standard-basis",
    "href": "30-basis-vectors-and-matrices.html#converting-non-standard-coordinates-to-the-standard-basis",
    "title": "15  Basis Vectors and Matrices",
    "section": "15.3 Converting Non-Standard Coordinates to the Standard Basis",
    "text": "15.3 Converting Non-Standard Coordinates to the Standard Basis\nWe can, given a set of coordinates for a vector in a non-standard basis system, transform this to the coordinates of a vector in the standrd basis system. For example, consider the vector a, which has coordinates defined in the basis given by B:\n\\[\n\\mathbf{a} = \\begin{bmatrix}3 \\\\ 2\\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} & 1.8 \\\\ \\frac{\\sqrt{2}}{2} & 0.2\n\\end{bmatrix}\n\\]\nWhat are the coordinates of this vector in the standard basis system? Figure 15.3 shows this graphically.\n\n\n\n\n\nFigure 15.4: Plot showing vector a (in teal) in the R1–R2 non-standard basis defined by b1 and b2. The coordinates in the standard basis are found by determining the projection onto the span of the standard basis vectors (i.e., the R1 and R2 axes in the standard basis).\n\n\n\n\n\n\n\n\nTo find the coordinates of a in the standard basis system, we need to find the projections onto the spans of the standard basis vectors. To do this, we can use the same equation we have been using to convert coordinates to a non-standard basis. Recall, this equation is:\n\\[\n\\mathbf{a}_{[\\mathrm{S}]} = \\mathbf{B}_{[\\mathrm{NS}]}\\mathbf{a}_{[\\mathrm{NS}]}\n\\]\nwhere \\(\\mathbf{a}_{[\\mathrm{S}]}\\) is the vector a in the standard basis, \\(\\mathbf{a}_{[\\mathrm{NS}]}\\) is the vector a in the non-standard basis, and \\(\\mathbf{B}_{[\\mathrm{NS}]}\\) is the basis matrix defining the non-standard system. To determine the coordinates for a in the standard system, we simply postmultiply the non-standard basis matrix by the vector a in the non-standard system.\n\n# Create basis matrix\nB = matrix(\n  data = c(sqrt(2)/2, sqrt(2)/2, 1.8, 0.2), \n  nrow = 2\n  )\n\n# Create a (non-standard)\na = matrix(\n  data = c(3, 2), \n  nrow = 2\n  )\n\n# Find a (standard)\nB %*% a\n\n        [,1]\n[1,] 5.72132\n[2,] 2.52132\n\n\nThe coordinates for a in the standard basis sytem is:\n\\[\n\\mathbf{a} = \\begin{bmatrix}5.721 \\\\ 2.521\\end{bmatrix}\n\\]",
    "crumbs": [
      "Matrix Transformations",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Basis Vectors and Matrices</span>"
    ]
  },
  {
    "objectID": "30-basis-vectors-and-matrices.html#final-thoughts",
    "href": "30-basis-vectors-and-matrices.html#final-thoughts",
    "title": "15  Basis Vectors and Matrices",
    "section": "15.4 Final Thoughts",
    "text": "15.4 Final Thoughts\nAlthough we have been working in two dimensions so it is easier to show these ideas visually, basis vectors and matrices can, of course, be expanded to define coordinate systems in higher dimensions. For example, to define the standard three-dimensional space, we use the following basis vectors:\n\\[\n\\mathbf{e}_1 = \\begin{bmatrix}\n1 \\\\ 0 \\\\ 0\n\\end{bmatrix} \\qquad\\mathbf{e}_2 = \\begin{bmatrix}\n0 \\\\ 1 \\\\ 0\n\\end{bmatrix} \\qquad\\mathbf{e}_3 = \\begin{bmatrix}\n0 \\\\ 0 \\\\ 1\n\\end{bmatrix}\n\\]\nwhich can be expressed as the basis matrix B:\n\\[\n\\mathbf{B} = \\begin{bmatrix}\n1 & 0 & 0\\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\n\\end{bmatrix}\n\\]\nIn general, to define an n-dimensional space, we need n basis vectors of dimension \\(n \\times 1\\), which create an \\(n \\times n\\) basis matrix. This implies that the basis matrix will be square.\n\nFYI\nWhen we have a linear combinations that take on the form:\n\\[\n\\underset{n \\times 1}{\\mathbf{y}} = \\underset{n \\times n}{\\mathbf{B}} ~\\underset{n \\times 1}{\\mathbf{x}}\n\\] where y and x are vectors and B is a square matrix, then y is the projection of the vector x (which is defined using the basis matrix B) onto the set of standard basis vectors.",
    "crumbs": [
      "Matrix Transformations",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Basis Vectors and Matrices</span>"
    ]
  },
  {
    "objectID": "32-quadratic-forms.html",
    "href": "32-quadratic-forms.html",
    "title": "16  Quadratic Form of a Matrix",
    "section": "",
    "text": "16.1 Linear Forms\nIn this chapter, you will learn about the quadratic forms of a matrix. The quadratic forms of a matrix comes up often in statistical applications. For example the sum of squares can be expressed in quadratic form. Similarly the SSCP, covariance matrix, and correlation matrix are also examples of the quadratic form of a matrix. Before we introduce the quadratic form of a matrix, we first examine the linear and bilinear forms of a matrix.\nYou are already quite familiar with the linear form of a matrix. The linear form of a matrix is simply a linear mapping of that matrix. In scalar algebraic notation, we might write:\n\\[\nf(x) = a_1x_1 + a_2x_2 + a_3x_3 + \\ldots + a_nx_n\n\\]\nIt is linear since the x-values are all to the first degree. We can, equivalently use matrix notation to express this as:\n\\[\nf(\\mathbf{x}) = \\mathbf{a}^\\intercal\\mathbf{x}\n\\]\nwhere,\n\\[\n\\mathbf{a}^\\intercal = \\begin{bmatrix}\na_1 & a_2 & a_3 & \\ldots & a_n\n\\end{bmatrix} \\qquad \\mathrm{and}  \\qquad \\mathbf{x}=\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Matrix Transformations",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Quadratic Form of a Matrix</span>"
    ]
  },
  {
    "objectID": "32-quadratic-forms.html#bilinear-form",
    "href": "32-quadratic-forms.html#bilinear-form",
    "title": "16  Quadratic Form of a Matrix",
    "section": "16.2 Bilinear Form",
    "text": "16.2 Bilinear Form\nBilinear form of a matrix extends the linear form by including two variables, x and y. For example, if we had:\n\\[\n\\mathbf{x}=\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}  \\qquad \\mathbf{y}=\\begin{bmatrix}\ny_1 \\\\ y_2\n\\end{bmatrix}\n\\] Then the bilinear form written in scalar algebra is:\n\\[\nf(x,y) = a_{11}x_1y_1 + a_{21}x_2y_1 + a_{31}x_3y_1 + a_{12}x_1y_2 + a_{22}x_2y_2 + a_{32}x_3y_2\n\\]\nThe linear part of bilinear implies that both variables are to the first power. And, only one x and one y appear in each term. Using matrix notation, we can write:\n\\[\nf(\\mathbf{x},\\mathbf{y}) = \\mathbf{x}^\\intercal\\mathbf{A}\\mathbf{y}\n\\]\nwhere\n\\[\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\ a_{31} & a_{32}\n\\end{bmatrix}\n\\]\nWe refer to the bilinear form of matrix A. Note that there is more than one bilinear form for A; by changing the values in the x and y vectors, we can obtain many different bilinear mappings.",
    "crumbs": [
      "Matrix Transformations",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Quadratic Form of a Matrix</span>"
    ]
  },
  {
    "objectID": "32-quadratic-forms.html#quadratic-form",
    "href": "32-quadratic-forms.html#quadratic-form",
    "title": "16  Quadratic Form of a Matrix",
    "section": "16.3 Quadratic Form",
    "text": "16.3 Quadratic Form\nThe quadratic form is a special case of the bilinear form in which \\(\\mathbf{x}=\\mathbf{y}\\). In this case we replace y with x so that we create terms with the different combinations of x:\n\\[\nf(x,x) = a_{11}x_1y_1 + a_{21}x_2y_1 + a_{31}x_3y_1 + a_{12}x_1y_2 + a_{22}x_2y_2 + a_{32}x_3y_2\n\\]\nA, simply means a linear function of a set of variables given in a vector x.\nConsider the following transformation of a square matrix A:\n\\[\n\\mathbf{x}^\\intercal\\mathbf{Ax}\n\\]\nwhere A is a \\(k \\times k\\) matrix and x is a \\(k \\times 1\\) vector. This transformations is referred to as the quadratic transformation or the quadratic form of A.",
    "crumbs": [
      "Matrix Transformations",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Quadratic Form of a Matrix</span>"
    ]
  },
  {
    "objectID": "32-quadratic-forms.html#example-quadratic-form",
    "href": "32-quadratic-forms.html#example-quadratic-form",
    "title": "16  Quadratic Form of a Matrix",
    "section": "16.4 Example: Quadratic Form",
    "text": "16.4 Example: Quadratic Form\nConsider the following square matrix A:\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n-3 & 5  \\\\\n4 & -2  \\\\\n\\end{bmatrix}\n\\]\nWe can compute the quadratic form by using the vector\n\\[\n\\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\ x_2\\end{bmatrix}\n\\]\nThen,\n\\[\n\\begin{split}\n\\mathbf{x}^\\intercal\\mathbf{Ax} &= \\begin{bmatrix}\nx_1 & x_2\\end{bmatrix}\\begin{bmatrix}\n-3 & 5  \\\\\n4 & -2  \\\\\n\\end{bmatrix}\\begin{bmatrix}\nx_1 \\\\ x_2\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}\nx_1 & x_2\\end{bmatrix}\\begin{bmatrix}\n-3x_1 + 5x_2  \\\\\n4x_1 -2x_2\n\\end{bmatrix} \\\\[2ex]\n&= x_1(-3x_1 + 5x_2) + x_2(4x_1 -2x_2) \\\\[2ex]\n&= -3x_1^2 + 5x_1x_2 + 4x_1x_2 -2x_2^2 \\\\[2ex]\n&= -3x_1^2 + 9x_1x_2 -2x_2^2\n\\end{split}\n\\]\nBy looking at the exponents in the final expression, you can see why this is called a quadratic form or transformation of A.",
    "crumbs": [
      "Matrix Transformations",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Quadratic Form of a Matrix</span>"
    ]
  },
  {
    "objectID": "32-quadratic-forms.html#positive-definite-matrices",
    "href": "32-quadratic-forms.html#positive-definite-matrices",
    "title": "16  Quadratic Form of a Matrix",
    "section": "16.5 Positive Definite Matrices",
    "text": "16.5 Positive Definite Matrices\nPositive definite matrices come up a lot in statistical applications. For example, nonsingular correlation matrices and covariance matrices are positive definite matrices. A symmetric matrix is said to be positive definite if all of its eigenvalues are positive. An alternative definition is that a symmetric matrix is positive definite if pre-multiplying and post-multiplying it by the same vector always gives a positive number as a result, independently of how we choose the vector. Mathematically, A is positive definite if:\n\\[\n\\mathbf{v}^\\intercal \\mathbf{Av} &gt; 0\n\\] These two definitions are equivalent.",
    "crumbs": [
      "Matrix Transformations",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Quadratic Form of a Matrix</span>"
    ]
  },
  {
    "objectID": "60-solving-systems-of-equations.html",
    "href": "60-solving-systems-of-equations.html",
    "title": "17  Systems of Equations",
    "section": "",
    "text": "17.1 Solving Systems of Equations with Matrix Algebra\nOne thing that matrix algebra is really useful for is solving systems of equations. For example, consider the following system of two equations:\n\\[\n\\begin{split}\n3x + 4y &= 10 \\\\[2ex]\n2x + 3y &= 7\n\\end{split}\n\\]\nThe set of equations is referred to as a system of equations, and “solving” this system of equations is akin to determining the values of x and y that satisfy all equations in the system. In an algebra class, you may have learned to solve these equations by graphing them, using substitution, or elimination. Based on these methods we would find that \\(x=2\\) and \\(y=1\\). Since this system has a solution, we say the system of equations is consistent.\nIf this were the only set of x- and y-values that could simultaneously solve all of the equations, then this consistent system is said to have a unique solution. In other systems of equations, there may be an infinite number of x- and y-values that provide solutions (referred to as a dependent system of equations) or no x- and y-values that offer a solution (an inconsistent system of equations).\nGraphically, a consistent system of equations with a unique solution intersect at a unique point. In our example, the two lines defined by the equations would intersect at the point \\((2,1)\\). If the system of equations were dependent, the two equations would form the same line. For example,\n\\[\n\\begin{split}\nx + y &= 6 \\\\[2ex]\n2x + 2y &= 12\n\\end{split}\n\\]\nIn this example, every \\((x,y)\\) value that lies on the two lines is a potential solution for the system; and there are an infinite number of these solutions. In an inconsistent system of equations, the two lines do not intersect; they are parallel.1 For example, the following system of equations is inconsistent:\n\\[\n\\begin{split}\nx + y &= 5 \\\\[2ex]\nx + y &= 10\n\\end{split}\n\\]\nThese two lines are parallel (do not intersect), so there is not any values of x and y that satisfy both equations. Note that the second equation in the system has been changed disproportionately from the first; namely the left-side of the first equation was multiplied by one to obtain the left-side of the second equation and the right-side of the first equation was multiplied by two. Algebraically, inconsistent systems share this characteristic that one side of the equation has been disproportionately changed from the other side.\nWe can also use matrix algebra to solve systems of equations. To do this, we will re-visit the earlier system of equations:\n\\[\n\\begin{split}\n3x + 4y &= 10 \\\\[2ex]\n2x + 3y &= 7\n\\end{split}\n\\]\nWe first need to write this system of equations using matrices. Because there are two equations, we can initially formulate each side of the equality as a \\(2 \\times 1\\) matrix:\n\\[\n\\begin{bmatrix}\n3x + 4y \\\\\n2x + 3y\n\\end{bmatrix} = \\begin{bmatrix}\n10 \\\\\n7\n\\end{bmatrix}\n\\]\nNow, we can re-write the left-hand side of the equality as the product of two matrices:\n\\[\n\\begin{bmatrix}\n3 & 4 \\\\\n2 & 3\n\\end{bmatrix}\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix} = \\begin{bmatrix}\n10 \\\\\n7\n\\end{bmatrix}\n\\]\nNote that the elements of the “left” (premultiplied) matrix on the left-hand side of the equality are composed of the four coefficients that make up the system of equations. The “right” (postmultiplied) matrix on the left-hand side of the equality is a column matrix composed of the unknowns in the original system of equations. The matrix on the right-hand side of the equality is a column matrix made up of the equality values in the original system of equations.\nNote that the matrix multiplication on the left-hand side of the equality is defined and produces the appropriately dimensioned matrix on the right-hand side of the equality.\n\\[\n\\underset{2\\times 2}{\\begin{bmatrix}\n3 & 4 \\\\\n2 & 3\n\\end{bmatrix}}\\underset{2\\times 1}{\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}} = \\underset{2\\times 1}{\\begin{bmatrix}\n10 \\\\\n7\n\\end{bmatrix}}\n\\]\nIf we use a more general notation,\n\\[\n\\underset{n\\times n}{\\mathbf{A}}~\\underset{n\\times 1}{\\mathbf{X}} = \\underset{n\\times 1}{\\mathbf{C}}\n\\]\nwhere n is the number of equations (and unknowns) in the system, A is a matrix representing the coefficients associated with each of the unknowns in the system equations, and C is the matrix representing the scalar equality values.\nTo solve this general equation, we premultiply both sides of the equation by \\(\\mathbf{A}^{-1}\\).\n\\[\n\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{X} = \\mathbf{A}^{-1}\\mathbf{C}\n\\]\nThis will result in\n\\[\n\\begin{split}\n(\\mathbf{A}^{-1}\\mathbf{A})\\mathbf{X} &= \\mathbf{A}^{-1}\\mathbf{C} \\\\[2ex]\n\\mathbf{I}\\mathbf{X} &= \\mathbf{A}^{-1}\\mathbf{C} \\\\[2ex]\n\\mathbf{X} &= \\mathbf{A}^{-1}\\mathbf{C}\n\\end{split}\n\\]\nThus, we can solve for the elements in X (x and y) by premultiplying C by the inverse of A. In our example,\n\\[\n\\begin{split}\n\\overset{\\mathbf{A}}{\\begin{bmatrix}\n3 & 4 \\\\\n2 & 3\n\\end{bmatrix}}\\overset{\\mathbf{X}}{\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}} &= \\overset{\\mathbf{C}}{\\begin{bmatrix}\n10 \\\\\n7\n\\end{bmatrix}} \\\\[2ex]\n\\overset{\\mathbf{A}^{-1}}{\\begin{bmatrix}\n3 & -4 \\\\\n-2 & 3\n\\end{bmatrix}}\\overset{\\mathbf{A}}{\\begin{bmatrix}\n3 & 4 \\\\\n2 & 3\n\\end{bmatrix}}\\overset{\\mathbf{X}}{\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}} &= \\overset{\\mathbf{A}^{-1}}{\\begin{bmatrix}\n3 & -4 \\\\\n-2 & 3\n\\end{bmatrix}}\\overset{\\mathbf{C}}{\\begin{bmatrix}\n10 \\\\\n7\n\\end{bmatrix}} \\\\[2ex]\n\\overset{\\mathbf{I}}{\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}}\\overset{\\mathbf{X}}{\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}} &= \\overset{\\mathbf{A}^{-1}\\mathbf{C}}{\\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix}} \\\\[2ex]\n\\overset{\\mathbf{X}}{\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}} &= \\overset{\\mathbf{A}^{-1}\\mathbf{C}}{\\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix}} \\\\[2ex]\n\\end{split}\n\\]\nBecause the two matrices here, \\(\\mathbf{X}\\) and \\(\\mathbf{A}^{-1}\\mathbf{C}\\), are equal, all of the elements in the same position are equal. Thus \\(x=2\\) and \\(y=1\\).",
    "crumbs": [
      "Systems of Equations",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Systems of Equations</span>"
    ]
  },
  {
    "objectID": "60-solving-systems-of-equations.html#solving-systems-of-equations-with-matrix-algebra",
    "href": "60-solving-systems-of-equations.html#solving-systems-of-equations-with-matrix-algebra",
    "title": "17  Systems of Equations",
    "section": "",
    "text": "17.1.1 Solving this System of Equations Using R\nWe can use R to carry out this matrix algebra.\n\n# Create A\nA = matrix(\n  data = c(3, 2, 4, 3),\n  nrow = 2\n)\n\n\n# Create C\nC = matrix(\n  data = c(10, 7),\n  nrow = 2\n)\n\n\n# Solve for X\nsolve(A) %*% C\n\n     [,1]\n[1,]    2\n[2,]    1\n\n\nWhat happens when we try to solve for a consistent system of equations that have an infinite number of solutions? Using our example from above:\n\n# Create A\nA = matrix(\n  data = c(1, 2, 1, 2),\n  nrow = 2\n)\n\n\n# Create C\nC = matrix(\n  data = c(6, 12),\n  nrow = 2\n)\n\n\n# Solve for X\nsolve(A) %*% C\n\nError in solve.default(A): Lapack routine dgesv: system is exactly singular: U[2,2] = 0\n\n\nWe cannot solve this system of equations because the inverse of A is singular (the determinant of A is zero). What happens when we try to solve for a dependent system of equations? Using our example from above:\n\n# Create A\nA = matrix(\n  data = c(1, 1, 1, 1),\n  nrow = 2\n)\n\n\n# Create C\nC = matrix(\n  data = c(5, 10),\n  nrow = 2\n)\n\n\n# Solve for X\nsolve(A) %*% C\n\nError in solve.default(A): Lapack routine dgesv: system is exactly singular: U[2,2] = 0\n\n\nWe also cannot solve this system of equations because the inverse of A is singular (the determinant of A is zero).",
    "crumbs": [
      "Systems of Equations",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Systems of Equations</span>"
    ]
  },
  {
    "objectID": "60-solving-systems-of-equations.html#linear-dependence-and-independence",
    "href": "60-solving-systems-of-equations.html#linear-dependence-and-independence",
    "title": "17  Systems of Equations",
    "section": "17.2 Linear Dependence and Independence",
    "text": "17.2 Linear Dependence and Independence\nWe recognize that when a redundant equation is present in a system of equations, the matrix of coefficients (A) is singular, the determinant is zero. This is one definition of a linearly dependent system of equations. When none of the equations in a system are redundant with any other equation within the system, the system of equations is linearly independent.\nConsider a vector z where \\(\\mathbf{z} = a\\mathbf{x} + b\\mathbf{y}\\). That is, z is a linear function of the vectors x and y. (We could also say z is linearly dependent on x and y.) When we have used vector addition or vector subtraction, we were actually creating linear combinations of vectors where the scalars were all equal to \\(+1\\) or \\(–1\\). For example, \\(\\mathbf{z} = \\mathbf{x} + \\mathbf{y}\\) is mathematically equivalent to \\(\\mathbf{z} = 1\\mathbf{x} + 1\\mathbf{y}\\). In this case, the coefficients for x and y are both \\(+1\\); \\(a=1\\) and \\(b=1\\).\nFormally, a set of vectors, \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots \\mathbf{a}_n\\), is linearly dependent if there exists a set of scalars (not all zero) where:\n\\[\nc_1\\mathbf{a}_1 + c_2 \\mathbf{a}_2 + \\ldots + c_n\\mathbf{a}_n = 0\n\\] If no set of scalars exist to satisfy this condition, the set of vectors is linearly independent. For the above condition to be true, at least one of the vectors must be redundant with another, it can be expressed as a linear combination of the others. Linear dependence implies redundancy among at least two of the vectors. For example consider the following three vectors:\n\\[\n\\mathbf{x} = \\begin{bmatrix}4\\\\6\\end{bmatrix} \\qquad \\mathbf{y}= \\begin{bmatrix}2\\\\1\\end{bmatrix} \\qquad \\mathbf{z}= \\begin{bmatrix}6\\\\11\\end{bmatrix}\n\\]\nThese three vectors can be written as:\n\\[\n\\begin{split}\n2\\mathbf{x} - 1 \\mathbf{y} -1 \\mathbf{z} &= 0 \\\\[2ex]\n2\\begin{bmatrix}4\\\\6\\end{bmatrix} - 1 \\begin{bmatrix}2\\\\1\\end{bmatrix} - 1 \\begin{bmatrix}6\\\\11\\end{bmatrix} &= 0\n\\end{split}\n\\]\nSince we have a set of scalars \\(\\{2,-1,-1\\}\\) that satisfy this equation where none of the scalars are zero, this system of equations is linearly dependent.",
    "crumbs": [
      "Systems of Equations",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Systems of Equations</span>"
    ]
  },
  {
    "objectID": "60-solving-systems-of-equations.html#singularity",
    "href": "60-solving-systems-of-equations.html#singularity",
    "title": "17  Systems of Equations",
    "section": "17.3 Singularity",
    "text": "17.3 Singularity\nRecall that in a singular matrix, the determinant if the matrix is zero. A matrix that is singular implies that one or more of the rows (or columns) is linearly dependent; there exists a set of scalars that satisfies our definition. Earlier we saw that:\n\\[\n\\mathbf{A} = \\begin{bmatrix}1 & 1 \\\\ 2 & 2\\end{bmatrix}\n\\]\nwas singular. Namely,\n\\[\n\\begin{split}\n\\vert\\mathbf{A}\\vert &= 1(2) - 2(1) \\\\[2ex]\n&= 0\n\\end{split}\n\\] We can also find scalars \\(c_1\\) and \\(c_2\\) such that the row vectors (or column vectors) of A can be written as \\(c_1\\mathbf{a}_1 + c_2 \\mathbf{a}_2 = 0\\). Using the two row vectors:\n\\[\n\\begin{split}\n2\\begin{bmatrix}1\\\\1\\end{bmatrix} - 1 \\begin{bmatrix}2\\\\2\\end{bmatrix} &= 0 \\\\[2ex]\n2\\mathbf{a}_1 - 1 \\mathbf{a}_2  &= 0\n\\end{split}\n\\]\nIn general, when A is singular the rows are linearly dependent. The columns are similarly linearly dependent when A is singular.\n\nFYI\nThere is a theorem in linear algebra that suggests that any n-element vector can be written as a linear combination of n linearly independent n-element vectors. Finding such vectors can be difficult, but we rely on this theorem in statistics.",
    "crumbs": [
      "Systems of Equations",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Systems of Equations</span>"
    ]
  },
  {
    "objectID": "60-solving-systems-of-equations.html#rank-of-a-matrix",
    "href": "60-solving-systems-of-equations.html#rank-of-a-matrix",
    "title": "17  Systems of Equations",
    "section": "17.4 Rank of a Matrix",
    "text": "17.4 Rank of a Matrix\nThe column rank of a matrix describes the dimensionality of the column space for that matrix, and the row rank describes the dimensionality of the row space for that matrix. It turns out that the column and row ranks of a matrix are the same, so we just refer to the dimensionality, or rank of a matrix. Because dimensionality is related to the independence of the rows/columns, the rank of a matrix is also the number of rows or columns that are linearly independent:\n\\[\n\\mathrm{Rank}(\\mathbf{A}) = \\begin{cases}\\mathrm{number~of~linearly~independent~rows~of~}\\mathbf{A} \\\\ \\mathrm{number~of~linearly~independent~columns~ of~}\\mathbf{A}\\end{cases}\n\\]\nFor an \\(n \\times p\\) dimensional matrix A, the maximum possible rank is the smaller value of n and p. That is,\n\\[\n0 \\leq \\mathrm{Rank}(\\underset{n\\times p}{\\mathbf{A}}) \\leq \\min(n,p)\n\\]\nThe rank of a matrix would be zero only if the matrix were a null matrix. For all other matrices, the minimum possible rank would be one.\nWhen all of the row and column vectors in a matrix are linearly independent the matrix is said to be of full rank. That is, when the rank of A is the maximum possible rank, it is of full rank. When the rank of A is smaller than both n and p, the matrix is said to be of deficient rank. This implies that at least one of the row (or column) vectors is linearly dependent. It turns out only a square matrix can be of full rank—rectangular matrices will always have a rank less than either n or p.\nFor example, consider the following matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}3 & 5 \\\\ 1 & 2\\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}1 & 3 & 5 \\\\ 1 & 2 & 3 \\\\ 2 & 5 & 8\\end{bmatrix}\n\\]\nThe rank of A is 2, since the two ro vectors (or the two column vectors) are linearly independent. Since 2 is the maximum possible rank of a \\(2 \\times 2\\) matrix, we consider A to be of full rank. Matrix B, on the other hand, is of deficient rank. This is because the rank of B is 2, which is less than the maximum possible rank of 3 for a \\(3\\times 3\\) matrix. This means that two of the three rows (or columns) of B are linearly independent. It turns out the third row of B is a linear combination of the first and second rows:\n\\[\n\\overset{\\mathrm{Row~3}}{\\begin{bmatrix}2 & 5 & 8\\end{bmatrix}} = \\overset{\\mathrm{Row~1}}{\\begin{bmatrix}1 & 3 & 5\\end{bmatrix}} + \\overset{\\mathrm{Row~2}}{\\begin{bmatrix}1 & 2 & 3\\end{bmatrix}}\n\\]\nWe can compute the rank of a matrix by using the rankMatrix() function from the Matrix package.\n\n# Create A\nA = matrix(\n  data = c(3, 5, 1, 2),\n  byrow = TRUE,\n  ncol = 2\n)\n\n# Compute rank of A\nMatrix::rankMatrix(A)\n\n[1] 2\nattr(,\"method\")\n[1] \"tolNorm2\"\nattr(,\"useGrad\")\n[1] FALSE\nattr(,\"tol\")\n[1] 4.440892e-16\n\n# Create B\nB = matrix(\n  data = c(1, 3, 5, 1, 2, 3, 2, 5, 8),\n  byrow = TRUE,\n  ncol = 3\n)\n\n# Compute rank of B\nMatrix::rankMatrix(B)\n\n[1] 2\nattr(,\"method\")\n[1] \"tolNorm2\"\nattr(,\"useGrad\")\n[1] FALSE\nattr(,\"tol\")\n[1] 6.661338e-16\n\n\nThe determinant of a matrix also gives us insight into whether a matrix is of full rank. Singular matrices (determinant = 0) are of deficient rank, while non-singular matrices are of full rank.\n\n# Compute determinant\ndet(A)\n\n[1] 1\n\n# Compute determinant\ndet(B)\n\n[1] 0\n\n\nAll of this implies that if A is non-singular (\\(\\lvert \\mathbf{A} \\rvert \\neq 0\\)), then:\n\nA has an inverse;\n\\(\\mathrm{rank}(\\mathbf{A}) = n\\), or A is full rank;\nAll rows of A are linearly independent; and\nAll columns of A are linearly independent.",
    "crumbs": [
      "Systems of Equations",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Systems of Equations</span>"
    ]
  },
  {
    "objectID": "60-solving-systems-of-equations.html#rank-and-the-solution-of-systems-of-equations",
    "href": "60-solving-systems-of-equations.html#rank-and-the-solution-of-systems-of-equations",
    "title": "17  Systems of Equations",
    "section": "17.5 Rank and the Solution of Systems of Equations",
    "text": "17.5 Rank and the Solution of Systems of Equations\nRemember, our goal was to use matrix algebra to solve a system of equations. In order to do this we need to evaluate the independence of the system of equations. Consider the following system of equations:\n\\[\n\\begin{split}\nx_1 + x_2 + x_3 &= 5 \\\\[2ex]\n2x_1 – 1x_2 + 6x_3 &= 12 \\\\[2ex]\nx_1 + 3x_2 + 5x_3 &= 17\n\\end{split}\n\\] The coefficient matrix (\\(\\mathbf{A}\\)) and the augmented coefficient matrix (\\(\\mathbf{A}\\vert \\mathbf{c}\\)) are:\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 1 & 1 \\\\\n2 & – 1 & 6\\\\\n1 & 3 & 5\n\\end{bmatrix} \\qquad \\mathbf{A}\\vert \\mathbf{c} = \\begin{bmatrix}\n1 & 1 & 1 & 5\\\\\n2 & – 1 & 6 & 12\\\\\n1 & 3 & 5 & 17\n\\end{bmatrix}\n\\]\nNote that the augmented coefficient matrix is simply the coefficient matrix with an appended column that include the solutions from the system of equations (the c vector from \\(\\mathbf{Ax}=\\mathbf{c}\\)).\nA theorem in linear algebra states that a system of equations is independent (consistent) if the rank of the coefficient matrix is the same as the rank of the augmented coefficient matrix, \\(\\mathrm{rank}(\\mathbf{A}) = \\mathrm{rank}(\\mathbf{A}\\vert \\mathbf{c})\\). Examining the rank of A, we find:\nComputing the rank of A:\n\n# Create A\nA = matrix(\n  data = c(1, 1, 1, 2, -1, 6, 1, 3, 5),\n  byrow = TRUE,\n  ncol = 3\n)\n\n# Create A|y\nA_y = matrix(\n  data = c(1, 1, 1, 5, 2, -1, 6, 12, 1, 3, 5, 17),\n  byrow = TRUE,\n  ncol = 4\n)\n\n# Compute rank of A\nMatrix::rankMatrix(A)\n\n[1] 3\nattr(,\"method\")\n[1] \"tolNorm2\"\nattr(,\"useGrad\")\n[1] FALSE\nattr(,\"tol\")\n[1] 6.661338e-16\n\n# Compute rank of A|y\nMatrix::rankMatrix(A_y)\n\n[1] 3\nattr(,\"method\")\n[1] \"tolNorm2\"\nattr(,\"useGrad\")\n[1] FALSE\nattr(,\"tol\")\n[1] 8.881784e-16\n\n\nThe rank of A is 3, which means A is of full rank. This also implies that A is non-singular (\\(\\lvert \\mathbf{A}\\rvert \\neq 0\\)), and that A has an inverse. Furthermore, since the rank of \\(\\mathbf{A}\\vert \\mathbf{c}\\) is also 3, the system of equations is consistent (linearly independent). This means that we can solve the equations using:\n\\[\n\\begin{split}\n\\mathbf{x} &= \\mathbf{A}^{-1}\\mathbf{y}\\\\[2ex]\n\\begin{bmatrix}\nx_1 \\\\\nx_2\\\\\nx_3\n\\end{bmatrix} &= \\begin{bmatrix}\n1 & 1 & 1 \\\\\n2 & – 1 & 6\\\\\n1 & 3 & 5\n\\end{bmatrix}^{-1} \\begin{bmatrix}\n5 \\\\\n12\\\\\n17\n\\end{bmatrix}\n\\end{split}\n\\]\nComputing this, we find \\(x_1 = 1\\), \\(x_2=2\\), and \\(x_3=2\\).\n\n# Create y\ny = matrix(\n  data = c(5, 12, 17),\n  ncol = 1\n)\n\n# Solve system of equations\nsolve(A) %*% y\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    2\n\n\nAnother theorem of linear algebra provides a way to evaluate whether a consistent set of equations has an infinite number of solutions. A consistent system of n equations in n unknowns has a unique solution if the rank of the coefficient matrix is equal to its order, that is \\(\\mathrm{rank}(\\mathbf{A}) = n\\).\nIn the example, there are three equations and three unknowns and the rank of the coefficient matrix, A, is 3. The solution we have computed is unique. Another way to state this is: A consistent system of equations where A is of order n has a unique solution if and only if \\(\\mathbf{A}^{-1}\\) exists. This is important in applications where we have the same number of unknowns as equations.\n\nMATH NOTE\nWhen the number of equations equals the number of unknowns, and \\(\\lvert \\mathbf{A} \\rvert \\neq 0\\), there is a unique solution. That is, when A is full rank, a unique solution exists for \\(\\mathbf{Ax} = \\mathbf{y}\\). This is true because:\n\nIf \\(\\lvert \\mathbf{A} \\rvert \\neq 0\\), then \\(\\mathrm{rank}(\\mathbf{A}) = n\\).\nIf \\(\\mathrm{rank}(\\mathbf{A} \\vert \\mathbf{c})  = \\mathrm{rank}(\\mathbf{A})\\), the equations are consistent.\nIf the equations are consistent and \\(\\mathrm{rank}(\\mathbf{A}) = n\\), there is a unique solution.\n\n\nFigure Figure 17.1 shows a flowchart that you can use to determine whether a system of equations is consistent or not, and if so, whether there is a unique solution or whether it has an infinite number of solutions.\n\n\n\n\n\nFigure 17.1: Flowchart to determine whether a system of equations is consistent or not, and how many solutions exist.",
    "crumbs": [
      "Systems of Equations",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Systems of Equations</span>"
    ]
  },
  {
    "objectID": "60-solving-systems-of-equations.html#exercises",
    "href": "60-solving-systems-of-equations.html#exercises",
    "title": "17  Systems of Equations",
    "section": "Exercises",
    "text": "Exercises\nConsider the following matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}1 & 0 & 0 \\\\0 & 1 & 0 \\\\0 & 0 & 1 \\end{bmatrix} \\qquad \\mathbf{B} = \\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\\\7 & 8 & 9 \\end{bmatrix} \\qquad \\mathbf{C} = \\begin{bmatrix}2 & 3 & 8 \\\\15 & 5 & 9\\\\6 & 9 & 24 \\end{bmatrix}\n\\]\n\nDetermine which of the matrices (A, B, and C) have linearly independent rows and which have linearly dependent rows.\n\n\nShow/Hide Solution\n\n\n\nAll rows in A are linearly independent, since it has a rank of 3.\nThe rank of B is 2, which implies that there is a linearly dependent row. There are a number of other ways to represent this dependency. For example, \\(\\mathrm{Row~3} = 2(\\mathrm{Row~2}) - 1(\\mathrm{Row~1})\\).\nThe rank of C is also 2, which implies that there is a linearly dependent row. There are a number of other ways to represent this dependency. For example, \\(\\mathrm{Row~3} = 3(\\mathrm{Row~2}) + 0(\\mathrm{Row~1})\\).\n\n\nConsider the following system of equations:\n\\[\n\\begin{split}\n2(x_1) + x_2 &= 6 \\\\[2ex]\nx_1 + 3(x_2) &= 8\n\\end{split}\n\\] 2. Write out the coefficient matrix (A) and the augmented coefficient matrix (\\(\\mathbf{A} \\vert \\mathbf{c}\\)).\n\nShow/Hide Solution\n\n\n\\[\n\\mathbf{A} = \\begin{bmatrix}2 & 1 \\\\ 1 & 3 \\end{bmatrix} \\qquad \\mathbf{A} \\vert \\mathbf{c} = \\begin{bmatrix}2 & 1 & 6 \\\\ 1 & 3 & 8 \\end{bmatrix}\n\\]\n\n\nUse the ranks of \\(\\mathbf{A}\\) and \\(\\mathbf{A} \\vert \\mathbf{c}\\) to determine whether the system of equations is consistent.\n\n\nShow/Hide Solution\n\n\nThe ranks of the two matrices are:\n\n\\(\\mathrm{Rank}(\\mathbf{A}) = 2\\)\n\\(\\mathrm{Rank}(\\mathbf{A} \\vert \\mathbf{c}) = 2\\)\n\nSince the rank of the coefficient matrix and the augmented coefficient matrix are equal, the system of equations is consistent.\n\n\nBased on the rank of the coefficient matrix, will there be a unique solution to the system of equations?\n\n\nShow/Hide Solution\n\n\nSince A is of full rank, the solution will be unique.\n\n\nUse R to solve the system of equations.\n\n\nShow/Hide Solution\n\n\n\n# Create A\nA = matrix(\n  data = c(2, 1, 1, 3), \n  ncol = 2\n  )\n\n# Create y\ny = matrix(\n  data = c(6, 8),\n  ncol = 1\n)\n\n# Solve system of equations\nsolve(A) %*% y\n\n     [,1]\n[1,]    2\n[2,]    2\n\n\n\nConsider the following system of equations:\n\\[\n\\begin{split}\n25(x_1) + 5(x_2) + x_3 &= 106.8 \\\\[2ex]\n64(x_1) + 8(x_2) + x_3 &= 177.2 \\\\[2ex]\n89(x_1) + 13(x_2) + 2(x_3) &= 284.0 \\\\[2ex]\n\\end{split}\n\\] 6. Use R to compute the determinant of the coefficient matrix. Based on the determinant, will there be a unique solution to the system of equations?\n\nShow/Hide Solution\n\n\n\n# Create A\nA = matrix(\n  data = c(25, 5, 1, 64, 8, 1, 89, 13, 2), \n  byrow = TRUE,\n  ncol = 3\n  )\n\n# Compute determinant\ndet(A)\n\n[1] 0\n\n\nSince A is singular, there is not a unique solution to the system of equations. (The rank of the coefficient matrix and the augmented coefficient matrix are both 2, so the system is consistent. And, since there is no unique solution, this means there are an infinite number of solutions.)",
    "crumbs": [
      "Systems of Equations",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Systems of Equations</span>"
    ]
  },
  {
    "objectID": "60-solving-systems-of-equations.html#footnotes",
    "href": "60-solving-systems-of-equations.html#footnotes",
    "title": "17  Systems of Equations",
    "section": "",
    "text": "Another way to think about dependent systems of equations is that they are redundant.↩︎",
    "crumbs": [
      "Systems of Equations",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Systems of Equations</span>"
    ]
  },
  {
    "objectID": "61-statistical-application-regression-estimates.html",
    "href": "61-statistical-application-regression-estimates.html",
    "title": "18  Statistical Appplication: Estimating Regression Coefficients",
    "section": "",
    "text": "18.1 Regression Model\nIn this chapter, you will learn about how matrix algebra is used to compute regression coefficients.\nRecall the model equation we use in linear regression:\n\\[\nY_i = \\beta_0 + \\beta_1(X_{1_i}) + \\beta_2(X_{2_i}) + \\ldots + \\beta_k(X_{k_i}) + \\epsilon_i\n\\]\nwhere the response variable (Y) is represented as a linear function of the set of predictors \\(X_1,X_2,\\ldots,X_k\\)) and a residual (\\(\\epsilon\\)). Equation terms with an i subscript vary across subjects. Terms without an i subscript are the same (fixed) across subjects.\nRecall that this notation is a mathematical expression of the n subject-specific equations:\n\\[\n\\begin{split}\nY_1 &= \\beta_0 + \\beta_1(X_{1_1}) + \\beta_2(X_{2_1}) + \\ldots + \\beta_k(X_{k_1}) + \\epsilon_1 \\\\\nY_2 &= \\beta_0 + \\beta_1(X_{1_2}) + \\beta_2(X_{2_2}) + \\ldots + \\beta_k(X_{k_2}) + \\epsilon_2 \\\\\nY_3 &= \\beta_0 + \\beta_1(X_{1_3}) + \\beta_2(X_{2_3}) + \\ldots + \\beta_k(X_{k_3}) + \\epsilon_3 \\\\\n\\vdots &~ ~~~~~\\vdots ~~~~~~~~~~\\vdots~~~~~~~~~~~~~~~~~~~\\vdots~~~~~~~~~~~~~~\\vdots~~~~~~~~~~~~~\\vdots~~~~~~~~~~~\\vdots  \\\\\nY_n &= \\beta_0 + \\beta_1(X_{1_n}) + \\beta_2(X_{2_n}) + \\ldots + \\beta_k(X_{k_n}) + \\epsilon_n\n\\end{split}\n\\]\nThese can be arranged into a set of vectors and matrices,\n\\[\n\\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ \\vdots \\\\ Y_n\\end{bmatrix} = \\begin{bmatrix}\\beta_0(1) + \\beta_1(X_{1_1}) + \\beta_2(X_{2_1}) + \\ldots + \\beta_k(X_{k_1}) \\\\ \\beta_0(1) + \\beta_1(X_{1_2}) + \\beta_2(X_{2_2}) + \\ldots + \\beta_k(X_{k_2}) \\\\ \\beta_0(1) + \\beta_1(X_{1_3}) + \\beta_2(X_{2_3}) + \\ldots + \\beta_k(X_{k_3}) \\\\ \\vdots \\\\ \\beta_0(1) + \\beta_1(X_{1_n}) + \\beta_2(X_{2_n}) + \\ldots + \\beta_k(X_{k_n})\\end{bmatrix} + \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n\\end{bmatrix}\n\\]\nWe can re-write this as,\n\\[\n\\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ \\vdots \\\\ Y_n\\end{bmatrix} = \\begin{bmatrix}1 & X_{1_1} & X_{2_1} & \\ldots & X_{k_1} \\\\ 1 & X_{1_2} & X_{2_2} & \\ldots & X_{k_2} \\\\ 1 & X_{1_3} & X_{2_3} & \\ldots & X_{k_3} \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & X_{1_n} & X_{2_n} & \\ldots & X_{k_n}\\end{bmatrix} \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\\\beta_2 \\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}+ \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n\\end{bmatrix}\n\\]\nNaming these vectors and matrices, we can use matrix notation to compactly write the regression model as,\n\\[\n\\underset{n \\times 1}{\\mathbf{y}} = \\underset{n \\times k}{\\mathbf{X}}~\\underset{k \\times 1}{\\mathbf{b}} + \\underset{n \\times 1}{\\mathbf{e}}\n\\]\nIn the equation above,",
    "crumbs": [
      "Systems of Equations",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Appplication: Estimating Regression Coefficients</span>"
    ]
  },
  {
    "objectID": "61-statistical-application-regression-estimates.html#regression-model",
    "href": "61-statistical-application-regression-estimates.html#regression-model",
    "title": "18  Statistical Appplication: Estimating Regression Coefficients",
    "section": "",
    "text": "y is a vector of the outcome values,\nX is a matrix referred to the design matrix (a.k.a., the model matrix, the data matrix),\nb is a vector of regression coefficients, and\ne is a vector of the residuals.",
    "crumbs": [
      "Systems of Equations",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Appplication: Estimating Regression Coefficients</span>"
    ]
  },
  {
    "objectID": "61-statistical-application-regression-estimates.html#estimating-the-regression-coefficients",
    "href": "61-statistical-application-regression-estimates.html#estimating-the-regression-coefficients",
    "title": "18  Statistical Appplication: Estimating Regression Coefficients",
    "section": "18.2 Estimating the Regression Coefficients",
    "text": "18.2 Estimating the Regression Coefficients\nWhen we fit a regression model to a dataset, one of the goals is to estimate the regression coefficients, \\(\\beta_0, \\beta_1, \\beta_2,\\ldots,\\beta_k\\). Based on the regression equation,\n\\[\n\\mathbf{Y} = \\mathbf{Xb} + \\boldsymbol{\\epsilon}\n\\]\nour goal is to solve for terms in the b vector. (Note that here (and moving forward) the dimensions of each matrix/vector have been omitted when we write the regression model.) How this is done, depends on the estimation method used. For the rest of this chapter, we will assume that Ordinary Least Squares (OLS) regression is being used to estimate the coefficients.\nIn OLS estimation, we want to find the coefficient values that produce the smallest sum of squared residuals. To do this, we first re-write the regression equation to isolate the error vector:\n\\[\n\\mathbf{e} = \\mathbf{y} - \\mathbf{Xb}\n\\]\nThe sum of squared residual can be expressed in matrix notation as \\(\\mathbf{e}^{\\intercal}\\mathbf{e}\\). This implies:\n\\[\n\\mathbf{e}^{\\intercal}\\mathbf{e} = (\\mathbf{y} - \\mathbf{Xb})^{\\intercal} (\\mathbf{y} - \\mathbf{Xb})\n\\]\nUsing the rules of transposes and expanding the right-hand side, we get,\n\\[\n\\mathbf{y}^{\\intercal}\\mathbf{y} - \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y} - \\mathbf{y}^{\\intercal}\\mathbf{X}\\mathbf{b} + \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{X}\\mathbf{b}\n\\]\nEach of these terms is a \\(1\\times 1\\) matrix1, which implies that each term is equal to its transpose. We will re-write the third term \\(\\mathbf{y}^{\\intercal}\\mathbf{X}\\mathbf{b}\\) as its transpose \\(\\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y}\\). Re-writing, we get:\n\\[\n\\mathbf{y}^{\\intercal}\\mathbf{y} - \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y} - \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y} + \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{X}\\mathbf{b}\n\\]\nCombining the two middle terms,\n\\[\n\\mathbf{y}^{\\intercal}\\mathbf{y} - 2\\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y} + \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{X}\\mathbf{b}\n\\]\nTo find the values for the elements in b that minimize the equation, we differentiate this expression with respect to b.\n\\[\n\\frac{\\delta}{\\delta\\mathbf{b}}~\\mathbf{y}^{\\intercal}\\mathbf{y} - 2\\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y} + \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{X}\\mathbf{b}\n\\]\n\nFYI\nAlthough calculus, especially calculus on matrices, is beyond the scope of this book, Fox (2009) gives the interested reader some mathematical background on optimization (i.e., minimizing). For now you just need to understand we can optimize a function by computing its derivative, setting the derivative equal to 0, and solving for any remaining unknowns.\n\nDifferentiating this we get\n\\[\n-2\\mathbf{X}^{\\intercal}\\mathbf{y} + 2\\mathbf{X}^{\\intercal}\\mathbf{Xb}\n\\]\nWe set this equal to zero and solve for b.\n\\[\n\\begin{split}\n-2\\mathbf{X}^{\\intercal}\\mathbf{y} + 2\\mathbf{X}^{\\intercal}\\mathbf{Xb} &= 0 \\\\[2ex]\n2\\mathbf{X}^{\\intercal}\\mathbf{Xb} &= 2\\mathbf{X}^{\\intercal}\\mathbf{y} \\\\[2ex]\n\\mathbf{X}^{\\intercal}\\mathbf{Xb} &= \\mathbf{X}^{\\intercal}\\mathbf{y}\n\\end{split}\n\\]\n\nVOCABULARY\nThis representation of the OLS equations:\n\\[\n\\mathbf{X}^{\\intercal}\\mathbf{Xb} = \\mathbf{X}^{\\intercal}\\mathbf{y}\n\\]\nis referred to as the Normal Equations. It is the basis for many methods of soving for b.\n\n\nTo isolate b we pre-multiply both sides of the equation by \\((\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\).\n\\[\n\\begin{split}\n(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}(\\mathbf{X}^{\\prime}\\mathbf{X})\\mathbf{b} &= (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}(\\mathbf{X}^{\\intercal}\\mathbf{y}) \\\\[2ex]\n\\mathbf{I}\\mathbf{b} &= (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y} \\\\[2ex]\n\\mathbf{b} &= (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y}\n\\end{split}\n\\]\nThe vector of regression coefficients can be obtain from:\n\\[\n\\mathbf{b} = (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y}\n\\]\nThis implies that the vector of regression coefficients can be obtained directly through manipulation of the design matrix and the vector of outcomes. In other words, the OLS coefficients is a direct function of the data.\n\n\n18.2.1 Example Using Data\nWe will use the following toy data set to illustrate how regression is carried out via matrix algebra.\n\n\n\n\nTable 18.1: Example set of education data.\n\n\n\n\n\n\nID\nSAT\nGPA\nSelf-Esteem\nIQ\n\n\n\n\n1\n560\n3.0\n11\n112\n\n\n2\n780\n3.9\n10\n143\n\n\n3\n620\n2.9\n19\n124\n\n\n4\n600\n2.7\n7\n129\n\n\n5\n720\n3.7\n18\n130\n\n\n6\n380\n2.4\n13\n82\n\n\n\n\n\n\n\n\nSay we wanted to fit a regression model using SAT and Self-Esteem to predict variation in GPA. We can estimate the regression coefficients by creating a design matrix (X), the vector of outcomes (y), and then using matrix algebra.\n\n# Create vector of outcomes\ny = c(3.0, 3.9, 2.9, 2.7, 3.7, 2.4)\n\n# Create design matrix\nX = matrix(\n  data = c(\n    rep(1, 6), \n    560, 780, 620, 600, 720, 380, \n    11, 10, 19, 7, 18, 13\n    ),\n  ncol = 3\n)\n\n# View X\nX\n\n     [,1] [,2] [,3]\n[1,]    1  560   11\n[2,]    1  780   10\n[3,]    1  620   19\n[4,]    1  600    7\n[5,]    1  720   18\n[6,]    1  380   13\n\n# Estimate coefficients\nb = solve(t(X) %*% X) %*% t(X) %*% y\nb\n\n            [,1]\n[1,] 0.659213517\n[2,] 0.003805501\n[3,] 0.009186998\n\n\nLet’s compare this to the estimates given in the lm() function.\n\n# Create data frame\nd = data.frame(\n  GPA = c(3.0, 3.9, 2.9, 2.7, 3.7, 2.4),\n  SAT = c(560, 780, 620, 600, 720, 380),\n  Self = c(11, 10, 19, 7, 18, 13)\n)\n\n# Fit model\nlm.1 = lm(GPA ~ 1 + SAT + Self, data = d)\n\n# View coefficients\ncoef(lm.1)\n\n(Intercept)         SAT        Self \n0.659213517 0.003805501 0.009186998 \n\n\nThe matrix algebra and lm() function produce identical coefficient estimates.",
    "crumbs": [
      "Systems of Equations",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Appplication: Estimating Regression Coefficients</span>"
    ]
  },
  {
    "objectID": "61-statistical-application-regression-estimates.html#the-mathbfxintercalmathbfx-matrix",
    "href": "61-statistical-application-regression-estimates.html#the-mathbfxintercalmathbfx-matrix",
    "title": "18  Statistical Appplication: Estimating Regression Coefficients",
    "section": "18.3 The \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) Matrix",
    "text": "18.3 The \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) Matrix\nIn computing estimates, the matrix that must be inverted is \\(\\mathbf{X}^\\intercal\\mathbf{X}\\). In simple regression (with a single predictor) this is:\n\\[\n\\begin{split}\n\\mathbf{X}^\\intercal\\mathbf{X} &= \\begin{bmatrix}1 & 1 & 1 & \\ldots & 1\\\\ X_1 & X_2 & X_3 & \\ldots & X_n\\end{bmatrix} \\begin{bmatrix}1 & X_1\\\\ 1 & X_2\\\\ 1 & X_3\\\\ \\vdots & \\vdots\\\\ 1& X_n\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}n & \\sum X_i\\\\ \\sum X_i & \\sum X_i^2\\end{bmatrix}\n\\end{split}\n\\] The determninant can then be found as:\n\\[\n\\begin{split}\n\\begin{vmatrix}n & \\sum X_i\\\\ \\sum X_i & \\sum X_i^2\\end{vmatrix} &= n\\sum X_i^2 - \\bigg(\\sum X_i\\bigg)^2 \\\\[2ex]\n&= n\\sum X_i^2 - (n\\bar{X})^2 \\\\[2ex]\n&= n \\bigg(\\sum X_i^2 - n\\bar{X}^2\\bigg) \\\\[2ex]\n&= n \\sum(X_i - \\bar{X})^2\n\\end{split}\n\\]\nThis will be a positive value as long as there is variation in X, which implies that the inverse of \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) should exist. Note that when there is very little variation in X, it may be that the inverse is computationally singular.\nWhen the design matrix expands to include more than one predictor, this is also the case. The inverse of the \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) matrix can be computed as long as there is variation in the predictors and no one predictor is a linear combination of the other columns in the design matrix.\n\n\n\n\n\nFox, J. (2009). A mathematical primer for social statistics. Sage.",
    "crumbs": [
      "Systems of Equations",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Appplication: Estimating Regression Coefficients</span>"
    ]
  },
  {
    "objectID": "61-statistical-application-regression-estimates.html#footnotes",
    "href": "61-statistical-application-regression-estimates.html#footnotes",
    "title": "18  Statistical Appplication: Estimating Regression Coefficients",
    "section": "",
    "text": "You should verify that the dimension of each term is \\(1\\times 1\\).↩︎",
    "crumbs": [
      "Systems of Equations",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Appplication: Estimating Regression Coefficients</span>"
    ]
  },
  {
    "objectID": "40-introduction-to-decomposition.html",
    "href": "40-introduction-to-decomposition.html",
    "title": "19  Introduction to Matrix Decompostion",
    "section": "",
    "text": "Matrix decomposition is a method of reducing or factoring a matrix into a set of product matrices. Another name for this is matrix factorization. Working with these product matrices often makes it easier to carry out more complex matrix operations (e.g., computing an inverse).\nIn many ways, matrix decomposition is similar to factoring scalars. For example, we can factor the scalar 36 as:\n\\[\n36 = 12 \\times 3\n\\]\nWe could also have used the following factorizations:\n\\[\n\\begin{split}\n36 &= 9 \\times 4 \\\\[0.5em]\n36 &= 18 \\times 2 \\\\[0.5em]\n36 &= 6 \\times 3 \\times 2 \\\\[0.5em]\n36 &= 2 \\times 2 \\times 3 \\times 3 \\\\[0.5em]\n\\end{split}\n\\]\nThere are potentially multiple ways to factor a scalar, and in different applications, some of these factorizations may prove more useful than others. For example, the last factorization in the example is referred to as the prime factorization (as the factors of 36 are all prime numbers) and is useful for some mathematical applications.\nThe same is true of matrix decompoosition.\n\nThere are many methods of matrix decomposition.\nDepending on the application, some of these methods are more useful than others.\n\nIn the following chapters, we will explore a few of the more common decomposition methods, including LU decomposition, QR decomposition, singular value decomposition, Cholsky decomposition, and eigen decomposition.",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to Matrix Decompostion</span>"
    ]
  },
  {
    "objectID": "41-lu-decomposition.html",
    "href": "41-lu-decomposition.html",
    "title": "20  LU Decompostion",
    "section": "",
    "text": "20.1 An Example of LU Decomposition\nOne common method of matrix decomposition is LU decomposition. Commonly used to computationally solve systems of equations, and to find the determinant of a matrix, LU decomposition decomposes a square matrix into a pair of triangular matrices to more easily carry out Gaussian elimination. It is based on a theorem in linear algebra, which states:\nAny nonsingular, square matrix, A, can be written as the product of two triangular matrices, L and U, of the same order such that matrix L is a lower-triangular matrix (all elements above the main diagonal are 0) and U is an upper-triangular matrix (all elements below the main diagonal are 0).\n\\[\n\\begin{split}\n\\underset{n\\times n}{\\mathbf{A}} &= \\underset{n\\times n}{\\mathbf{L}}~ \\underset{n\\times n}{\\mathbf{U}} \\\\[2ex]\n&= \\begin{bmatrix}\nl_{11} & 0 & 0 & \\ldots & 0\\\\\nl_{21}& l_{22} & 0 & \\ldots & 0\\\\\nl_{31}& l_{32} & l_{33} & \\ldots & 0\\\\\n\\vdots & \\vdots &\\vdots & \\ddots & \\vdots \\\\\nl_{n1} & l_{n2} & l_{n3} & \\ldots & l_{nn}\n\\end{bmatrix}\\begin{bmatrix}\nu_{11} & u_{12} & u_{13} & \\ldots & u_{1n}\\\\\n0 & u_{22} & u_{23} & \\ldots & u_{2n}\\\\\n0 & 0 & u_{33} & \\ldots & u_{3n}\\\\\n\\vdots & \\vdots  &\\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & u_{nn}\n\\end{bmatrix}\n\\end{split}\n\\]\nThe goal of LU decomposition would be to find the values for each of the non-zero elements in L and U.\nConsider the following \\(2 \\times 2\\) matrix:\n\\[\n\\underset{2\\times 2}{\\mathbf{A}} = \\begin{bmatrix}\n5 & 1  \\\\\n-4 & 2  \\\\\n\\end{bmatrix}\n\\]\nThen LU decomposition would define A as the product of a lower- and upper-triangular matrix:\n\\[\n\\begin{split}\n\\underset{2\\times 2}{\\mathbf{A}} &= \\underset{2\\times 2}{\\mathbf{L}}~\\underset{2\\times 2}{\\mathbf{U}}\\\\[1em]\n\\begin{bmatrix}\n5 & 1  \\\\\n-4 & 2  \\\\\n\\end{bmatrix} &= \\begin{bmatrix}\nl_{11} & 0 \\\\\nl_{21}& l_{22} \\\\\n\\end{bmatrix}\\begin{bmatrix}\nu_{11} & u_{12}  \\\\\n0 & u_{22} \\\\\n\\end{bmatrix}\n\\end{split}\n\\] To determine the elements of L and U, we can write out the system of equations based on the matrix algebra, we get:\n\\[\n\\begin{split}\n5 &= l_{11}(u_{11}) + 0(0) \\\\\n1 &= l_{11}(u_{12}) + 0(u_{22})\\\\\n-4 &= l_{21}(u_{11}) + l_{22}(0)\\\\\n2 &= l_{21}(u_{12}) + l_{22}(u_{22})\\\\\n\\end{split}\n\\]\nUnfortunately there are more unknowns (6) than equations (4) which means the system is underdetermined. To find a unique solution, we need to add additional constraints on the system. One way to do this is to require L to be a unit triangular matrix (i.e. elements on the main diagonal are ones). In our example, \\(l_{11} = l_{22} = 1\\), and our system of equations becomes:\n\\[\n\\begin{split}\n5 &= 1(u_{11}) + 0(0) \\\\\n1 &= 1(u_{12}) + 0(u_{22})\\\\\n-4 &= l_{21}(u_{11}) + 1(0)\\\\\n2 &= l_{21}(u_{12}) + 1(u_{22})\\\\\n\\end{split}\n\\]\nThis system of equations is uniquely solvable (i.e., four equations; four unknowns). Solving for these unknowns, we find:\n\\[\n\\begin{split}\nl_{21} &= -0.8\\\\\nu_{11} &= 5 \\\\\nu_{12} &= 1\\\\\nu_{22} &= 2.8\\\\\n\\end{split}\n\\]\nThus, the LU decomposition of A is,\n\\[\n\\begin{bmatrix}\n5 & 1  \\\\\n-4 & 2  \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 0 \\\\\n-0.8 & 1 \\\\\n\\end{bmatrix}\\begin{bmatrix}\n5 & 1  \\\\\n0 & 2.8 \\\\\n\\end{bmatrix}\n\\]\nChecking our work in R, we find the solution holds.\n# Create L\nL = matrix(\n  data = c(1, -0.8, 0, 1), \n  nrow = 2\n  )\n\n# Create U\nU = matrix(\n  data = c(5, 0, 1, 2.8), \n  nrow = 2\n  )\n\n# Product\nL %*% U\n\n     [,1] [,2]\n[1,]    5    1\n[2,]   -4    2\nWe can carry out LU decomposition using the lu.decomposition() function from the matrixcalc package. This function outputs a list with the L and U matrices.\n# Create A\nA = matrix(\n  data = c(5, -4, 1, 2), \n  nrow = 2\n  )\n\n# Load matrixcalc library\nlibrary(matrixcalc)\n\n# PLU decomposition\nlu_decomp = lu.decomposition(A)\n\n# View results\nlu_decomp\n\n$L\n     [,1] [,2]\n[1,]  1.0    0\n[2,] -0.8    1\n\n$U\n     [,1] [,2]\n[1,]    5  1.0\n[2,]    0  2.8\nHere the results are,\n\\[\n\\begin{split}\n{\\mathbf{L}} &= \\begin{bmatrix}\n1 & 0  \\\\\n-0.8 & 1  \\\\\n\\end{bmatrix} \\\\[1em]\n{\\mathbf{U}} &= \\begin{bmatrix}\n5 & 1  \\\\\n0 & 2.8  \\\\\n\\end{bmatrix}\n\\end{split}\n\\]\nTo double-check that the decomposition worked, we can compute LU and see if we re-obtain A. We use the object name along with the $ notation to access each element of the output.\n# Compute LU\nlu_decomp$L %*% lu_decomp$U\n\n     [,1] [,2]\n[1,]    5    1\n[2,]   -4    2\nThere are other methods of LU decomposition, including pivoting methods (PLU decomposition), which are more applicable in practice. There is also LDU decomposition in which A is decomposed into unit triangular matrices L and U, and diagonal matrix D.",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>LU Decompostion</span>"
    ]
  },
  {
    "objectID": "41-lu-decomposition.html#an-example-of-lu-decomposition",
    "href": "41-lu-decomposition.html#an-example-of-lu-decomposition",
    "title": "20  LU Decompostion",
    "section": "",
    "text": "MATH NOTE\nThe determinant of the decomposed matrix (A) can be computed from finding the product of the diagonal elements of both the L and U matrices from the LU decomposition. From the example, the determinant of A is:\n\\[\n\\begin{split}\n\\mathrm{det}(\\mathbf{A}) &= 1 \\times 1 \\times 5 \\times 2.8 \\\\[2ex]\n&= 14\n\\end{split}\n\\] This is the same result as taking \\(5(2) - (-4)(1) = 14\\). The det() function uses this method for computing the determinant of a matrix.",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>LU Decompostion</span>"
    ]
  },
  {
    "objectID": "41-lu-decomposition.html#solving-systems-of-equations-with-the-lu-decomposition",
    "href": "41-lu-decomposition.html#solving-systems-of-equations-with-the-lu-decomposition",
    "title": "20  LU Decompostion",
    "section": "20.2 Solving Systems of Equations with the LU Decomposition",
    "text": "20.2 Solving Systems of Equations with the LU Decomposition\nImagine if we wanted to solve a set of simultaneous equations to compute unknown values of B using our matrix A, and known values for Y, say,\n\\[\n\\mathbf{A}\\mathbf{X}=\\mathbf{Y}\n\\]\nTo find the elements of X we would need the inverse of our matrix A. However, we know have an alternative solution from the LU decomposition. Since \\(\\mathbf{A}=\\mathbf{L}\\mathbf{U}\\), we can re-write our simultaneuous equations as:\n\\[\n\\mathbf{L}\\mathbf{U}\\mathbf{X}=\\mathbf{Y}\n\\]\nPre-multiplying this by L inverse, we get\n\\[\n\\begin{split}\n\\mathbf{L}^{-1}\\mathbf{L}\\mathbf{U}\\mathbf{X}&=\\mathbf{L}^{-1}\\mathbf{Y} \\\\[0.5em]\n\\mathbf{U}\\mathbf{X}&=\\mathbf{L}^{-1}\\mathbf{Y}\n\\end{split}\n\\]\nLet us call the right-hand side of this equation Z. This gives us two sets of equations related to Z:\n\\[\n\\begin{split}\n\\mathbf{U}\\mathbf{X} &= \\mathbf{Z} \\\\[0.5em]\n\\mathbf{L}^{-1}\\mathbf{Y} &= \\mathbf{Z}\n\\end{split}\n\\]\nThe second equation, we can also express (after pre-multiplying by L) as \\(\\mathbf{L}\\mathbf{Z} = \\mathbf{Y}\\). Thus we now have the two equations that are now based on the matrices L and U from our decomposition.\n\\[\n\\begin{split}\n\\mathbf{U}\\mathbf{X} &= \\mathbf{Z} \\\\[0.5em]\n\\mathbf{L}\\mathbf{Z} &= \\mathbf{Y}\n\\end{split}\n\\]\nRemember, the goal was to solve for X, so to do this, we first solve the second equation, \\(\\mathbf{L}\\mathbf{Z} = \\mathbf{Y}\\), for Z (which is unknown), and then use that to solve the first equation for X. Let’s see it in action using our example. To do so, let’s solve this set of simultaneous equations:\n\\[\n\\underset{\\mathbf{A}}{\\begin{bmatrix}\n5 & 1  \\\\\n-4 & 2  \\\\\n\\end{bmatrix}}\\underset{\\mathbf{X}}{\\begin{bmatrix}\nx_{11}  \\\\\nx_{21}  \\\\\n\\end{bmatrix}} = \\underset{\\mathbf{Y}}{\\begin{bmatrix}\n1  \\\\\n-3  \\\\\n\\end{bmatrix}}\n\\]\n\n20.2.1 Step 1: Solve for Z\n\\[\n\\begin{split}\n\\mathbf{L}\\mathbf{Z} &= \\mathbf{Y} \\\\[0.5em]\n\\begin{bmatrix}\n1 & 0  \\\\\n-0.8 & 1  \\\\\n\\end{bmatrix}\\begin{bmatrix}\nz_{11}  \\\\\nz_{21}  \\\\\n\\end{bmatrix} &= \\begin{bmatrix}\n1  \\\\\n-3  \\\\\n\\end{bmatrix}\n\\end{split}\n\\] The two equations from this multiplication are:\n\\[\n\\begin{split}\nz_{11} &= 1 \\\\[0.5em]\n-0.8z_{11} + z_{21} &= -3\n\\end{split}\n\\]\nThe triangular matrix makes this really easy to solve these equations for the two elements of Z. Since solving these equations boils down to substituting values into each subsequent equation, this method of solving the equations is referred to as forward substitution. Here,\n\\[\n\\begin{split}\nz_{11}&=1 \\\\[2ex]\nz_{21}&=-2.2\n\\end{split}\n\\]\nWe can now use these values of Z in the second equation.\n\n\n20.2.2 Step 2: Solve for X\n\\[\n\\begin{split}\n\\mathbf{U}\\mathbf{X} &= \\mathbf{Z} \\\\[0.5em]\n\\begin{bmatrix}\n5 & 1  \\\\\n0 & 2.8  \\\\\n\\end{bmatrix} \\begin{bmatrix}\nx_{11}  \\\\\nx_{21}  \\\\\n\\end{bmatrix} &= \\begin{bmatrix}\n1  \\\\\n-2.2  \\\\\n\\end{bmatrix}\n\\end{split}\n\\]\nThe two equations from this multiplication are:\n\\[\n\\begin{split}\n5(x_{11}) + x_{21} &= 1 \\\\[0.5em]\n2.8(x_{21}) &= -2.2\n\\end{split}\n\\]\nAgain, the triangular matrix makes this really easy to solve these equations for the two elements of X. Solving these equations boils down to substituting values from later equations into each of the earlier equations. Hence, this method of solving the equations is referred to as back substitution. After carrying out the algebra,\n\\[\n\\mathbf{B} \\approx\\begin{bmatrix}\n0.36  \\\\\n-0.79  \\\\\n\\end{bmatrix}\n\\]\nThe exciting thing here is that we have solved for X without ever finding the inverse of the A matrix! It turns out that this is also a much more computationally efficient method to solve systems of equations. (Even though it maybe didn’t feel like it when we used a \\(2 \\times 2\\) matrix.)\n\n\n\n20.2.3 Using R to Solve the Two Equations\nWe can also use R to solve the two equations, \\(\\mathbf{LZ}=\\mathbf{Y}\\) and \\(\\mathbf{UX}=\\mathbf{Z}\\). The function forwardsolve() (forward substitution) can be used to solve the equation \\(\\mathbf{LZ}=\\mathbf{Y}\\), and more generally, any equation where a lower-triangular matrix is being pre-multiplied by a matrix of unknown elements to produce another known matrix. This function takes a lower-triangular matrix as its first argument and the solution matrix as its second argument. The syntax to solve the following equation for the elements of Z is given below.\n\\[\n\\begin{split}\n\\mathbf{L}\\mathbf{Z} &= \\mathbf{Y} \\\\[0.5em]\n\\begin{bmatrix}\n1 & 0  \\\\\n-0.8 & 1  \\\\\n\\end{bmatrix}\\begin{bmatrix}\nz_{11}  \\\\\nz_{21}  \\\\\n\\end{bmatrix} &= \\begin{bmatrix}\n1  \\\\\n-3  \\\\\n\\end{bmatrix}\n\\end{split}\n\\]\n\n# Create L\nL = matrix(\n  data = c(1, -0.8, 0, 1),\n  nrow = 2\n)\n\n# Create Y\nY = matrix(\n  data = c(1, -3),\n  nrow = 2\n)\n\n# Solve for Z\nZ = forwardsolve(L, Y)\n\n# View Z\nZ\n\n     [,1]\n[1,]  1.0\n[2,] -2.2\n\n\nTo solve an upper-triangular system of equations—one in which an upper-triangular matrix is being pre-multiplied by a matrix of unknown elements to produce another known matrix—we use the backsolve() function (backward substitution). This function takes an upper-triangular matrix as its first argument and the solution matrix as its second argument. The syntax to solve the following equation for the elements of X is given below.\n\\[\n\\begin{split}\n\\mathbf{U}\\mathbf{X} &= \\mathbf{Z} \\\\[0.5em]\n\\begin{bmatrix}\n5 & 1  \\\\\n0 & 2.8  \\\\\n\\end{bmatrix} \\begin{bmatrix}\nx_{11}  \\\\\nx_{21}  \\\\\n\\end{bmatrix} &= \\begin{bmatrix}\n1  \\\\\n-2.2  \\\\\n\\end{bmatrix}\n\\end{split}\n\\]\n\n# Create L\nU = matrix(\n  data = c(5, 0, 1, 2.8),\n  nrow = 2\n)\n\n# Solve for X\nX = backsolve(U, Z)\n\n# View X\nX\n\n           [,1]\n[1,]  0.3571429\n[2,] -0.7857143\n\n\nThese functions can also take as arguments the lower- or upper-triangular matrices produced from the lu() function. For example:\n\n# Solve for Z\nZ = forwardsolve(lu_decomp$L, Y)\n\n# Solve for X\nX = backsolve(lu_decomp$U, Z)\n\n# View X\nX\n\n           [,1]\n[1,]  0.3571429\n[2,] -0.7857143",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>LU Decompostion</span>"
    ]
  },
  {
    "objectID": "41-lu-decomposition.html#application-of-lu-decomposition-in-computing",
    "href": "41-lu-decomposition.html#application-of-lu-decomposition-in-computing",
    "title": "20  LU Decompostion",
    "section": "20.3 Application of LU Decomposition in Computing",
    "text": "20.3 Application of LU Decomposition in Computing\nOne application of LU decomposition in computing is in the computation of a determinant. The determinant is often computed by taking the product of the elements on the diagonal of both the L and U matrices. Since LU decomposition is quite efficient, this is a computationally efficient way of computing the determinant. The det() function R, for example, uses this method to compute the determinant. This method, however, can lead to “wrong” results, especially for matrices that have a determinant of 0.\nConsider the following matrix,\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n3 & 2  \\\\\n1.2 & 0.8  \\\\\n\\end{bmatrix}\n\\]\nThe determinant can be computed as:\n\\[\n\\begin{split}\n\\mathrm{det}(\\mathbf{A}) &= 3(0.8) - 1.2(2) \\\\\n&= 2.4 - 2.4 \\\\\n&= 0\n\\end{split}\n\\]\nHowever, if we use the det() function on the matrix, we find:\n\n# Create A\nA = matrix(\n    data = c(3, 1.2, 2, 0.8),\n    nrow = 2\n)\n\n# Compute determinant\ndet(A)\n\n[1] 3.330669e-16\n\n\nWhile close to 0, the det() function does not return 0.\nThis function, as mentioned earlier, computes the determinant by carrying out an LU decomposition using a Fortran routine called LAPACK (Linear Algebra PACKage). This routine is also used in the lu.decomposition() function.\n\n# Use LAPACK routine for LU decomposition\nlu.decomposition(A)\n\n$L\n     [,1] [,2]\n[1,]  1.0    0\n[2,]  0.4    1\n\n$U\n     [,1]         [,2]\n[1,]    3 2.000000e+00\n[2,]    0 1.110223e-16\n\n\nNote that the diagonal elements of these matrices are 1, 1, 3, and \\(1.110223\\times 10^{-16}\\). Multiplying these together, we get \\(3.330669\\times 10^{-16}\\), the same result as we got from det(). The routines used to carry out the LU decomposition are computationally approximate, and suffer from the same floating point issues that all computation suffers from. Because of this, the U matrix has a diagonal element of \\(1.110223\\times 10^{-16}\\) rather than 0.\nNote that using zero in the U matrix produces the correct decomposition.\n\\[\n\\begin{split}\n\\underset{\\mathbf{A}}{\\begin{bmatrix}\n3 & 2  \\\\\n1.2 & 0.8  \\\\\n\\end{bmatrix}} &= \\underset{\\mathbf{L}}{\\begin{bmatrix}\n1 & 0  \\\\\n0.4 & 1  \\\\\n\\end{bmatrix}}\\underset{\\mathbf{U}}{\\begin{bmatrix}\n3 & 2  \\\\\n0 & 0  \\\\\n\\end{bmatrix}}\\\\[1em]\n\\begin{bmatrix}\n5 & 1  \\\\\n-4 & 2  \\\\\n\\end{bmatrix} &= \\begin{bmatrix}\nl_{11} & 0 \\\\\nl_{21}& l_{22} \\\\\n\\end{bmatrix}\\begin{bmatrix}\nu_{11} & u_{12}  \\\\\n0 & u_{22} \\\\\n\\end{bmatrix}\n\\end{split}\n\\]",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>LU Decompostion</span>"
    ]
  },
  {
    "objectID": "42-statistical-application-lu-decomposition.html",
    "href": "42-statistical-application-lu-decomposition.html",
    "title": "21  Statistical Application: Estimating Regression Coefficients with LU Decomposition",
    "section": "",
    "text": "In OLS regression our goal is to estimate regression coefficients (b) from a data matrix (X) and vector of outcomes (y). To do this we want to solve the following equation for b:\n\\[\n(\\mathbf{X}^{\\intercal}\\mathbf{X})\\mathbf{b} = \\mathbf{X}^{\\intercal}\\mathbf{y}\n\\]\nPre-multiplying both sides by \\((\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\) gives us our conventional solution for b, namely \\(\\mathbf{b}=(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y}\\). However, this requires us to find the inverse of \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\). Unfortunately, the computational functions that are used to compute inverses of matrices sometimes end up being numerically unstable. For example, consider the following simulated data set of \\(n=50\\) cases:\n\n# Number of cases\nn = 50\n\n# Create 50 x-values evenly spread b/w 1 and 500 \nx = seq(from = 1, to = 500, len = n)\n\n# Create X matrix\nX = cbind(1, x, x^2, x^3)\n\n# Create b matrix\nb = matrix(\n  data = c(1, 1, 1, 1), \n  nrow = 4\n  )\n\n# Create vector of y-values\nset.seed(1)\ny = X %*% b + rnorm(n, mean = 0, sd = 1)\n\n# Find determinant of (X^T)X\ndet(t(X) %*% X)\n\n[1] 3.113748e+32\n\n\nHere we have simulated data using the following statistical model:\n\\[\ny_i = 1 + 1(x_i) + 1(x_i^2) + 1(x_i^3) + e_i \\qquad \\mathrm{where~} e_i\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,1)\n\\]\nThe determinant of \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) is not zero (although it is close to zero), so we should be able to find an inverse. What happens if we try to use solve() to find the inverse of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix to obtain the regression coefficient estimates? Here crossprod(X) is equivalent to t(X) %*% X, and crossprod(X, y) is equivalent to t(X) %*% y.\n\n# Try to compute b\nsolve(crossprod(X)) %*% crossprod(X, y)\n\nError in solve.default(crossprod(X)): system is computationally singular: reciprocal condition number = 2.93617e-17\n\n\nThe standard R function for inverse is computationally singular. To see why this happens, we can take a closer look at the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix. Here we will examine these values:\n\n# Set the number of digits\noptions(digits = 4)\n\n# Compute X^T(X) matrix\ncrossprod(X)\n\n                        x                    \n          50        12525 4.217e+06 1.597e+09\nx      12525      4217364 1.597e+09 6.454e+11\n     4217364   1597455115 6.454e+11 2.716e+14\n  1597455115 645401757068 2.716e+14 1.176e+17\n\n\nNote the difference of several orders of magnitude. On a computer, we have a limited range of numbers. This makes some numbers behave like 0, when we also have to consider very large numbers. This in turn leads to what is essentially division by 0, which produces errors.\n\n\n21.0.1 Estimating Regression Coefficients Using LU Decomposition\nRemember, our goal is to solve the following for b.\n\\[\n(\\mathbf{X}^{\\intercal}\\mathbf{X})\\mathbf{b} = \\mathbf{X}^{\\intercal}\\mathbf{y}\n\\]\nThis is exactly the type of problem that LU decomposition can help solve. Note that the first “term”, \\((\\mathbf{X}^{\\intercal}\\mathbf{X})\\), is a square matrix. The right-hand side of the equation is known from the data, and we want to solve for the elements of b. This is the exact format of \\(\\mathbf{AX}=\\mathbf{Y}\\). So rather than trying to find the inverse of \\((\\mathbf{X}^{\\intercal}\\mathbf{X})\\), we can carry out the decomposition on the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix to produce an L and U matrix that we can then use to solve for b.\n\n# LU decomposition\nlibrary(matrixcalc)\nlu_decomp = lu.decomposition(crossprod(X))\n\n# View results\nlu_decomp\n\n$L\n           [,1]   [,2]  [,3] [,4]\n[1,]        1.0      0   0.0    0\n[2,]      250.5      1   0.0    0\n[3,]    84347.3    501   1.0    0\n[4,] 31949102.3 227105 751.5    1\n\n$U\n     [,1]    [,2]      [,3]      [,4]\n[1,]   50   12525 4.217e+06 1.597e+09\n[2,]    0 1079851 5.410e+08 2.452e+11\n[3,]    0       0 1.863e+10 1.400e+13\n[4,]    0       0 1.953e-03 3.095e+14\n\n\nNow we can solve for Z in the equation \\(\\mathbf{LZ}=\\mathbf{Y}\\), but remembering that here, the right-hand side of this equation is \\(\\mathbf{X}^{\\intercal}\\mathbf{y}\\), so we are solving for Z in:\n\\[\n\\mathbf{LZ}=\\mathbf{X}^{\\intercal}\\mathbf{y}\n\\]\nIn our example,\n\\[\n\\begin{bmatrix}\n1 & 0 & 0 & 0\\\\\n2.640052 \\times 10^{-3} & 1  & 0 & 0\\\\\n7.840596  \\times 10^{-6} & 7.919771  \\times 10^{-3} & 1 & 0\\\\\n3.129978  \\times 10^{-8} & 7.211598  \\times 10^{-5} & 2.495756  \\times 10^{-2} & 1\n\\end{bmatrix}\\begin{bmatrix}\nz_{11}  \\\\\nz_{21}  \\\\\nz_{31}  \\\\\nz_{41}  \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n1.601685  \\times 10^{9}\\\\\n6.470034  \\times 10^{11}\\\\\n2.722570  \\times 10^{14}\\\\\n1.178380  \\times 10^{17}\\\\\n\\end{bmatrix}\n\\]\nThen we can solve the equation:\n\\[\n\\mathbf{Ub}=\\mathbf{Z}\n\\]\nwhich in our example is:\n\\[\n\\begin{bmatrix}\n1.597455 \\times 10^{9} & 6.454018 \\times 10^{11} & 2.7161 \\times 10^{14} & 1.175658 \\times 10^{17}\\\\\n1 & -1.064388 \\times 10^{8} & -7.166250 \\times 10^{10} & -3.876978 \\times 10^{13}\\\\\n1 &             1 &  3.542182 \\times 10^{7} & 3.066370 \\times 10^{10}\\\\\n1 &             1 &             1 & -5.169923 \\times 10^{7}\n\\end{bmatrix}\\begin{bmatrix}\nb_{11}  \\\\\nb_{21}  \\\\\nb_{31}  \\\\\nb_{41}  \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n1.601685 \\times 10^{9}\\\\\n6.469992 \\times 10^{11}\\\\\n2.722518 \\times 10^{14}\\\\\n1.178312 \\times 10^{17}\n\\end{bmatrix}\n\\]\n\n# Solve for Z\nZ = forwardsolve(lu_decomp$L,  crossprod(X,y))\n\n# Solve for b\nb = backsolve(lu_decomp$U, Z)\n\n# View b\nb\n\n       [,1]\n[1,] 0.9038\n[2,] 1.0066\n[3,] 1.0000\n[4,] 1.0000\n\n\nWe have successfully estimated the coefficients, which are all near 1. Note that fitting the model using lm() we obtain the same coefficients.\n\n# Fit model using columns of the X matrix\nlm.1 = lm(y ~ 1 + X[ , 2] + X[ , 3] + X[ , 4])\n\n# View coefficientw\ncoef(lm.1)\n\n(Intercept)      X[, 2]      X[, 3]      X[, 4] \n     0.9038      1.0066      1.0000      1.0000 \n\n\nThe lm() function (and most other statistical software) uses decomposition to solve for coefficients rather than inverting the \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) matrix. Although the estimates here are the same as those from LU decomposition, the lm() function actually uses QR decomposition. We will examine this method in the next chapter.",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Statistical Application: Estimating Regression Coefficients with LU Decomposition</span>"
    ]
  },
  {
    "objectID": "43-cholesky-decomposition.html",
    "href": "43-cholesky-decomposition.html",
    "title": "22  Cholesky Decompostion",
    "section": "",
    "text": "22.1 Example of Cholesky Decomposition\nAnother decomposition method is Cholesky (or Choleski) decomposition. The Cholesky decomposition method—used in statistical applications from nonlinear optimization, to Monte Carlo simulation methods, to Kalman filtering—is much more computationally efficient than the LU method. The Cholesky method decomposes a symmetric, positive definite matrix A into the product of two matrices, \\(\\mathbf{L}\\) (a lower-triangular matrix) and \\(\\mathbf{L}^*\\) (the conjugate transpose of \\(\\mathbf{L}\\)).1\n\\[\n\\underset{n \\times n}{\\mathbf{A}} = \\mathbf{LL}^*\n\\]\nThe conjugate transpose is computed by taking the transpose of a matrix and then finding the complex conjugate of each element in the matrix. To understand what a complex conjugate is, we first remind you of the idea of a complex number. Remember that all real and imaginary numbers are complex numbers which can be expressed as \\(a+bi\\), where a is the real part of the number and b is the imaginary part of the number, and i is the square root of \\(-1\\). For example the number 2 can be expressed as \\(2 + 0i\\). Note: The complex conjugate of a real number is just the real number, since \\(a+0i = a-0i=a\\).\nThe complex conjugate of a number is itself a complex number that has the exact same real part and an imaginary part equal in magnitude but opposite in sign. For example the complex conjugate for the number \\(3 + 2i\\) is \\(3 - 2i\\).\nAs an example, say that matrix A was\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n1 & 3+i & -2+3i  \\\\\n0 & 4 & 0-i  \\\\\n\\end{bmatrix}\n\\]\nThe conjugate transpose of A, symbolized as \\(\\mathbf{A}^*\\), can be found by first computing the transpose of A, and then finding the complex conjugate of each element in the transpose.\n\\[\n\\mathbf{A}^{\\intercal} = \\begin{bmatrix}\n1 & 0  \\\\\n3+i0 & 4  \\\\\n-2+3i & 0-i\n\\end{bmatrix}\n\\]\nAnd,\n\\[\n\\mathbf{A}^{*} = \\begin{bmatrix}\n1 & 0  \\\\\n3-i0 & 4  \\\\\n-2-3i & 0+i\n\\end{bmatrix}\n\\]\nSay we wanted to compute a Cholsky decomposition on a \\(2 \\times 2\\) symmetric matrix:\n\\[\n\\underset{2\\times 2}{\\mathbf{A}} = \\begin{bmatrix}\n5 & -4  \\\\\n-4 & 5  \\\\\n\\end{bmatrix}\n\\]\nThe Cholesky decomposition would factor A into the following:\n\\[\n\\begin{split}\n\\underset{2\\times 2}{\\mathbf{A}} &= \\underset{2\\times 2}{\\mathbf{L}}~\\underset{2\\times 2}{\\mathbf{L}^\\intercal}\\\\[1em]\n\\begin{bmatrix}\n5 & -4  \\\\\n-4 & 5  \\\\\n\\end{bmatrix} &= \\begin{bmatrix}\nl_{11} & 0  \\\\\nl_{21} & l_{22}  \\\\\n\\end{bmatrix} \\begin{bmatrix}\nl_{11} & l_{21}  \\\\\n0 & l_{22}  \\\\\n\\end{bmatrix}\n\\end{split}\n\\]\nCarrying out the matrix algebra we have the following three unique equations:\n\\[\n\\begin{split}\n5 &= l_{11}(l_{11}) \\\\[0.5em]\n-4 &= l_{11}(l_{21}) \\\\[0.5em]\n5 &= l_{21}(l_{21}) + l_{22}(l_{22})\n\\end{split}\n\\]\nSince we have three equations with three unknowns we can solve for each element in L.\n\\[\n\\begin{split}\nl_{11} &= \\sqrt{5} \\approx 2.24\\\\[0.5em]\nl_{21} &= \\dfrac{-4}{\\sqrt{5}} \\approx -1.79 \\\\[0.5em]\nl_{22} &= \\sqrt{1.8} \\approx 1.34\n\\end{split}\n\\]\nSo the Cholsky decomposition is,\n\\[\n\\begin{bmatrix}\n5 & -4  \\\\\n-4 & 5  \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n\\sqrt{5}  & 0  \\\\\n\\dfrac{-4}{\\sqrt{5}} & \\sqrt{1.8}  \\\\\n\\end{bmatrix} \\begin{bmatrix}\n\\sqrt{5}  & \\dfrac{-4}{\\sqrt{5}}  \\\\\n0 & \\sqrt{1.8}  \\\\\n\\end{bmatrix}\n\\]\nOr using the approximations,\n\\[\n\\begin{bmatrix}\n5 & -4  \\\\\n-4 & 5  \\\\\n\\end{bmatrix} \\approx \\begin{bmatrix}\n2.24  & 0  \\\\\n-1.79 & 1.34  \\\\\n\\end{bmatrix} \\begin{bmatrix}\n2.24  & -1.79  \\\\\n0 & 1.34  \\\\\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Cholesky Decompostion</span>"
    ]
  },
  {
    "objectID": "43-cholesky-decomposition.html#cholesky-decomposition-using-r",
    "href": "43-cholesky-decomposition.html#cholesky-decomposition-using-r",
    "title": "22  Cholesky Decompostion",
    "section": "22.2 Cholesky Decomposition using R",
    "text": "22.2 Cholesky Decomposition using R\nWe can use the chol() function to compute the Cholesky decomposition. For example to carry out the Cholesky decomposition on A form the previous section, we would use the following syntax:\n\n# Create A\nA = matrix(\n  data = c(5, -4, -4, 5), \n  nrow = 2\n  )\n\n# Cholesky decomposition\ncholesky_decomp = chol(A)\n\n# View results\ncholesky_decomp\n\n         [,1]      [,2]\n[1,] 2.236068 -1.788854\n[2,] 0.000000  1.341641\n\n\nNote that the output from chol() is actually the transpose matrix, \\(\\mathbf{L}^\\intercal\\). To obtain L, we need to take the transpose of this output. We can also verify that the decomposition worked.\n\n# Obtain L\nt(cholesky_decomp)\n\n          [,1]     [,2]\n[1,]  2.236068 0.000000\n[2,] -1.788854 1.341641\n\n# Check results L(L^T) = A\nt(cholesky_decomp) %*% cholesky_decomp\n\n     [,1] [,2]\n[1,]    5   -4\n[2,]   -4    5\n\n\n\nCAUTION\nThe chol() function does not check for symmetry. If you use the function to decompose a nonsymmetric matrix, the results will likely be meaningless.\n\nFor example, consider the following nonsymmetric matrix:\n\\[\n\\mathbf{A} = \\begin{bmatrix}5 & 1\\\\ -4 & 2\\end{bmatrix}\n\\]\nComputing the decomposition, we get:\n\n# Create A\nA = matrix(\n  data = c(5, -4, 1, 2), \n  nrow = 2\n  )\n\n# Cholsky decomposition\nchol(A)\n\n         [,1]      [,2]\n[1,] 2.236068 0.4472136\n[2,] 0.000000 1.3416408\n\n\nChecking the matrix multiplication we do not get back to the original matrix, A:\n\n# Check results\nt(chol(A)) %*% chol(A)\n\n     [,1] [,2]\n[1,]    5    1\n[2,]    1    2",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Cholesky Decompostion</span>"
    ]
  },
  {
    "objectID": "43-cholesky-decomposition.html#statistical-application-estimating-regression-coefficents-with-cholesky-decomposition",
    "href": "43-cholesky-decomposition.html#statistical-application-estimating-regression-coefficents-with-cholesky-decomposition",
    "title": "22  Cholesky Decompostion",
    "section": "22.3 Statistical Application: Estimating Regression Coefficents with Cholesky Decomposition",
    "text": "22.3 Statistical Application: Estimating Regression Coefficents with Cholesky Decomposition\nWe can again use the the OLS solution to compute the elements of b by solving the system of equations represented in:\n\\[\n(\\mathbf{X}^{\\intercal}\\mathbf{X})\\mathbf{b} = \\mathbf{X}^{\\intercal}\\mathbf{y}\n\\]\nSo long as \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) is positive definite, we can use Cholesky decomposition to solve for the elements of b using the same two-step process we did for LU decomposition. Namely,\n\\[\n\\begin{split}\n&\\mathrm{Solve~~}\\mathbf{LZ} &= \\mathbf{X}^{\\intercal}\\mathbf{y}~~\\mathrm{for~}\\mathbf{Z} \\\\[2ex]\n&\\mathrm{Solve~~}\\mathbf{L}^\\intercal\\mathbf{b} & =\\mathbf{Z}~~\\mathrm{for~}\\mathbf{b}\n\\end{split}\n\\]\nUsing the same simulated data as the example from last chapter, consider the following simulated data set of \\(n=50\\) cases:\n\n# Number of cases\nn = 50\n\n# Create 50 x-values evenly spread b/w 1 and 500 \nx = seq(from = 1, to = 500, len = n)\n\n# Create X matrix\nX = cbind(1, x, x^2, x^3)\n\n# Create b matrix\nb = matrix(\n  data = c(1, 1, 1, 1), \n  nrow = 4\n  )\n\n# Create vector of y-values\nset.seed(1)\ny = X %*% b + rnorm(n, mean = 0, sd = 1)\n\nWe can again use forwardsolve() and backsolve() to help solve the two equations.\n\n# Obtain L^T and L\nL_T = chol(t(X)%*%X)\nL = t(L_T)\n\n# Solve for Z\nZ = forwardsolve(L,  crossprod(X,y))\n\n# Solve for b\nb = backsolve(L_T, Z)\n\n# View b\nb\n\n          [,1]\n[1,] 0.9038377\n[2,] 1.0066440\n[3,] 0.9999622\n[4,] 1.0000001",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Cholesky Decompostion</span>"
    ]
  },
  {
    "objectID": "43-cholesky-decomposition.html#footnotes",
    "href": "43-cholesky-decomposition.html#footnotes",
    "title": "22  Cholesky Decompostion",
    "section": "",
    "text": "There are also variations on the Cholesky decomposition method, including LDL and LDL\\(^\\intercal\\) decomposition. In these methods, D is a diagonal matrix.↩︎",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Cholesky Decompostion</span>"
    ]
  },
  {
    "objectID": "44-qr-decomposition.html",
    "href": "44-qr-decomposition.html",
    "title": "23  QR Decompostion",
    "section": "",
    "text": "23.1 QR Decomposition using R\nAnother decomposition method is QR decomposition. One advantage of QR decomposition over LU decomposition is that this method does not require that the decomposition be carried out on a square matrix. QR decomposition results in factoring matrix A (having independent columns) into the product of two matrices, namely Q and R: .\n\\[\n\\underset{m\\times n}{\\mathbf{A}} = \\underset{m\\times m}{\\mathbf{Q}}~ \\underset{m\\times n}{\\mathbf{R}}\n\\]\nwhere Q is an orthogonal matrix1 and R is an upper-triangular matrix.\nAlthough there is a way to hand-calculate the matrices Q and R (e.g., using the Gram-Schmidt process), we will rely on computation. To carry out a QR decomposition in R, we will use the qr() function which factors the matrix and returns a list of output related to the QR decomposition. To extract the actual Q and R matrices from this output, we will use the qr.Q() and qr.R() functions, respectively.\n# Create matrix A\nA = matrix(\n  data = c(5, -4, 1, 2), \n  nrow = 2\n  )\n\n# Carry out QR decomposition\nqr_decomp = qr(A)\n\n# View Q matrix\nqr.Q(qr_decomp)\n\n           [,1]      [,2]\n[1,] -0.7808688 0.6246950\n[2,]  0.6246950 0.7808688\n\n# View R matrix\nqr.R(qr_decomp)\n\n          [,1]      [,2]\n[1,] -6.403124 0.4685213\n[2,]  0.000000 2.1864327\nWe find:\n\\[\n\\begin{split}\n\\mathbf{A} &= \\mathbf{QR} \\\\[2ex]\n\\begin{bmatrix}5 & 1 \\\\ -4 & 2\\end{bmatrix} &= \\begin{bmatrix}-0.7809 & 0.6247 \\\\ 0.6247 & 0.7809\\end{bmatrix}\\begin{bmatrix}-6.403 & 0.4685 \\\\ 0 & 2.1864\\end{bmatrix}\n\\end{split}\n\\]\nYou can see that R is an upper-triangular matrix, and we can check that Q is orthonormal by testing whether \\(\\mathbf{Q}^{\\intercal} = \\mathbf{Q}^{-1}\\).\n# Q^T\nt(qr.Q(qr_decomp))\n\n           [,1]      [,2]\n[1,] -0.7808688 0.6246950\n[2,]  0.6246950 0.7808688\n\n# Q^-1\nsolve(qr.Q(qr_decomp))\n\n           [,1]      [,2]\n[1,] -0.7808688 0.6246950\n[2,]  0.6246950 0.7808688\nWe can also check to see that the product of the decomposed matrices is A.\n# A = QR?\nqr.Q(qr_decomp) %*% qr.R(qr_decomp)\n\n     [,1] [,2]\n[1,]    5    1\n[2,]   -4    2",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>QR Decompostion</span>"
    ]
  },
  {
    "objectID": "44-qr-decomposition.html#statistical-application-estimating-regression-coefficents-with-qr-decomposition",
    "href": "44-qr-decomposition.html#statistical-application-estimating-regression-coefficents-with-qr-decomposition",
    "title": "23  QR Decompostion",
    "section": "23.2 Statistical Application: Estimating Regression Coefficents with QR Decomposition",
    "text": "23.2 Statistical Application: Estimating Regression Coefficents with QR Decomposition\nWe can use the two decomposed matrices, Q and R, to compute the elements of b in the OLS solution, solving the system of equations represented in:\n\\[\n(\\mathbf{X}^{\\intercal}\\mathbf{X})\\mathbf{b} = \\mathbf{X}^{\\intercal}\\mathbf{y}\n\\]\nSince QR decomposition works on non-square matrices, we can decompose X as:\n\\[\n\\mathbf{X} = \\mathbf{QR}\n\\]\nThen the OLS equation can be written as:\n\\[\n\\begin{split}\n(\\mathbf{QR})^{\\intercal}\\mathbf{QR}\\mathbf{b} &= (\\mathbf{QR})^{\\intercal}\\mathbf{y} \\\\[2ex]\n\\mathbf{R}^{\\intercal}\\mathbf{Q}^{\\intercal}\\mathbf{QR}\\mathbf{b} &= \\mathbf{R}^{\\intercal}\\mathbf{Q}^{\\intercal}\\mathbf{y}\n\\end{split}\n\\]\nSince Q is orthonormal, and \\(\\mathbf{QQ}^\\intercal = \\mathbf{Q}^\\intercal\\mathbf{Q}=\\mathbf{I}\\), then this reduces to:\n\\[\n\\begin{split}\n\\mathbf{R}^{\\intercal}\\mathbf{I}\\mathbf{R}\\mathbf{b} &= \\mathbf{R}^{\\intercal}\\mathbf{Q}^{\\intercal}\\mathbf{y} \\\\[2ex]\n\\mathbf{R}^{\\intercal}\\mathbf{R}\\mathbf{b} &= \\mathbf{R}^{\\intercal}\\mathbf{Q}^{\\intercal}\\mathbf{y} \\\\[2ex]\n\\mathbf{R}\\mathbf{b} &= \\mathbf{Q}^{\\intercal}\\mathbf{y}\n\\end{split}\n\\]\nSince R is an upper-triangular matrix, we can use back substitution to solve for the elements of b. Using the same simulated data as the example from last chapter, consider the following simulated data set of \\(n=50\\) cases:\n\n# Number of cases\nn = 50\n\n# Create 50 x-values evenly spread b/w 1 and 500 \nx = seq(from = 1, to = 500, len = n)\n\n# Create X matrix\nX = cbind(1, x, x^2, x^3)\ncolnames(X) &lt;- c(\"Intercept\", \"x\", \"x2\", \"x3\")\n\n# Create b matrix\nb = matrix(\n  data = c(1, 1, 1, 1), \n  nrow = 4\n  )\n\n# Create vector of y-values\nset.seed(1)\ny = X %*% b + rnorm(n, mean = 0, sd = 1)\n\nWe can use QR decomposition to solve for b.\n\n# Carry out QR decomposition\nQR = qr(X)\n\n# Solve for b\nbacksolve(qr.R(QR), t(qr.Q(QR)) %*% y)\n\n          [,1]\n[1,] 0.9038372\n[2,] 1.0066440\n[3,] 0.9999622\n[4,] 1.0000001\n\n\n\nFYI\nQR decomposition is how the lm() function computes the regression coefficients.",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>QR Decompostion</span>"
    ]
  },
  {
    "objectID": "44-qr-decomposition.html#footnotes",
    "href": "44-qr-decomposition.html#footnotes",
    "title": "23  QR Decompostion",
    "section": "",
    "text": "The columns of an orthogonal matrix are orthogonal unit vectors and it has the property that \\(\\mathbf{Q}^{T}\\mathbf{Q}=\\mathbf{I}\\) or that \\(\\mathbf{Q}^{\\intercal} = \\mathbf{Q}^{-1}\\).↩︎",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>QR Decompostion</span>"
    ]
  },
  {
    "objectID": "45-spectral-decomposition.html",
    "href": "45-spectral-decomposition.html",
    "title": "24  Spectral Decompostion",
    "section": "",
    "text": "24.0.1 An Example\nSpectral decomposition (a.k.a., eigen decomposition) is used primarily in principal components analysis (PCA). This method decomposes a square matrix, A, into the product of three matrices:\n\\[\n\\underset{n\\times n}{\\mathbf{A}} = \\underset{n\\times n}{\\mathbf{P}}~ \\underset{n\\times n}{\\mathbf{D}}~ \\underset{n\\times n}{\\mathbf{P}^{\\intercal}}\n\\]\nwhere, P is a n-dimensional square matrix whose ith column is the ith eigenvector of A, and D is a n-dimensional diagonal matrix whose diagonal elements are composed of the eigenvalues of A. That is, the spectral decomposition is based on the eigenstructure of A.\nRecall that in a previous chapter we used the following \\(2 \\times 2\\) matrix as an example:\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n-3 & 5  \\\\\n4 & -2  \\\\\n\\end{bmatrix}\n\\]\nThe eigenstructure for this matrix was:\n\\[\n\\begin{split}\n\\lambda_1 &= -7 \\qquad &\\mathbf{e}_1 = \\begin{bmatrix}\\frac{5}{\\sqrt{41}} \\\\ -\\frac{4}{\\sqrt{41}}\\end{bmatrix}\\\\[2ex]\n\\lambda_2 &= 2 \\qquad &\\mathbf{e}_2 = \\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{bmatrix} \\\\[2ex]\n\\end{split}\\]\nThe P and D matrices of the spectral decomposition are composed of the eigenvectors and eigenvalues, respectively.\n\\[\n\\begin{split}\n\\mathbf{P} &= \\begin{bmatrix}\\frac{5}{\\sqrt{41}} & \\frac{1}{\\sqrt{2}} \\\\ -\\frac{4}{\\sqrt{41}} & \\frac{1}{\\sqrt{2}}\\end{bmatrix} \\\\[2ex]\n\\mathbf{D} &= \\begin{bmatrix}7 & 0 \\\\ 0 & -2\\end{bmatrix}\n\\end{split}\n\\]\nRecall also that the eigen() function provided the eigenvalues and eigenvectors for an inputted square matrix. The eigenvectors were outputted as columns in a matrix, so, the $vector output from the function is, in fact, outputting the matrix P. The eigen() function is actually carrying out the spectral decomposition! We can use this output to verify the decomposition by computing whether \\(\\mathbf{PDP}^{-1}=\\mathbf{A}\\).\n# Create A\nA = matrix(\n  data = c(-3, 4, 5, -2), \n  nrow = 2\n  )\n\n# Compute eigenvalues and eigenvectors\nspec_decomp = eigen(A)\n\n# Create P\nP = spec_decomp$vectors\n\n# Create D\nD = diag(spec_decomp$values)\n\n# Verify the decomposition\nP %*% D %*% solve(P)\n\n     [,1] [,2]\n[1,]   -3    5\n[2,]    4   -2\nOf note, when A is symmetric, then the P matrix will be orthogonal; \\(\\mathbf{P}^{-1}=\\mathbf{P}^\\intercal\\). We can illustrate this by an example:\n# Create symmetric matrix A\nA = matrix(\n  data = c(1, 4, 2, 4, 1, 3, 2, 3, 1), \n  nrow = 3\n  )\n\n# Compute eigenvalues and eigenvectors\nspec_decomp = eigen(A)\n\n# Create P\nP = spec_decomp$vectors\n\n# Inverse of P\nsolve(P)\n\n           [,1]       [,2]       [,3]\n[1,] -0.5843738 -0.6345775 -0.5057852\n[2,]  0.5449251  0.1549789 -0.8240377\n[3,]  0.6013018 -0.7571611  0.2552315\n\n# Transpose of P\nt(P)\n\n           [,1]       [,2]       [,3]\n[1,] -0.5843738 -0.6345775 -0.5057852\n[2,]  0.5449251  0.1549789 -0.8240377\n[3,]  0.6013018 -0.7571611  0.2552315\nThis is a useful property since it means that the inverse of P is easy to compute.",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Spectral Decompostion</span>"
    ]
  },
  {
    "objectID": "45-spectral-decomposition.html#solving-systems-of-equations-with-spectral-decomposition",
    "href": "45-spectral-decomposition.html#solving-systems-of-equations-with-spectral-decomposition",
    "title": "24  Spectral Decompostion",
    "section": "24.1 Solving Systems of Equations with Spectral Decomposition",
    "text": "24.1 Solving Systems of Equations with Spectral Decomposition\nWe can use spectral decomposition to more easily solve systems of equations. For example, in OLS estimation, our goal is to solve the following for b.\n\\[\n(\\mathbf{X}^{\\intercal}\\mathbf{X})\\mathbf{b} = \\mathbf{X}^{\\intercal}\\mathbf{y}\n\\]\nSince \\((\\mathbf{X}^{\\intercal}\\mathbf{X})\\) is a square, symmetric matrix, we can decompose it into \\(\\mathbf{PDP}^\\intercal\\). Thus,\n\\[\n\\mathbf{PDP}^{\\intercal}\\mathbf{b} = \\mathbf{X}^{\\intercal}\\mathbf{y}\n\\] Solving for b, we find:\n\\[\n\\begin{split}\n\\big(\\mathbf{PDP}^{\\intercal}\\big)^{-1}\\mathbf{PDP}^{\\intercal}\\mathbf{b} &= \\big(\\mathbf{PDP}^{\\intercal}\\big)^{-1} \\mathbf{X}^{\\intercal}\\mathbf{y} \\\\[2ex]\n\\mathbf{b} &= (\\mathbf{P}^\\intercal)^{-1}\\mathbf{D}^{-1}\\mathbf{P}^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y} \\\\[2ex]\n&= \\mathbf{P} \\mathbf{D}^{-1}\\mathbf{P}^\\intercal\\mathbf{X}^{\\intercal}\\mathbf{y}\n\\end{split}\n\\]\nThe orthogonal P matrix makes this computationally easier to solve. Moreover, since D is a diagonal matrix, \\(\\mathbf{D}^{-1}\\) is also easy to compute. Namely, \\(\\mathbf{D}^{-1}\\) is also diagonal with elements on the diagonal equal to \\(\\frac{1}{\\lambda_i}\\).\nConsider our ongoing simulated data:\n\n# Number of cases\nn = 50\n\n# Create 50 x-values evenly spread b/w 1 and 500 \nx = seq(from = 1, to = 500, len = n)\n\n# Create X matrix\nX = cbind(1, x, x^2, x^3)\n\n# Create b matrix\nb = matrix(\n  data = c(1, 1, 1, 1), \n  nrow = 4\n  )\n\n# Create vector of y-values\nset.seed(1)\ny = X %*% b + rnorm(n, mean = 0, sd = 1)\n\nWe start by using spectral decomposition to decompose \\(\\mathbf{X}^\\intercal\\mathbf{X}\\).\n\n# Spectral decomposition of (X^T)X\nspec_decomp = eigen(t(X) %*% X)\n\n# Create P and D^{-1}\nP = spec_decomp$vectors\nD_inv = diag(1/spec_decomp$values)\n\nNow we can carry out the matrix algebra to compute b.\n\n# Compute b\nP %*% D_inv %*% t(P) %*% t(X) %*% y\n\n          [,1]\n[1,] 0.8971488\n[2,] 1.0071665\n[3,] 0.9999593\n[4,] 1.0000001",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Spectral Decompostion</span>"
    ]
  },
  {
    "objectID": "46-svd-decomposition.html",
    "href": "46-svd-decomposition.html",
    "title": "25  Singular Value Decompostion",
    "section": "",
    "text": "25.1 SVD Using R\nSingular value decomposition (SVD) is commonly used for data compression or variable reduction, and plays a large role in machine learning applications. SVD decomposes a matrix into the product of three matrices:\n\\[\n\\underset{m\\times n}{\\mathbf{A}} = \\underset{m\\times n}{\\mathbf{U}}~ \\underset{n\\times n}{\\mathbf{D}}~ \\underset{n\\times n}{\\mathbf{V}^{\\intercal}}\n\\]\nwhere, U and V are an orthogonal matrices, and D is a diagonal matrix. From the properties of the transpose, we know that,\n\\[\n\\mathbf{A}^{\\intercal} = \\mathbf{V}\\mathbf{D}^{\\intercal}\\mathbf{U}^{\\intercal}\n\\]\nAnd for mathematical convenience, we can take advantage that U and V are an orthogonal matrices (\\(\\mathbf{U}^{\\intercal}\\mathbf{U}=\\mathbf{V}^{\\intercal}\\mathbf{V}=\\mathbf{I}\\)) by expressing two equations:\n\\[\n\\begin{split}\n\\mathbf{A}\\mathbf{A}^{\\intercal}\\mathbf{U} &= \\mathbf{U}\\mathbf{S}\\mathbf{V}^{\\intercal}\\mathbf{V}\\mathbf{D}^{\\intercal}\\mathbf{U}^{\\intercal}\\mathbf{U} \\\\[0.5em]\n&= \\mathbf{U}\\mathbf{D}^2\n\\end{split}\n\\]\nAnd,\n\\[\n\\begin{split}\n\\mathbf{A}^{\\intercal}\\mathbf{A}\\mathbf{V} &= \\mathbf{V}\\mathbf{D}^{\\intercal}\\mathbf{U}^{\\intercal}\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal}\\mathbf{V}\\\\[0.5em]\n&= \\mathbf{V}\\mathbf{D}^2\n\\end{split}\n\\]\nThese two equations are called eigenvalue equations, which show up all over the place in statistical work. These are easy to solve by computation. For example, we would solve the second eigenvalue equation to find V and D, and then find U by solving \\(\\mathbf{A}=\\mathbf{UDV}^\\intercal\\) for U, namely:\n\\[\n\\mathbf{U} = \\mathbf{A}\\mathbf{V}\\mathbf{D}^{-1}\n\\]\nIn practice, we will use the svd() function in R to carry out singular value decomposition.\n# Create A\nA = matrix(\n  data = c(5, -4, 1, 2), \n  nrow = 2\n  )\n\n# Singular value decomposition\nsv_decomp = svd(A)\n\n# View results\nsv_decomp\n\n$d\n[1] 6.422483 2.179842\n\n$u\n           [,1]      [,2]\n[1,] -0.7630200 0.6463749\n[2,]  0.6463749 0.7630200\n\n$v\n            [,1]       [,2]\n[1,] -0.99659268 0.08248053\n[2,]  0.08248053 0.99659268\nThe resulting decomposition is:\n\\[\n\\begin{split}\n\\mathbf{U} &= \\begin{bmatrix}\n-0.76 & 0.65 \\\\\n0.65 & 0.76  \\\\\n\\end{bmatrix} \\\\[1em]\n\\mathbf{D} &= \\begin{bmatrix}\n6.42 & 0 \\\\\n0 & 2.18  \\\\\n\\end{bmatrix} \\\\[1em]\n\\mathbf{V} &= \\begin{bmatrix}\n-1.00 & 0.08 \\\\\n0.08 & 1.00  \\\\\n\\end{bmatrix}\n\\end{split}\n\\] We can verify that \\(\\mathbf{A}=\\mathbf{USV}^\\intercal\\). Note that the svd() function outputs the diagonal elements of the D matrix, so we need to use the diag() function to create the actual D matrix.\n# Verify the results of the SVD\nsv_decomp$u %*% diag(sv_decomp$d) %*% t(sv_decomp$v)\n\n     [,1] [,2]\n[1,]    5    1\n[2,]   -4    2",
    "crumbs": [
      "Matrix Decomposition",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Singular Value Decompostion</span>"
    ]
  },
  {
    "objectID": "55-regression-important-matrices.html",
    "href": "55-regression-important-matrices.html",
    "title": "26  Important Matrices in Regression",
    "section": "",
    "text": "26.1 Design Matrix\nIn this chapter, you will learn about important and useful matrices in regression applications. To illustrate these matrices, we will use the following toy data set to fit a regression model using SAT and Self-Esteem to predict variation in GPA.\nRecall that the regression model is denoted as\n\\[\n\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e}\n\\]\nwhere y is the outcome vector, X is the design matrix, e is the vector of residuals, and b is the vector of coefficients estimated as,\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\n\\]\nThe design matrix, X plays a critical role in the regression model and in any estimation computed from the model. There is also a direct link from the model formula inputted in lm() and the design matrix. Namely, that each term “added” in the right-hand side of the formula (after the tilde) indicates a unique column in the design matrix. Below we use the toy data set to show the connection between different lm() formulations and the design matrix used.\n\\[\n\\begin{split}\n&\\mathtt{lm(GPA \\sim 1 + SAT)} \\\\[2ex]\n& \\mathbf{X} = \\begin{bmatrix}1 & 560 \\\\ 1 & 780 \\\\ 1 & 620\\\\ 1 & 600\\\\ 1 & 720\\\\ 1 & 380 \\end{bmatrix}\n\\end{split}\n\\]\nIn this design matrix we have two columns, indicated by the two terms being added on the right-hand side of the formula. The 1 in the formula indicates that the design matrix should include a column of 1s. The second term (we added SAT) is comprised of the SAT scores.\n\\[\n\\begin{split}\n&\\mathtt{lm(GPA \\sim 1 + SAT + IQ)} \\\\[2ex]\n& \\mathbf{X} = \\begin{bmatrix}1 & 560 & 112 \\\\ 1 & 780 & 143 \\\\ 1 & 620 & 124\\\\ 1 & 600 & 129\\\\ 1 & 720 & 130\\\\ 1 & 380 & 82 \\end{bmatrix}\n\\end{split}\n\\]\nIn this design matrix we have three columns, indicated by the three terms being added on the right-hand side of the formula. The first two columns are the same as in the previous design matrix, but now we also include a column of the IQ scores.\n\\[\n\\begin{split}\n&\\mathtt{lm(GPA \\sim 1 + SAT + IQ + SAT:IQ)} \\\\[2ex]\n& \\mathbf{X} = \\begin{bmatrix}1 & 560 & 112 & 62720 \\\\ 1 & 780 & 143 & 111540 \\\\ 1 & 620 & 124 & 76880\\\\ 1 & 600 & 129 & 77400\\\\ 1 & 720 & 130 & 93600\\\\ 1 & 380 & 82 & 31160 \\end{bmatrix}\n\\end{split}\n\\]\nIn this design matrix we have four columns, indicated by the four terms being added on the right-hand side of the formula. The first three columns are the same as in the previous design matrix. The column of interaction terms is based on the product of the SAT and IQ scores.\n\\[\n\\begin{split}\n&\\mathtt{lm(GPA \\sim 1 + SAT + I(SAT\\widehat{}2)} \\\\[2ex]\n& \\mathbf{X} = \\begin{bmatrix}1 & 560 & 313600 \\\\ 1 & 780 & 608400 \\\\ 1 & 620 & 384400\\\\ 1 & 600 & 360000\\\\ 1 & 720 & 518400\\\\ 1 & 380 & 144400 \\end{bmatrix}\n\\end{split}\n\\]\nIn this design matrix we have three columns, indicated by the three terms being added on the right-hand side of the formula. The first two columns are the same as in the previous design matrix. The quadratic effect of SAT constitutes the elements in the third column.1",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Important Matrices in Regression</span>"
    ]
  },
  {
    "objectID": "55-regression-important-matrices.html#vector-of-fitted-values",
    "href": "55-regression-important-matrices.html#vector-of-fitted-values",
    "title": "26  Important Matrices in Regression",
    "section": "26.2 Vector of Fitted Values",
    "text": "26.2 Vector of Fitted Values\nWe can compute the vector of fitted values using\n\\[\n\\hat{\\mathbf{y}} =  \\mathbf{Xb}\n\\]\nHere X has dimensions \\(n \\times k\\) and b has dimensions \\(k \\times 1\\). In both cases \\(k=p+1\\) where p is equal to the number of predictors in the model. This product gives the \\(n\\times 1\\) vector of fitted values, \\(\\hat{\\mathbf{y}}\\), which is,\n\\[\n\\begin{bmatrix}\\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_n\\end{bmatrix}\n\\]\n\n# Create vector of outcomes\ny = c(3.0, 3.9, 2.9, 2.7, 3.7, 2.4)\n\n# Create design matrix\nX = matrix(\n  data = c(\n    rep(1, 6), \n    560, 780, 620, 600, 720, 380, \n    11, 10, 19, 7, 18, 13\n    ),\n  ncol = 3\n)\n\n# Estimate coefficients\nb = solve(t(X) %*% X) %*% t(X) %*% y\n\n# Obtain vector of fitted values\ny_hat = X %*% b\n\n# View y_hat\ny_hat\n\n         [,1]\n[1,] 2.891351\n[2,] 3.719374\n[3,] 3.193177\n[4,] 3.006823\n[5,] 3.564540\n[6,] 2.224735",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Important Matrices in Regression</span>"
    ]
  },
  {
    "objectID": "55-regression-important-matrices.html#the-h-matrix-hat-matrix",
    "href": "55-regression-important-matrices.html#the-h-matrix-hat-matrix",
    "title": "26  Important Matrices in Regression",
    "section": "26.3 The H-Matrix (Hat Matrix)",
    "text": "26.3 The H-Matrix (Hat Matrix)\nOne useful matrix in regression is the hat matrix, or the H-matrix. To obtain the H-matrix we substitute the matrix formulation of the coefficient vector, b, into the equation to compute the fitted values,\n\\[\n\\begin{split}\n\\mathbf{\\hat{y}} &= \\mathbf{Xb} \\\\[2ex]\n&= \\mathbf{X}(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y}\n\\end{split}\n\\]\nThe set of products in this expression involving the X terms is referred to as H-matrix, that is:\n\\[\n\\begin{split}\n\\mathbf{\\hat{y}} &= \\underbrace{\\mathbf{X}(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}}_\\mathbf{H}\\mathbf{y} \\\\[2ex]\n&= \\mathbf{Hy}\n\\end{split}\n\\]\nwhere,\n\\[\n\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\n\\]\nNote that the H-matrix can be created completely from the design matrix and its transpose.\n\n# Create H\nH = X %*% solve(t(X) %*% X) %*% t(X)\n\n# View H\nH\n\n           [,1]         [,2]         [,3]       [,4]        [,5]        [,6]\n[1,] 0.22438375  0.137685003  0.059548164  0.2737852  0.02944891  0.27514901\n[2,] 0.13768500  0.575213416 -0.004685053  0.3380184  0.21144618 -0.25767793\n[3,] 0.05954816 -0.004685053  0.494121584 -0.1607883  0.43511643  0.17668712\n[4,] 0.27378517  0.338018387 -0.160788251  0.4941216 -0.10178310  0.15664621\n[5,] 0.02944891  0.211446179  0.435116432 -0.1017831  0.49436783 -0.06859625\n[6,] 0.27514901 -0.257677931  0.176687124  0.1566462 -0.06859625  0.71779184\n\n\nIt also has the following properties:\n\nH is a square \\(n \\times n\\) matrix\nH is a symmetric matrix.\nH is an idempotent matrix.\n\nIdempotency is a mathematical property that implies certain operations can be repeatedly applied to a structure without changing it. An idempotent matrix is a matrix that can be post-multiplied by itself and the result is the originasl matrix. In our example, since H is idempotent,\n\\[\n\\mathbf{HH} = \\mathbf{H}\n\\]\nWe can verify this using R\n\n# Verify H is idempotent\nH %*% H\n\n           [,1]         [,2]         [,3]       [,4]        [,5]        [,6]\n[1,] 0.22438375  0.137685003  0.059548164  0.2737852  0.02944891  0.27514901\n[2,] 0.13768500  0.575213416 -0.004685053  0.3380184  0.21144618 -0.25767793\n[3,] 0.05954816 -0.004685053  0.494121584 -0.1607883  0.43511643  0.17668712\n[4,] 0.27378517  0.338018387 -0.160788251  0.4941216 -0.10178310  0.15664621\n[5,] 0.02944891  0.211446179  0.435116432 -0.1017831  0.49436783 -0.06859625\n[6,] 0.27514901 -0.257677931  0.176687124  0.1566462 -0.06859625  0.71779184\n\n\nAs shown previously, we can also use the H-matrix to compute the fitted values, by post-multiplying H by the vector of outcomes:\n\n# Compute fitted values\nH %*% y\n\n         [,1]\n[1,] 2.891351\n[2,] 3.719374\n[3,] 3.193177\n[4,] 3.006823\n[5,] 3.564540\n[6,] 2.224735\n\n\nIn this computation, we can see that the fitted values can be expressed as linear combinations of the response vector Y using coefficients found in H. (This is why H is often referred to as the hat matrix.) For example, the first fitted value is computed as:\n\\[\n\\begin{split}\n\\hat{y}_1 &= 0.2244(3.0) +  0.1377(3.9) +  0.0595(2.9) +  0.2738(2.7) +  0.0294(3.7) + 0.2751(2.4) \\\\[2ex]\n&= 2.891\n\\end{split}\n\\]\nThe H-matrix has many uses in regression. Aside from using it to compute the vectors of fitted values and residuals, it is also used to obtain leverage measures for each of the observations. Additionally, the trace of the H-matrix indicates the number of parameters (including the intercept) that are being estimated in the regression model. For example, in our regression model, we are estimating three parameters: \\(\\beta_0\\), \\(\\beta_{\\mathrm{SAT}}\\), and \\(\\beta_{\\mathrm{Self\\mbox{-}Esteem}}\\).\n\n# Compute trace of H\nsum(diag(H))\n\n[1] 3",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Important Matrices in Regression</span>"
    ]
  },
  {
    "objectID": "55-regression-important-matrices.html#vector-of-residuals",
    "href": "55-regression-important-matrices.html#vector-of-residuals",
    "title": "26  Important Matrices in Regression",
    "section": "26.4 Vector of Residuals",
    "text": "26.4 Vector of Residuals\nThe \\(n\\times 1\\) vector of residuals, e, can be computed as,\n\\[\n\\mathbf{e} = \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n\\end{bmatrix} = \\mathbf{y} - \\mathbf{\\hat{y}}\n\\]\nDoing a little substitution,\n\\[\n\\begin{split}\n\\mathbf{e} &= \\mathbf{y} - \\mathbf{\\hat{y}} \\\\\n&= \\mathbf{y} - \\mathbf{Hy} \\\\\n&= (\\mathbf{I} - \\mathbf{H}) \\mathbf{y}\n\\end{split}\n\\]\nThus the residuals can also be expressed as linear combinations of the response vector y. The matrix \\((\\mathbf{I}-\\mathbf{H})\\) has some of the same properties as H, namely, it is\n\nSquare,\nSymmetric, and\nIdempotent\n\nWe can compute the residual vector using R\n\n# Create 6x6 identity matrix\nI = diag(6)\n\n# Compute the vector of residuals\nresiduals = (I - H) %*% y\n\n# View vector of residuals\nresiduals\n\n           [,1]\n[1,]  0.1086490\n[2,]  0.1806259\n[3,] -0.2931770\n[4,] -0.3068230\n[5,]  0.1354599\n[6,]  0.1752652",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Important Matrices in Regression</span>"
    ]
  },
  {
    "objectID": "55-regression-important-matrices.html#footnotes",
    "href": "55-regression-important-matrices.html#footnotes",
    "title": "26  Important Matrices in Regression",
    "section": "",
    "text": "The I() function makes the quadratic a specific column in the design matrix. You need the I() function because ^ is a special character in formula syntax that indicates crossing. Without the I() term the design matrix would only have two columns; a column of 1s and a column of SAT scores. You can read more by accesing the help page for formula, namely help(formula).↩︎",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Important Matrices in Regression</span>"
    ]
  },
  {
    "objectID": "56-regression-sums-of-squares.html",
    "href": "56-regression-sums-of-squares.html",
    "title": "27  Sums of Squares in Regression",
    "section": "",
    "text": "27.1 Sums of Squares\nIn this chapter, you will learn about how matrix algebra is used to compute sum of square terms in regression.\nRecall that the regression model can be used to partition the total variation in the outcome into that which is explained by the regression model and that which is not explained by the model. To do this we compute sums of square terms such that:\n\\[\n\\mathrm{SS}_{\\mathrm{Total}} = \\mathrm{SS}_{\\mathrm{Model}} + \\mathrm{SS}_{\\mathrm{Residual}}\n\\]\nwhere, algebraically,\n\\[\n\\begin{split}\n\\mathrm{SS}_{\\mathrm{Total}} &= \\sum(Y_i - \\bar{Y})^2\\\\[2ex]\n\\mathrm{SS}_{\\mathrm{Model}} &= \\sum(\\hat{Y}_i - \\bar{Y})^2\\\\[2ex]\n\\mathrm{SS}_{\\mathrm{Residual}} &= \\sum(Y_i - \\hat{Y}_i)^2\n\\end{split}\n\\]\nWe can also express each of these sums of squares using matrix notation. Remember that:\n\\[\n\\mathbf{y} = \\hat{\\mathbf{y}} + \\mathbf{e}\n\\]\nTo make this mathematically easier, we will subtract the mean vector from the outcome.\n\\[\n\\mathbf{y} - \\bar{\\mathbf{y}} = \\hat{\\mathbf{y}} + \\mathbf{e} - \\bar{\\mathbf{y}}\n\\]\nRe-arranging the right-hand side of the equation:\n\\[\n\\mathbf{y} - \\bar{\\mathbf{y}} = (\\hat{\\mathbf{y}}- \\bar{\\mathbf{y}}) + \\mathbf{e}\n\\]\nNow remember that the residuals and the fitted values are independent, which means that the vectors are orthogonal and the Pythagorean Theorem applies. That is:\n\\[\n(\\mathbf{y} - \\bar{\\mathbf{y}})^2 = (\\hat{\\mathbf{y}}- \\bar{\\mathbf{y}})^2 + \\mathbf{e}^2\n\\]\nSince these are vectors, squaring them creates dot products, which are sums. In this case, the sums of squares. That is, in matrix notation:\n\\[\n\\begin{split}\n\\mathrm{SS}_{\\mathrm{Total}} &= (\\mathbf{y} - \\bar{\\mathbf{y}})^2 \\\\[2ex]\n&= (\\mathbf{y} - \\bar{\\mathbf{y}})^\\intercal(\\mathbf{y} - \\bar{\\mathbf{y}})\\\\[4ex]\n\\mathrm{SS}_{\\mathrm{Model}} &= (\\hat{\\mathbf{y}}- \\bar{\\mathbf{y}})^2\\\\[2ex]\n&= (\\hat{\\mathbf{y}}- \\bar{\\mathbf{y}})^\\intercal (\\hat{\\mathbf{y}}- \\bar{\\mathbf{y}}) \\\\[4ex]\n\\mathrm{SS}_{\\mathrm{Residual}} &= \\mathbf{e}^2 \\\\[2ex]\n&= \\mathbf{e}^\\intercal \\mathbf{e}\n\\end{split}\n\\]",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Sums of Squares in Regression</span>"
    ]
  },
  {
    "objectID": "56-regression-sums-of-squares.html#sums-of-squares",
    "href": "56-regression-sums-of-squares.html#sums-of-squares",
    "title": "27  Sums of Squares in Regression",
    "section": "",
    "text": "27.1.1 Mean Centering the Outcome\nIf the outcome, y, is mean centered—it is a mean deviation vector—then the math is simplified further. In this case, since the mean of a mean centered variable is zero, the quantity \\(\\mathbf{y} - \\bar{\\mathbf{y}}=\\mathbf{y} - \\mathbf{0} =\\mathbf{y}\\), and the total sum of squares is, simply,\n\\[\n\\mathrm{SS}_{\\mathrm{Total}} = \\mathbf{y}^\\intercal\\mathbf{y}\n\\]\nSimilarly, the model sum of squares also is simplified using a mean centered outcome to,\n\\[\n\\mathrm{SS}_{\\mathrm{Model}} = \\hat{\\mathbf{y}}^\\intercal\\hat{\\mathbf{y}}\n\\] Finally, the residual sum of squares is\n\\[\n\\mathrm{SS}_{\\mathrm{Residual}} = \\mathbf{e}^\\intercal\\mathbf{e}\n\\]\nThis means that for mean centered data,\n\\[\n\\begin{split}\n\\mathrm{SS}_{\\mathrm{Total}} &= \\mathrm{SS}_{\\mathrm{Model}} + \\mathrm{SS}_{\\mathrm{Residual}} \\\\[2ex]\n\\mathbf{y}^\\intercal\\mathbf{y} &= \\hat{\\mathbf{y}}^\\intercal\\hat{\\mathbf{y}} + \\mathbf{e}^\\intercal\\mathbf{e}\n\\end{split}\n\\]",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Sums of Squares in Regression</span>"
    ]
  },
  {
    "objectID": "56-regression-sums-of-squares.html#sums-of-squares-as-functions-of-the-data",
    "href": "56-regression-sums-of-squares.html#sums-of-squares-as-functions-of-the-data",
    "title": "27  Sums of Squares in Regression",
    "section": "27.2 Sums of Squares as Functions of the Data",
    "text": "27.2 Sums of Squares as Functions of the Data\nThe model and residual sums of squares can also be written as products of the design matrix, X, and the vector of outcomes, y. To do this, we will make use of the relationships between \\(\\hat{\\mathbf{y}}\\), e, and the H-matrix. Remember that\n\\[\n\\begin{split}\n\\hat{\\mathbf{y}} &= \\mathbf{Hy} \\\\[2ex]\n\\mathbf{e} &= (\\mathbf{I} - \\mathbf{H}) \\mathbf{y}\n\\end{split}\n\\]\nwhere \\(\\mathbf{H}=\\mathbf{X} (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\).\nWe can re-write the sum of squared model using this formula,\n\\[\n\\begin{split}\n\\mathrm{SS}_{\\mathrm{Model}} &= \\hat{\\mathbf{y}}^\\intercal\\hat{\\mathbf{y}} \\\\[2ex]\n&= (\\mathbf{Hy})^\\intercal\\mathbf{Hy}\n\\end{split}\n\\]\nUsing the rules of transposes\n\\[\n\\mathrm{SS}_{\\mathrm{Model}} = \\mathbf{y}^\\intercal\\mathbf{H}^\\intercal\\mathbf{Hy}\n\\]\nAlso, remember that the H matrix is square and symmetric (\\(\\mathbf{H}=\\mathbf{H}^\\intercal\\)) and that it is idempotent (\\(\\mathbf{HH} = \\mathbf{H}\\)). This implies that \\(\\mathbf{HH}^\\intercal = \\mathbf{H}\\), and,\n\\[\n\\begin{split}\n\\mathrm{SS}_{\\mathrm{Model}} &= \\mathbf{y}^\\intercal\\mathbf{H}^\\intercal\\mathbf{Hy} \\\\[2ex]\n&= \\mathbf{y}^\\intercal\\mathbf{Hy} \\\\[2ex]\n&= \\mathbf{y}^\\intercal\\mathbf{X} (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\mathbf{y}\n\\end{split}\n\\] We can carry out a similar substitution and reduction to re-write the sum of squared residuals.\n\\[\n\\begin{split}\n\\mathrm{SS}_{\\mathrm{Model}} &= \\mathbf{y}^\\intercal\\mathbf{H}^\\intercal\\mathbf{Hy} \\\\[2ex]\n&= \\mathbf{y}^\\intercal\\mathbf{Hy} \\\\[2ex]\n&= \\mathbf{y}^\\intercal\\mathbf{X} (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\mathbf{y}\n\\end{split}\n\\]\nWe can write \\(\\hat{\\mathbf{y}}=\\mathbf{Xb}\\), which means\n\\[\n\\begin{split}\n\\mathrm{SS}_{\\mathrm{Model}} &= (\\mathbf{Xb})^\\intercal\\mathbf{Xb} \\\\[2ex]\n&= \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb}\n\\end{split}\n\\]\nThen, since \\(\\mathbf{b}=(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\\), we can substitute, getting,\n\\[\n\\begin{split}\n\\mathrm{SS}_{\\mathrm{Model}} &= \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb}\\\\[2ex]\n&= \\bigg((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\\bigg)^\\intercal\\mathbf{X}^\\intercal\\mathbf{X}\\bigg((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\\bigg)\n\\end{split}\n\\]\nThen, using the rules of transposes and inverses,\n\\[\n\\begin{split}\n\\mathrm{SS}_{\\mathrm{Model}} &= \\bigg((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\\bigg)^\\intercal\\mathbf{X}^\\intercal\\mathbf{X}\\bigg((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\\bigg) \\\\[2ex]\n&= \\mathbf{y}^\\intercal (\\mathbf{X}^\\intercal)^\\intercal \\bigg((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\bigg)^\\intercal \\mathbf{X}^\\intercal\\mathbf{X}(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y} \\\\[2ex]\n&= \\mathbf{y}^\\intercal\\mathbf{X} \\bigg((\\mathbf{X}^\\intercal\\mathbf{X})^\\intercal\\bigg)^{-1} \\mathbf{I}\\mathbf{X}^\\intercal\\mathbf{y} \\\\[2ex]\n&= \\mathbf{y}^\\intercal\\mathbf{X} (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\mathbf{y}\n\\end{split}\n\\]\nFinally, we can also rewrite the residual sum of squares, again, using the rules of matrix algebra.\n\\[\n\\begin{split}\n\\mathrm{SS}_{\\mathrm{Residual}} &= \\mathbf{e}^\\intercal\\mathbf{e} \\\\[2ex]\n&= \\bigg((\\mathbf{I}-\\mathbf{H})\\mathbf{y}\\bigg)^\\intercal (\\mathbf{I}-\\mathbf{H})\\mathbf{y} \\\\[2ex]\n&= \\mathbf{y}^\\intercal (\\mathbf{I}-\\mathbf{H})^\\intercal (\\mathbf{I}-\\mathbf{H})\\mathbf{y} \\\\[2ex]\n&= \\mathbf{y}^\\intercal (\\mathbf{I}-\\mathbf{H})\\mathbf{y} \\\\[2ex]\n&= \\mathbf{y}^\\intercal (\\mathbf{y}-\\mathbf{Hy}) \\\\[2ex]\n&= \\mathbf{y}^\\intercal\\mathbf{y} - \\mathbf{y}^\\intercal\\mathbf{Hy} \\\\[2ex]\n&= \\mathbf{y}^\\intercal\\mathbf{y} - \\mathbf{y}^\\intercal\\mathbf{X} (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\mathbf{y}\n\\end{split}\n\\]\nAnd, of course \\(\\mathrm{SS}_{\\mathrm{Total}}=\\mathbf{y}^\\intercal\\mathbf{y}\\). Each of the sums of squares is a function of the design matrix or the vector of outcomes. Don’t forget that for these computations, the vector of outcomes needs to be mean-centered!\n\n\n27.2.1 Example Using R\nTo illustrate these computations, we will again use the following toy data set to fit a regression model using SAT and Self-Esteem to predict variation in GPA.\n\n\n\n\nTable 27.1: Example set of education data.\n\n\n\n\n\n\nID\nSAT\nGPA\nSelf-Esteem\nIQ\n\n\n\n\n1\n560\n3.0\n11\n112\n\n\n2\n780\n3.9\n10\n143\n\n\n3\n620\n2.9\n19\n124\n\n\n4\n600\n2.7\n7\n129\n\n\n5\n720\n3.7\n18\n130\n\n\n6\n380\n2.4\n13\n82\n\n\n\n\n\n\n\n\nFitting the model and partitioning the variation (not shown), we find:\n\n\\(\\mathrm{SS}_{\\mathrm{Total}} = 1.7\\)\n\\(\\mathrm{SS}_{\\mathrm{Model}} = 1.426\\)\n\\(\\mathrm{SS}_{\\mathrm{Residual}} = 0.274\\)\n\nUsing the matrix algebra, we can replicate these values.\n\n# Create vector of outcomes\ny = c(3.0, 3.9, 2.9, 2.7, 3.7, 2.4)\n\n# Mean-center y\nmc_y = y - mean(y)\n\n# Create design matrix\nX = matrix(\n  data = c(\n    rep(1, 6),\n    560, 780, 620, 600, 720, 380, \n    11, 10, 19, 7, 18, 13\n    ),\n  ncol = 3\n)\n\n\n# Compute SS_total\nt(mc_y) %*% mc_y\n\n     [,1]\n[1,]  1.7\n\n# Compute SS_model\nt(mc_y) %*% X %*% solve(t(X) %*% X) %*% t(X) %*% mc_y\n\n         [,1]\n[1,] 1.426409\n\n# Compute SS_residuals\nt(mc_y) %*% mc_y - t(mc_y) %*% X %*% solve(t(X) %*% X) %*% t(X) %*% mc_y\n\n          [,1]\n[1,] 0.2735907",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Sums of Squares in Regression</span>"
    ]
  },
  {
    "objectID": "57-regression-se-and-inference.html",
    "href": "57-regression-se-and-inference.html",
    "title": "28  Standard Errors and Variance Estimates",
    "section": "",
    "text": "28.1 Residual Variance of the Model\nIn this chapter, you will learn about how matrix algebra is used to compute standard errors and variance estimates in regression. These allow us to compute confidence intervals and carry out hypothesis tests.\nTo illustrate computations, we will again use the following toy data set to fit a regression model using SAT and Self-Esteem to predict variation in GPA.\nRecall that the estimate of the residual variance (i.e., mean squared residuals) for the model is\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{\\mathrm{SS}_{\\mathrm{Residuals}}}{\\mathrm{df}_{\\mathrm{Residuals}}}\n\\]\nUsing matrices, the sum of squared error (\\(\\mathrm{SS}_{\\mathrm{Residuals}}\\)) is\n\\[\n\\mathrm{SS}_{\\mathrm{Residuals}} = \\mathbf{e}^\\intercal\\mathbf{e}\n\\]\nThe degrees-of-freedom associated with the residuals is \\(\\mathrm{df}_{\\mathrm{Residuals}} = n-k\\), where \\(k\\) is the number of coefficients (including the intercept) being estimated in the model. Recall that the value of \\(k\\) is the trace of the H-matrix. This implies,\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{\\mathbf{e}^\\intercal\\mathbf{e}}{n - \\mathrm{tr}(\\mathbf{H})}\n\\]\nUsing our toy example,\n# Create vector of outcomes\ny = c(3.0, 3.9, 2.9, 2.7, 3.7, 2.4)\n\n# Create design matrix\nX = matrix(\n  data = c(\n    rep(1, 6),\n    560, 780, 620, 600, 720, 380, \n    11, 10, 19, 7, 18, 13\n    ),\n  ncol = 3\n)\n\n# Compute SS_residual\nb = solve(t(X) %*% X) %*% t(X) %*% y\ne = y - X %*% b\nss_resid = t(e) %*% e\n\n# Compute df_residual\nH = X %*% solve(t(X) %*% X) %*% t(X)\nk = sum(diag(H))\ndf_resid = 6 - k\n\n# Compute estimate of error variance\nvar_e = ss_resid / df_resid\nvar_e\n\n          [,1]\n[1,] 0.0911969\nIf we take the sqaure root of this estimate, that gives us the residual standard error (RSE) of the model, a.k.a., the root mean square error (RMSE).\n# Find RSE/RMSE\nsqrt(var_e)\n\n          [,1]\n[1,] 0.3019882",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Standard Errors and Variance Estimates</span>"
    ]
  },
  {
    "objectID": "57-regression-se-and-inference.html#coefficient-variances-and-covariances",
    "href": "57-regression-se-and-inference.html#coefficient-variances-and-covariances",
    "title": "28  Standard Errors and Variance Estimates",
    "section": "28.2 Coefficient Variances and Covariances",
    "text": "28.2 Coefficient Variances and Covariances\nIn matrix applications, it is typical to indicate the error variances and covariances for the coefficients in a variance–covariance matrix. In this matrix the error variances for each of the coefficients is given along the main diagonal of the matrix. The covariances between the coefficients are given in the off-diagonal elements. For example, the variance–covariance matrix for a simple regression model is:\n\\[\n\\boldsymbol{\\sigma^2_B} = \\begin{bmatrix}\\mathrm{Var}(b_0) & \\mathrm{Cov}(b_0,b_1) \\\\ \\mathrm{Cov}(b_0,b_1)  & \\mathrm{Var}(b_1) \\end{bmatrix}\n\\]\nExamining the variance-covariance matrix of the coefficients, we find that it is a square, symmetric matrix with dimensions of \\(k \\times k\\). This matrix is defined as:\n\\[\n\\boldsymbol{\\sigma^2_B} = \\sigma^2_{\\epsilon} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\n\\]\n\nIn our toy example,\n\n# Variance-covariance matrix of the coefficients\nV_b = as.numeric(var_e) * solve(t(X) %*% X)\nV_b\n\n              [,1]             [,2]            [,3]\n[1,]  0.4741275943 -0.0005503737663 -0.009476931832\n[2,] -0.0005503738  0.0000009501116 -0.000002245718\n[3,] -0.0094769318 -0.0000022457184  0.000834370776\n\n\nWe use as.numeric() to convert the estimate of the model-level error variance to a numeric scalar. The estimated variance for each of the coefficients are:\n\n\\(\\mathrm{Var}(\\hat\\beta_0) = 0.474\\)\n\\(\\mathrm{Var}(\\hat\\beta_{\\mathrm{SAT}}) = 0.00000095\\)\n\\(\\mathrm{Var}(\\hat\\beta_{\\mathrm{Self\\mbox{-}Esteem}}) = 0.00083\\)\n\nThe covariances between these coefficients are also given in this matrix:\n\n\\(\\mathrm{Cov}(\\hat\\beta_0,\\hat\\beta_{\\mathrm{SAT}}) = -0.00055\\)\n\\(\\mathrm{Cov}(\\hat\\beta_0,\\hat\\beta_{\\mathrm{Self\\mbox{-}Esteem}}) = -0.0095\\)\n\\(\\mathrm{Cov}(\\hat\\beta_{\\mathrm{SAT}},\\hat\\beta_{\\mathrm{Self\\mbox{-}Esteem}}) = -0.0000022\\)\n\nIn practice, the variance-covariance matrix of the regression coefficients can be obtained directly from R using the vcov() function.\n\n# Fit model\nlm.1 = lm(y ~ 1 + X[ , 2] + X[ , 3])\n\n# Obtain variance-covariance matrix of the coefficients\nvcov(lm.1)\n\n              (Intercept)           X[, 2]          X[, 3]\n(Intercept)  0.4741275943 -0.0005503737663 -0.009476931832\nX[, 2]      -0.0005503738  0.0000009501116 -0.000002245718\nX[, 3]      -0.0094769318 -0.0000022457184  0.000834370776",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Standard Errors and Variance Estimates</span>"
    ]
  },
  {
    "objectID": "57-regression-se-and-inference.html#standard-errors-for-the-coefficients",
    "href": "57-regression-se-and-inference.html#standard-errors-for-the-coefficients",
    "title": "28  Standard Errors and Variance Estimates",
    "section": "28.3 Standard Errors for the Coefficients",
    "text": "28.3 Standard Errors for the Coefficients\nWhile the variances are mathematically convenient, in applied work, the standard errors (SEs) of the coefficients are more commonly presented. Recall that the standard errors are given in the same metric as the coefficient estimates, and represent the uncertainty in that estimate. They are also used to construct test statistics (e.g., z or t) and confidence intervals for the estimates. The estimated standard error for each regression coefficient can be found by computing the square root of the variance estimates, that is:\n\\[\n\\mathrm{SE}(\\beta_j) = \\sqrt{\\mathrm{Var}(\\beta_j)}\n\\]\nThus, we can find the standard errors by computing the square roots of the diagonal elements in the variance–covariance matrix of the coefficients.\n\n# Compute SEs\nse = sqrt(diag(V_b))\n\n# View SEs\nse\n\n[1] 0.6885692371 0.0009747367 0.0288854769",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Standard Errors and Variance Estimates</span>"
    ]
  },
  {
    "objectID": "57-regression-se-and-inference.html#correlation-between-the-coefficients",
    "href": "57-regression-se-and-inference.html#correlation-between-the-coefficients",
    "title": "28  Standard Errors and Variance Estimates",
    "section": "28.4 Correlation Between the Coefficients",
    "text": "28.4 Correlation Between the Coefficients\nRecall that correlation, which is a standardized covariance, is often times more interpretable than the covariance. We can obtain the correlation coefficient between two coefficients, \\(\\hat\\beta_j\\) and \\(\\hat\\beta_k\\), using\n\\[\n\\mathrm{Cor}(b_j,b_k) = \\frac{\\mathrm{Cov}(b_j,b_k)}{\\sqrt{\\mathrm{Var}(b_j)\\times \\mathrm{Var}(b_k)}}\n\\]\nUsing the values from our example, we can compute the correlation between each set of regression coefficients:\n\\[\n\\begin{split}\n\\mathrm{Cor}(\\hat\\beta_0,\\hat\\beta_{\\mathrm{SAT}}) &= \\frac{-0.00055}{\\sqrt{0.474 \\times 0.00000095}} &= -0.820 \\\\[2ex]\n\\mathrm{Cor}(\\hat\\beta_0,\\hat\\beta_{\\mathrm{Self\\mbox{-}Esteem}}) &= \\frac{-0.0095}{\\sqrt{0.474 \\times 0.00083}} &= -0.479 \\\\[2ex]\n\\mathrm{Cor}(\\hat\\beta_{\\mathrm{SAT}},\\hat\\beta_{\\mathrm{Self\\mbox{-}Esteem}}) &= \\frac{-0.0000022}{\\sqrt{0.00000095 \\times 0.00083}} &= -0.078\n\\end{split}\n\\]\nIn R, if we have assigned the variance-covariance matrix to an object, we can use indexing to access the different elements to compute the correlation.\n\n# Compute correlation between b_0 and b_SAT\nV_b[1, 2] / sqrt(V_b[1, 1] * V_b[2, 2])\n\n[1] -0.8200169\n\n# Compute correlation between b_0 and b_selfesteem\nV_b[1, 3] / sqrt(V_b[1, 1] * V_b[3, 3])\n\n[1] -0.4764755\n\n# Compute correlation between b_SAT and b_selfesteem\nV_b[2, 3] / sqrt(V_b[2, 2] * V_b[3, 3])\n\n[1] -0.07976061\n\n\nThe correlations indicates that the intercept is highly, negatively correlated with both the other coefficients. Whereas the effects of SAT and self-esteem seem to be mostly uncorrelated (i.e., independent).",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Standard Errors and Variance Estimates</span>"
    ]
  },
  {
    "objectID": "57-regression-se-and-inference.html#inference-model-level",
    "href": "57-regression-se-and-inference.html#inference-model-level",
    "title": "28  Standard Errors and Variance Estimates",
    "section": "28.5 Inference: Model-Level",
    "text": "28.5 Inference: Model-Level\nThe model-level (i.e., omnibus) null hypothesis for the regression model\n\\[\nY_i = \\beta_0 + \\beta_1(X_{1_i}) + \\beta_2(X_{2_i}) + \\beta_3(X_{3_i}) + \\ldots + \\beta_k(X_{k_i}) + \\epsilon_i\n\\]\nis commonly expressed as:\n\\[\nH_0: \\beta_1 = \\beta_2 = \\beta_3 = \\ldots = \\beta_k = 0\n\\]\nIf we write the set of predictor coefficients as a vector, \\(\\boldsymbol{\\beta}^\\intercal = \\begin{bmatrix}  \\beta_1 & \\beta_2 & \\beta_3 & \\ldots & \\beta_k\\end{bmatrix}\\), then we can write the model-level null hypothesis using vector notation:\n\\[\nH_0: \\boldsymbol{\\beta} = 0\n\\]\nNote that the we can express the vector of coefficients as either a column vector (\\(\\boldsymbol{\\beta}\\)) or as a row vector (\\(\\boldsymbol{\\beta}^\\intercal\\)). To test this we compute an F-statistic based on the ratio of the mean square for the model and that for the residuals. This is evaluted in an F-distribution having \\(k-1\\) and \\(n-k\\) degrees of freedom. Using our toy-example:\n\n# Compute MS_model\nmc_y = y - mean(y) # Mean-center y\nss_model = t(mc_y) %*% X %*% solve(t(X) %*% X) %*% t(X) %*% mc_y\nms_model = ss_model / (k - 1)\nms_model\n\n          [,1]\n[1,] 0.7132047\n\n# Compute MS_residual\nss_resid = t(mc_y) %*% mc_y - t(mc_y) %*% X %*% solve(t(X) %*% X) %*% t(X) %*% mc_y\nms_resid = ss_resid / (6 - k)\nms_resid\n\n          [,1]\n[1,] 0.0911969\n\n# Compute F\nF_stat = ms_model / ms_resid\nF_stat\n\n         [,1]\n[1,] 7.820492\n\n# Compute p-value\n1 - pf(F_stat, df1 = (k-1), df2 = (6-k))\n\n           [,1]\n[1,] 0.06456224",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Standard Errors and Variance Estimates</span>"
    ]
  },
  {
    "objectID": "57-regression-se-and-inference.html#inference-coefficient-level",
    "href": "57-regression-se-and-inference.html#inference-coefficient-level",
    "title": "28  Standard Errors and Variance Estimates",
    "section": "28.6 Inference: Coefficient-Level",
    "text": "28.6 Inference: Coefficient-Level\nThe coefficient-level null hypotheses for the regression model are:\n\\[\n\\begin{split}\nH_0: \\beta_j = 0\n\\end{split}\n\\]\nfor each j. To test this we compute a t-statistic based on the ratio of the coefficent estimate and that its associated standard error. This is evaluted in a t-distribution having \\(n-k\\) degrees of freedom. Using our toy-example:\n\n# Compute t for all three coefficients\nt_stat = b / se\nt_stat\n\n          [,1]\n[1,] 0.9573671\n[2,] 3.9041321\n[3,] 0.3180490\n\n# Compute p-value\n2 * pt(-abs(t_stat), df = (6-k), lower.tail = TRUE)\n\n           [,1]\n[1,] 0.40900915\n[2,] 0.02983868\n[3,] 0.77130154\n\n\nWe could also compute a confidence interval using,\n\\[\nB_j \\pm t^*(\\mathrm{SE}_{B_j})\n\\]\nwhere \\(B_j\\) is the jth coefficient, \\(\\mathrm{SE}_{B_j}\\) is the standard error associated with that coefficient, and \\(t^*\\) is the appropriate critical value. Using our toy example,\n\n# Obtain critical value for 95% CI\nt_star = abs(qt(.025, df = (6-k)))\n\n# Compute lower limit of CI\nb - t_star*se\n\n              [,1]\n[1,] -1.5321211074\n[2,]  0.0007034536\n[3,] -0.0827394812\n\n# Compute upper limit of CI\nb + t_star*se\n\n            [,1]\n[1,] 2.850548142\n[2,] 0.006907548\n[3,] 0.101113477",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Standard Errors and Variance Estimates</span>"
    ]
  },
  {
    "objectID": "58-regression-assumptions.html",
    "href": "58-regression-assumptions.html",
    "title": "29  Assumptions of the Regression Model",
    "section": "",
    "text": "29.1 Regression Assumptions\nIn this chapter, you will learn about how matrix algebra is used to express and understand the distributional assumptions underlying the regression model. To illustrate computations, we will again use the following toy data set to fit a regression model using SAT and Self-Esteem to predict variation in GPA.\nThere are several distributional assumptions about the errors for the regression model, namely that the residuals (conditioned on each fitted value) are:\nMathematically, we express this as part of the model as:\n\\[\n\\epsilon_{i|\\hat{y}} \\overset{\\mathrm{i.i.d~}}{\\sim} \\mathcal{N}\\left(0, \\sigma^2_{\\epsilon}\\right)\n\\] The mathematical expression says the residuals conditioned on \\(\\hat{y}\\) (having the same fitted value) are independent and identically normally distributed with a mean of 0 and some variance (\\(\\sigma^2_{\\epsilon}\\)).",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Assumptions of the Regression Model</span>"
    ]
  },
  {
    "objectID": "58-regression-assumptions.html#regression-assumptions",
    "href": "58-regression-assumptions.html#regression-assumptions",
    "title": "29  Assumptions of the Regression Model",
    "section": "",
    "text": "Independent;\nNormally distributed;\nAverage residual is 0; and\nConstant variance.",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Assumptions of the Regression Model</span>"
    ]
  },
  {
    "objectID": "58-regression-assumptions.html#expressing-the-assumptions-uising-matrix-algebra",
    "href": "58-regression-assumptions.html#expressing-the-assumptions-uising-matrix-algebra",
    "title": "29  Assumptions of the Regression Model",
    "section": "29.2 Expressing the Assumptions uising Matrix Algebra",
    "text": "29.2 Expressing the Assumptions uising Matrix Algebra\nWe can express these same assumptions using matrix notation. First recall that the residuals from the model can be expressed as a vector:\n\\[\n\\boldsymbol{\\epsilon} = \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2\\\\ \\epsilon_3\\\\ \\vdots \\\\ \\epsilon_n\\end{bmatrix}\n\\]\nNote: Here we use \\(\\boldsymbol{\\epsilon}\\) rather than e to indicate that the vector is for the residuals in the population. The assumption that the average residual (at each \\(\\hat{y}\\)) is zero can be written using expectations as:\n\\[\n\\begin{split}\nE(\\boldsymbol{\\epsilon}) = \\mathbf{0} \\\\[2em]\nE\\begin{pmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2\\\\ \\epsilon_3\\\\ \\vdots \\\\ \\epsilon_n\\end{bmatrix}\\end{pmatrix} = \\begin{bmatrix}0 \\\\ 0\\\\ 0\\\\ \\vdots \\\\ 0\\end{bmatrix}\n\\end{split}\n\\]\nThe constant variance assumption says that the variance of the residuals (at each \\(\\hat{y}\\)) is the same, \\(\\sigma^2_{\\epsilon}\\). Remember that this is the residual variance for the model, which we estimate as:\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{\\mathbf{e}^\\intercal\\mathbf{e}}{n - \\mathrm{tr}(\\mathbf{H})}\n\\]\nThe independence assumption implies that the covariance between two residuals is 0. Namely,\n\\[\n\\mathrm{Cov}(\\epsilon_i, \\epsilon_j) = 0\n\\]\nfor all \\(i\\neq j\\).",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Assumptions of the Regression Model</span>"
    ]
  },
  {
    "objectID": "58-regression-assumptions.html#variance-covariance-matrix-of-the-residuals",
    "href": "58-regression-assumptions.html#variance-covariance-matrix-of-the-residuals",
    "title": "29  Assumptions of the Regression Model",
    "section": "29.3 Variance-Covariance Matrix of the Residuals",
    "text": "29.3 Variance-Covariance Matrix of the Residuals\nIf we combine the assumption of independence with the assumption of constant variance, we can write out the variance-covariance matrix of the residuals.\n\\[\n\\boldsymbol{\\Sigma_\\epsilon} = \\begin{bmatrix}\\sigma^2_{\\epsilon} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\sigma^2_{\\epsilon} & 0 & \\ldots & 0\\\\ 0 & 0 & \\sigma^2_{\\epsilon} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\sigma^2_{\\epsilon}\\end{bmatrix}\n\\]\nNotice that the variance-covariance matrix of the residuals is a diagonal, scalar matrix. The variances are given along the main diagonal and the covariances are the off-diagonal elements. This matrix indicates that the variances are all the same value and the covariances between residuals is 0 (i.e., independence).\nThis matrix can also be expressed as the product of \\(\\sigma^2_{\\epsilon}\\) and an \\(n \\times n\\) identity matrix:\n\\[\n\\begin{split}\n\\boldsymbol{\\Sigma_\\epsilon}  &= \\begin{bmatrix}\\sigma^2_{\\epsilon} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\sigma^2_{\\epsilon} & 0 & \\ldots & 0\\\\ 0 & 0 & \\sigma^2_{\\epsilon} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\sigma^2_{\\epsilon}\\end{bmatrix} \\\\[2ex]\n&= \\sigma^2_{\\epsilon} \\begin{bmatrix}1 & 0 & 0 & \\ldots & 0 \\\\ 0 & 1 & 0 & \\ldots & 0\\\\ 0 & 0 & 1 & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & 1\\end{bmatrix}\\\\[2ex]\n&= \\sigma^2_{\\epsilon}~ \\mathbf{I}\n\\end{split}\n\\]\nUsing our toy example, the variance-covariance matrix of the residuals can be computed as:\n\n# Create vector of outcomes\ny = c(3.0, 3.9, 2.9, 2.7, 3.7, 2.4)\n\n# Create design matrix\nX = matrix(\n  data = c(\n    rep(1, 6),\n    560, 780, 620, 600, 720, 380, \n    11, 10, 19, 7, 18, 13\n    ),\n  ncol = 3\n)\n\n# Compute residual variance\nb = solve(t(X) %*% X) %*% t(X) %*% y\ne = y - X %*% b\nss_resid = t(e) %*% e\ndf_resid = 6 - 2\nvar_e = ss_resid / df_resid\n\n# Compute variance-covariance matrix of residuals\nas.numeric(var_e) * diag(6)\n\n           [,1]       [,2]       [,3]       [,4]       [,5]       [,6]\n[1,] 0.06839767 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n[2,] 0.00000000 0.06839767 0.00000000 0.00000000 0.00000000 0.00000000\n[3,] 0.00000000 0.00000000 0.06839767 0.00000000 0.00000000 0.00000000\n[4,] 0.00000000 0.00000000 0.00000000 0.06839767 0.00000000 0.00000000\n[5,] 0.00000000 0.00000000 0.00000000 0.00000000 0.06839767 0.00000000\n[6,] 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.06839767",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Assumptions of the Regression Model</span>"
    ]
  },
  {
    "objectID": "58-regression-assumptions.html#regression-model-revisited",
    "href": "58-regression-assumptions.html#regression-model-revisited",
    "title": "29  Assumptions of the Regression Model",
    "section": "29.4 Regression Model: Revisited",
    "text": "29.4 Regression Model: Revisited\nUsing matrix notation, we compactly specify the regression model as:\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\\]\nwhere \\(\\boldsymbol{\\epsilon}_{\\vert\\hat{y}}\\) is a vector of independent random variables (conditioned on some predicted value) that is normally distributed with: \\(E(\\boldsymbol{\\epsilon})=0\\) and \\(\\boldsymbol{\\Sigma_\\epsilon}= \\sigma^2_{\\epsilon}\\mathbf{I}\\).",
    "crumbs": [
      "Regression Applications",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Assumptions of the Regression Model</span>"
    ]
  }
]